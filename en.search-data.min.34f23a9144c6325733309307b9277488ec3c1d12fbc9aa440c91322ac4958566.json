[{"id":0,"href":"/docs/algorithm/","title":"Algorithm","section":"Docs","content":" 000 Sample "},{"id":1,"href":"/docs/batch/","title":"Batch","section":"Docs","content":" 002 Batch Performance 001 Performance Improved Batch "},{"id":2,"href":"/docs/clean_code/","title":"Clean Code","section":"Docs","content":" 001 Argument "},{"id":3,"href":"/docs/data_structure/","title":"Data Structure","section":"Docs","content":" 000 Sample "},{"id":4,"href":"/docs/ddd/","title":"Ddd","section":"Docs","content":" 000 Sample "},{"id":5,"href":"/docs/docker/","title":"Docker","section":"Docs","content":" 000 Sample "},{"id":6,"href":"/docs/etc/","title":"Etc","section":"Docs","content":" 004 Sonar Qube 005 Rabbit Mq 003 Huge Traffic Handling 002 Tps 001 Jmeter "},{"id":7,"href":"/docs/java/","title":"Java","section":"Docs","content":" 001 Facade Pattern "},{"id":8,"href":"/docs/jenkins/","title":"Jenkins","section":"Docs","content":" 000 Sample "},{"id":9,"href":"/docs/jpa/","title":"Jpa","section":"Docs","content":" 000 Sample "},{"id":10,"href":"/docs/kafka/","title":"Kafka","section":"Docs","content":" 004 Kafka Broker 005 Broker Log Segment 003 Transactional Outbox Pattern 002 Kafka Intro 001 Kafka Cdc "},{"id":11,"href":"/docs/kotlin/","title":"Kotlin","section":"Docs","content":" 005 Kotlin Extract List Method 004 Kotlin Scoping Functions 003 Kotlin Basic 002 Functional Programming Example 001 Functional Programming "},{"id":12,"href":"/docs/linux/","title":"Linux","section":"Docs","content":" 000 Sample "},{"id":13,"href":"/docs/mongodb/","title":"Mongodb","section":"Docs","content":" 003 Spring Data Mongodb 004 Object Mapping 005 Mongo Operations 006 Reactive Mongo Repository 007 Query Method 001 Reactive Mongodb 002 Mongodb Document "},{"id":14,"href":"/docs/msa/","title":"Msa","section":"Docs","content":" 003 Circuit Breaker 001 Reactive 002 Reactive Microservice "},{"id":15,"href":"/docs/mvc/","title":"Mvc","section":"Docs","content":" 000 Sample "},{"id":16,"href":"/docs/mysql/","title":"Mysql","section":"Docs","content":" 000 Sample "},{"id":17,"href":"/docs/network/","title":"Network","section":"Docs","content":" 000 Sample "},{"id":18,"href":"/docs/operating_system/","title":"Operating System","section":"Docs","content":" 001 Thread "},{"id":19,"href":"/docs/oracle/","title":"Oracle","section":"Docs","content":" 000 Sample "},{"id":20,"href":"/docs/parallel_programming/","title":"Parallel Programming","section":"Docs","content":" 030 Synchronized Etc 029 Method Synchronized 027 Spin Lock 028 Synchronized Basic 026 Monitor 024 Mutual Exclusion 025 Semaphore 020 Single Multi Thread 021 Synchronized CPU 022 Critical Section 023 Thread Safe 017 Thread Type 018 Thread Group 019 Thread Local 012 Thread Interrupt 013 Thread Info 014 Thread Priority 015 Thread Uncaught Exception Handler 016 Thread Stop Flag 011 Thread Join 010 Thread Sleep 009 Thread Status 008 Thread Start 004 CPU Bound Io Bound 005 User Kernel Systemcall 006 User Mode Kernel Mode Thread 007 Java Thread 003 Context Switching 001 Process Thread 002 Parallel Concurrent "},{"id":21,"href":"/docs/r2dbc/","title":"R2dbc","section":"Docs","content":" 005 R2dbc Metadata Mapping 006 R2dbc Entity Operations 007 R2dbc Repository 008 R2dbc Query Method 003 R2dbc Entity Template 004 R2dbc Object Mapping 002 R2dbc Mysql 001 R2dbc Intro "},{"id":22,"href":"/docs/rdbms/","title":"Rdbms","section":"Docs","content":" 004 Window Function 005 Subquery 001 Basic Join 002 Date Interval 003 Groupby "},{"id":23,"href":"/docs/reactive_streams/","title":"Reactive Streams","section":"Docs","content":" 002 Impl1 Reactor 003 Impl2 Rxjava 004 Impl3 Munity 001 Reactive Streams Component "},{"id":24,"href":"/docs/redis/","title":"Redis","section":"Docs","content":" 008 Spring Session Redis 007 Redis Cluster Migration 003 Reactive Redis Intro 004 Lettuce 005 Reactive Redis Template 006 Reactive Operations 001 Redis Datastructure 002 Redis Cache "},{"id":25,"href":"/docs/security/","title":"Security","section":"Docs","content":" 000 Sample "},{"id":26,"href":"/docs/servlet/","title":"Servlet","section":"Docs","content":" 000 Sample "},{"id":27,"href":"/docs/spring/","title":"Spring","section":"Docs","content":" 000 Sample "},{"id":28,"href":"/docs/springbatch/","title":"Springbatch","section":"Docs","content":" 001 Springbatch Start "},{"id":29,"href":"/docs/webflux/","title":"Webflux","section":"Docs","content":" 000 Sample "},{"id":30,"href":"/docs/parallel_programming/030_synchronized_etc/","title":"030 Synchronized Etc","section":"Parallel Programming","content":" 강의 메모 - synchronized 특성 # 재진입성 # 모니터 내에서 이미 synchronized 영역에 들어간 스레드가 다시 같은 모니터 영역으로 들어갈 수 있는데, 이를 \u0026ldquo;모니터 재진입\u0026quot;이라고 한다 재진입 가능하다는 것은 락의 획득이 호출 단위가 아닌 스레드 단위로 일어난다는 것을 의미하며 이미 락을 획득한 스레드는 같은 락을 얻기 위해 대기할 필요 없이 synchronized 블록을 만났을 때 같은 락을 확보하고 진입한다 같은 모니터라면 대기/경쟁 없이 바로 진입 가능 (스레드 단위) 상속하게 되면 자식은 부모의 락과 동일한 락을 가지게 된다 자식 클래스에서 method() 실행됨 부모클래스의 method() 실행 서로 모니터가 동일하므로 재진입이 가능한것 동기화 된 메서드에서 다른 동기화 된 메서드를 호출하는 경우 이미 락(lock)을 가지고 있는 스레드가 같은 락을 확보하고 재진입 시 데드락이 발생하지 않고 정상적으로 진행할 수 있게 된다 가시성 # synchronized 는 가시성을 지원한다 가시성이란 한 스레드가 공유자원을 수정하거나 쓰기 작업을 했을 때 다른 스레드가 수정한 내용이 보이는 것을 말한다 CPU 특성상 cache memory를 가진다. cache memory에 저장된 데이터를 가지고 연산작업 하는게 더 빠르다. 항상 main memory에서 가져오는게 아니라 cache memory에서 가져와서 연산작업을 할 수 있기 때문에 main memory에 반드시 저장하지 않는 경우도 있다. 이때 동일한 변수 A가 cache memory != main memory(실시간 업데이트 안해줘서)일 수 있다. 그래서 데이터 불일치가 발생할 수 있다. -\u0026gt; 가시성이 확보되지 않아서 발생하는 문제라고 본다. 가시성은 volatile 강의 챕터에서 자세히 다룹니다 그 외 특징 # sleep() 을 실행한 스레드는 동기화 영역에서 대기 중이더라도 획득한 락을 놓거나 해제하지 않는다 대기하더라도 해당 스레드가 lock을 획득한 상태여도 lock을 반납하거나 해제하지 않는다. 계속 가지고 있는다. wait() : 이때는 lock을 반납한다. 다른 쓰레드가 lock을 획득해서 본인의 wait 상태를 깨워줄 수 있다. synchronized 의 동기화 영역에 진입하지 못하고 대기 중인 스레드는 인터럽트 되지 않는다 synchronized 의 동기화 영역에 진입하지 못하고 대기 중인 스레드가 다시 경쟁해서 모니터를 획득하는 것은 순서가 정해져 있지 않다 = 비공정성 모니터를 계속 획득하지 못하는 기아 상태의 스레드가 나올 수도 있지만 스케줄러가 적절하게 조절한다 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":31,"href":"/docs/parallel_programming/029_method_synchronized/","title":"029 Method Synchronized","section":"Parallel Programming","content":" 강의 메모 - synchronized 메서드 동기화 # 인스턴스 메서드 동기화 (synchronized method) # 인스턴스 단위로 모니터가 동작하며 동일한 인스턴스 안에서 synchronized 가 적용된 곳은 하나의 락을 공유한다 인스턴스가 여러개일 경우 인스턴스별로 모니터 객체를 가지므로 스레드는 모니터 별로 락을 획득해서 동기화 영역에 진입하고 빠져 나올 때 락을 해제 할 수 있다 MyClass 내부적으로 가지고있는 객체 타입 : this (=모니터) 위 두 메서드의 모니터가 동일하다. 정적 메서드 동기화 (static synchronized method) # 클래스 단위로 모니터가 동작하며 synchronized 가 적용된 곳은 하나의 락을 공유한다 인스턴스와는 별개의 모니터를 가지고 임계 영역을 동기화 하기 때문에 인스턴스 단위로 메서드를 호출할지라도 락은 클래스 단위로 스레드간 공유된다 클래스는 메모리에 오직 하나만 존재하므로 하나의 모니터를 공유해서 동기화 하고자 할 때 사용 할 수 있다 정적메서드이기 때문에 클래스 타입으로 주어짐 -\u0026gt; MyCalss = monitor 인스턴스 메서드 동기화 (synchronized method) + 정적 메서드 동기화 (static synchronized method) # synchronized method 와 static synchronized method 가 혼용되었을 경우는 각 모니터별로 동기화를 진행한다 모니터가 섞여 있기 때문에 동기화가 의도한대로 정확하게 동작하는지 주의가 필요하다 위 1, 2 메서드의 모니터는 같다 위 3,4 메서드의 모니터는 같다 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":32,"href":"/docs/kafka/004_kafka_broker/","title":"004 Kafka Broker","section":"Kafka","content":" 강의 메모 # 카프카 브로커, 클러스터, 주키퍼 # 주키퍼 # 카프카 클러스터를 운영하기 위해 반드시 필요한 애플리케이션 카프카 2.0 버전 까지는 반드시 필요했지만 카프카 3.0부터는 주키퍼가 없더라도 운영됨 아직까지는 주키퍼를 완벽하게 대체하지 못하므로 주키퍼가 있는 카프카 클러스터 운영 기업이 많음\n1개의 브로커는 하나의 서버나 인스턴스에서 동작 브로커 3개가 있는 모습\n브로커 # 데이터를 분산 저장하여 장애가 발생하더라도 안전하게 사용할 수 있도록 도와주는 애플리케이션 하나의 서버에는 하나의 카프카 브로커 프로세스가 실행됨 3대 이상의 브로커 서버를 1개의 클러스터로 묶어서 운영한다 카프카 클러스터로 묶인 브로커들은 프로듀서가 보낸 데이터를 안전하게 분산 저장하고 복제하는 역할을 수행한다.\n주키퍼 앙상블 # 카프카 클러스터를 실행하기 위해서는 주키퍼가 필요 주키퍼의 서로 다른 znode에 클러스터를 지정 roor znode에 각 클러스터별 znode를 생성하고 클러스터 실행시 root가 아닌 하위 znode로 설정 각 클러스터별로 운영 가능 하나의 클러스터별로 주키퍼를 별도로 가져갈 수 있지만, 리소스 낭비가 발생할 수도 있음 여러개의 카프카 클러스터를 동시에 운영하는 경우도 있다. ex) 도메인별 등 브로커의 역할 - 컨트롤러, 데이터 삭제 # 컨트롤러 # 다수 브로커 중 1대가 컨트롤러 역할함\n다른 브로커들의 상태 체크 이슈가 발생하면 해당 브로커에 존재하는 리더 파티션을 재분배 여러대의 브로커 중 이슈가 발생했을때 장애가 발생한 브로커를 제외하고, 여기에 원래 있던 리더 파티션을 재분배 카프카는 지속적으로 데이터를 처리해야하기 때문에 비정상인 브로커를 빠르게 클러스터에서 제외시켜야한다 리더 파티션을 빠르게 팔로워 파티션이 승급받아서 다시 정상 운영되어야함 다른 브로커가 컨트롤러 역할을 하고 다시 리더 파티션의 데이터를 분배한다 데이터 삭제 # 브로커가 로그 세그먼트(log segment) 단위로 데이터를 삭제한다. 일정시간, 용량에 따라서 데이터 삭제 수행 compact 옵션으로 특수한 상황에서 가장 최신의 key가 있는 레코드를 제외하고 나머지 레코드들을 삭제 수행할 수 있음\n컨슈머 오프셋 저장 # 컨슈머 그룹은 토픽이 특정 파티션으로부터 데이터를 가져가서 처리하고, 이 파티션의 어느 레코드까지 가져갔는지 확인하기 위해 오프셋을 커밋한다 _consumer_offsets 토픽에 저장 (internal 토픽) 여기에 저장된 오프셋을 토대로 컨슈머 그룹은 다음 레코드를 가져가서 처리한다.\n그룹 코디네이터 # 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할을 한다. 컨슈머가 컨슈머 그룹에서 빠지면 매칭되지 않은 파티션을 정상 동작하는 컨슈머로 할당하여 끊임없이 데이터가 처리되도록 도와준다. 문제가 있는 컨슈머를 삭제하고 다시 파티션을 컨슈머로 재할당하는 과정 : 리밸런드(rebalance)\n데이터의 저장 # conifg/server.properties의 log.dir 옵션에 정의한 디렉토리에 데이터를 저장 (파일 시스템) 토픽 이름과 파티션 번호의 조합으로 하위 디렉토리를 생성하여 데이터를 저장\nReferences 강의 : https://www.inflearn.com/course/%EC%95%84%ED%8C%8C%EC%B9%98-%EC%B9%B4%ED%94%84%EC%B9%B4-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D "},{"id":33,"href":"/docs/kafka/005_broker_log_segment/","title":"005 Broker Log Segment","section":"Kafka","content":" 강의 메모 # 로그와 세그먼트 # log.segment.bytes : 바이트 단위의 최대 세그먼트 크기 지정 (기본 값은 1GB)\nlog.roll.ms(hours) : 세그먼트가 신규 생성된 이후 다음 파일로 넘어가는 시간 주기 (기본값은 7일)\n프로듀서가 데이터를 전송하면 active 되어있는 .log 파일에 offset 22번으로 들어간다. (FIFO)\n가장 최초의 offset 번호가 파일 이름이 된다\n가장 마지막 세그먼트 파일을 active segment라고 한다.\n브로커의 삭제 대상에서 포함되지 않는다. retention 옵션에 ㄸ라 삭제 대상으로 지정된다. 세그먼트와 삭제 주기 (cleanup.policy=delete) # retention.ms(minutes, hours) : 세그먼트를 보유할 최대 기간 (기본값 7일) retention.bytes : 파티션 당 로그 적재 바이트 값 (기본값 -1; 지정하지않음) 기간과 바이트 중 운영하면서 디스크, 들어오는 데이터 용량에 맞게 수정해야한다 log.retention.check.interval.ms : 세그먼트가 삭제 영역에 들어왔는지 확인하는 간격 (기본값 5분) 파일 단위로 삭제되기 때문에, 로그 단위로 개별 삭제는 불가능하다 이미 적재된 데이터에 대해서 수정 불가능 -\u0026gt; 데이터 적재(프로듀서)/사용(컨슈머)할때 데이터를 검증하는 것이 좋다 validation 별도 지원은 없으므로 send() 보내기 전에 로직에서 처리를 하던지, 컨슈머 입장에서 받아올때 로직에서 처리하던지 해야할듯 cleanup.policy=compact # 압축(compression)과는 다른 개념 토픽 압축 정책 : 메시지 키 별로 해당 메시지 키의 레코드 중 오래된 데이터를 삭제하는 정책 compact 로직에 의해서 어떤 세그먼트 단위로 삭제가 전체 통으로 일어나는게 아니라 message key 단위로 가장 최근에 남아져있는 것만 메시지 키만 남기고 삭제한다. 일부 레코드만 삭제가 될 수 있다. 압축은 active segment를 제외한 데이터가 대상이다. 13(K3)이 16(K3)보다 오래된 데이터이므로 삭제되고, 10(K1), 12(K1), 15(K1) 중에는 15번이 남겨지겠다. 나머지는 삭제(10, 12) 테일/헤드 영역, 클린/더티 로그 # 테일 영역 압축 정책에 의해 압축이 완료된 레코드들 클린(clean) 로그 라고도 부른다 중복 메시지 키가 없다 헤드 영역 압축 정책이 되기 전 레코드들 더티(dirty) 로그 라고도 부른다. 중복된 메시지 키가 있다. min.cleanable.dirty.ratio # 액티브 세그먼트를 제외한 세그먼트에 남아있는 테일 영역의 레코드 개수와 헤드 영역의 레코드의 비율을 뜻한다 ex) 0.5로 설정 : 테일 영역의 레코드 개수가 헤드 영역의 레코드 개수와 동일할 경우 압축 실행 0.9 : 크게 설정하면, 한번 압축을 할때 많은 데이터가 줄어들므로 압축 효과가 좋지만, 이 비율이 될때까지 용량을 차지하므로 용량 효율이 좋지 않다. 0.1 : 작게 설정하면, 압축이 자주 일어나서 가장 최신 데이터만 유지할 수 있지만 압축이 자주 발생하기 때문에 브로커에 부담을 줄 수 있다.\nReferences 강의 : https://www.inflearn.com/course/%EC%95%84%ED%8C%8C%EC%B9%98-%EC%B9%B4%ED%94%84%EC%B9%B4-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D "},{"id":34,"href":"/docs/kafka/003_transactional_outbox_pattern/","title":"003 Transactional Outbox Pattern","section":"Kafka","content":" 기술 블로그 정리 # Transactional Outbox 패턴이 등장하게된 상황 # Event Driven Architecture \u0026gt; 메시지 발행의 신뢰성 보장 패턴 Event driven ARchitecture : Message Broker를 이용해 다양한 메시지(이벤트)를 publish(발행) 하고, 그에 연관된 작업을 비동기적으로 처리하여 시스템을 통합 DB 트랜잭션을 실행한 뒤, 연관 메시지를 Message Broker에 publish 하게 되는데, 메시지 publish가 반드시 완료되어야하는 경우가 있다. 예시) 리디 주문 기능; 주문이 발생하면 사용한 캐시\u0026amp;포인트 금액을 차감하고 상품을 지급하며 주문 완료로 상태를 바꾸는 DB 트랜잭션이 발생 -\u0026gt; Message Broker에 주문 완료 메시지를 publish 이 경우, DB와 Message Broker의 트랜잭션 원자성 보장이 어렵다. DB상 주문완료 처리가 되었떠라도 Message Broker에 메시지를 publish 하는데 실패할 수 있고, DB의 주문 완료 처리를 rollback 하기도 어렵다. Transactional Outbox 패턴을 도입한 배경 # 예시) 리디 서비스 리디 서비스는 Kafka를 중심으로 통합되고있다. 그리고 데이터 영속화를 위해서 MySQL을 사용한다. 주로 MySQL을 이용해 비즈니스 로직을 처리하고, 비동기적인 처리를 위해 메시지를 Kafka에 publish 한다.\n[Kafka 도입 초기] DB 트랜잭션 완료 -\u0026gt; kafka 메시지 publish -\u0026gt; 실패한 메시지를 dead-letter-queue DB 테이블에 저장 -\u0026gt; 별도 batch process에서 retry 수행\n한계상황 DB 트랜잭션과 메시지를 publish를 원자적으로 처리할 수 없기 때문에, DB 트랜잭션이 완료되어도 메시지 publish를 보장할 수 없다. 일정 시간 간격으로 batch process가 실행되면서 retry 하므로 실시간성이 없고, 신속하지 못하다. 메시지간 publish 순서가 중요한 경우, 토픽의 경우 retry 되는 메시지가 늦게 publish 되면서 기대했던 순서가 뒤바뀔 수 있다. Transactional Outbobx 패턴 구현 # Polling Publisher # Outbox DB 테이블에 polling 하는 것만으로 publish 할 메시지를 가져올 수 있다. DB 트랜잭션이 실행되는 시점부터 publish 할 메시지 정보를 각 비즈니스 로직에서 생성하여 Outbox DB 테이블에 저장하기 때문 장점 전반적인 구조가 단순해서 구현하기 간편 단점 높은 비용의 polling이 DB 부하로 이어질 수 있음 Transaction Log Tailing # DB 트랜잭션 로그(커밋 로그)를 테일링(tailing)하는 방법\n애플리케이션에서 커밋된 업데이트는 각 DB 트랜잭션 로그 항목(log entry)으로 남는다.\n트랜잭션 로그 마이너로 트랜잭션 로그를 읽어 변경분을 하나씩 메시지로 메시지 브로커에 발행하는 방법\n해당 log에 대한 CDC(Change Data Capture)를 구현 단점\nMySQL binlog에 대해 CDC를 구현하고 그 결과를 바탕으로 kafka consumer에서 사용하는 메시지 포맷으로 데이터를 생성하는 작업이 필요 구현 방법 선택 # Polling Publisher Message Relay 구현 실제 구현 # 테이블 : message Message Broker로 publish할 메시지 정보를 저장 테이블 : processed_message publish 되거나 publish가 불가능해서 skip하려는 message id를 저장 처리 완료된 row를 임시로 저장해두었다가 삭제하는 방식 사용 용도 publish 완료된 메시지를 바로 삭제하지 않고 processed_message 테이블에 저장해두었다가 나중에 삭제하도록 수정 publish할 데이터를 가져올때 message 테이블에는 있지만, processed_message 테이블에는 없는 데이터를 가져와야 하기 때문에 LEFT JOIN을 해야한다. row가 적을수록 SELECT 쿼리 성능이 좋아지므로 processed_message 테이블의 row 개수를 일정한 수준을 유지해야한다. (삭제 주기를 그에 맞게 정해야할 것 같다) CREATE TABLE `message` ( `id` bigint NOT NULL AUTO_INCREMENT, `topic` varchar NOT NULL, `type` varchar NOT NULL, `key` varchar DEFAULT NULL, `payload` longtext NOT NULL, `source` varchar NOT NULL, `created_at` datetime NOT NULL DEFAULT current_timestamp(), `updated_at` datetime NOT NULL DEFAULT current_timestamp() ON UPDATE current_timestamp(), PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; CREATE TABLE `processed_message` ( `id` bigint NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; References https://ridicorp.com/story/transactional-outbox-pattern-ridi/ "},{"id":35,"href":"/docs/parallel_programming/027_spin_lock/","title":"027 Spin Lock","section":"Parallel Programming","content":" 강의 메모 - SpinLock \u0026amp; Busy Waiting # 개요 # 스핀락(SpinLock)은 뮤텍스나 세마포어와 같은 동기화 기법의 일종으로, 기다리지 않고 스레드가 임계영역을 사용할 수 있을 때까지 계속 반복하여 검사하는 동기화 메커니즘이다 작동방식 # 스레드가 공유 자원에 접근하려고 할 때, 먼저 스핀락을 시도한다 test_and_set() 함수가 이전 락 값인 0 을 반환하면 아직 락이 잠기지 않았다는 것을 의미하며 while 루프를 빠져 나온다 스레드는 스핀락을 얻고 해당 자원을 사용한다 그러나 이전 락 값이 1이면 이미 다른 스레드에 의해 잠긴 것을 의미하며 스핀락을 얻을 때까지 계속해서 반복적으로 검사를 수행한다 스레드가 자원 사용이 끝나면 lock 을 0 으로 변경해서 스핀락을 해제한다 Busy Waiting (바쁜 대기) # Busy waiting은 스레드가 어떤 조건이 만족될 때까지 계속해서 반복적으로 검사하는 것을 말한다 스레드가 특정 조건을 기다리는 동안 아무런 유용한 작업을 수행하지 않고, 무한 반복 루프를 돌며 CPU 자원을 계속 사용하는 것을 의미한다 스핀락은 이러한 busy waiting을 사용하는 동기화 기법 중 하나이다 스핀락 자바 구현 # 임계영역을 수행하고 unlock() 호출 스핀락의 장점, 단점 # 장점\n컨텍스트 스위칭 비용 감소 스핀락은 뮤텍스(Mutex)나 세마포어(Semaphore)와 같은 블로킹 기반의 동기화 기법과 달리, 스레드가 공유 자원을 얻을 때까지 블로킹하지 않고 반복적으로 검사(busy waiting)한다. 따라서 스핀락은 컨텍스트 스위칭 비용을 감소시키고, 빠른 공유 자원 접근을 가능하게 한다 대기 시간 감소: 스핀락은 블로킹 대기 없이 바로 공유 자원에 접근하려고 시도하기 때문에, 컨텍스트 스위칭 하는 시간 보다 임계영역의 대기 시간이 더 짧을 때 유리하다 단점\n무한 루프로 인한 CPU 리소스 낭비 스핀락은 공유 자원이 사용 중일 때 무한 루프를 돌면서 계속해서 검사하므로, 다른 스레드가 공유 자원을 해제하지 않는 경우에는 busy waiting으로 인해 CPU 리소스가 낭비될 수 있다 스핀락은 공유 자원에 대한 경쟁이 많은 경우, 또는 대기 시간이 긴 경우에는 비효율적이며 스레드들이 공유 자원에 대한 경쟁이 강하게 발생할 경우 스핀락을 사용하면 대기 시간이 더 길어질 수 있다 싱글 코어 \u0026amp; 멀티 코어\n싱글코어에서 스핀락을 사용하면, 해당 스레드가 무한 루프를 돌면서 다른 스레드가 CPU를 점유할 기회를 주지 않기 때문에 싱글코어 환경에서는 일반적으로 busy waiting으로 인해 성능이 저하될 수 있으므로 멀티코어 환경에서 사용하는 것이 더 효율적이다 ※ 스핀락은 멀티 코어 환경에 상관없이 대기 시간이 긴 경우나 공유 자원에 대한 경쟁이 많은 경우에는 다른 동기화 기법을 고려하는 것이 좋다 대부분의 경우, 스핀락보다는 뮤텍스(Mutex)나 세마포어(Semaphore) 등의 블로킹 기반의 동기화 기법을 사용하는 것이 더 적합할 수 있다.\nReferences 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":36,"href":"/docs/parallel_programming/028_synchronized_basic/","title":"028 Synchronized Basic","section":"Parallel Programming","content":" 강의 메모 - synchronized 기본 # 개요 # 자바는 단일 연산 특성을 보장하기 위해 synchronized 키워드를 제공하고 있으며 synchronized 구문을 통해 모니터 영역을 동기화 할수 있다 synchronized 는 명시적으로 락을 구현하는 것이 아닌 자바에 내장된 락으로서 이를 암묵적인 락(Intrinsic Lock) 혹은 모니터락 (Monitor Lock) 이라고 한다 자바에 내장된 락 (암묵적인 락, 모니터락) synchronized 은 동일한 모니터를 가진 객체에 대해 오직 하나의 스레드만 임계영역에 접근할 수 있도록 보장하며 모니터의 조건 변수를 통해 스레드간 협력으로 동기화를 보장해 준다 synchronized 가 적용된 한 개의 메서드만 호출해도 같은 모니터의 모든 synchronized 메서드까지 락에 잠기게 되어 락이 해제될 때 까지는 접근이 안되는 특징을 가지고 있다 앞에서 배운 \u0026lsquo;모니터\u0026rsquo;를 synchronized 키워드에 넣었으므로 모니터 장점을 가지고있음 락은 스레드가 synchronized 블록에 들어가기 전에 자동 확보되며 정상적이든 비정상적이든 예외가 발생해서든 해당 블록을 벗어날 때 자동으로 해제된다 lock은 자동 확보/자동 해제 synchronized 는 모니터 락을 사용하여 동기화 할 수 있는 4가지 방법을 제공한다 # method synchronized method static synchronized method block synchronized block static synchronized method 원리는 동일 세부적으로는 인스턴스냐, 클래스냐 (method, block 각각 안에서도 세부적으로 나누는 기준) A 메서드 안에 4가지(위 구분 4가지)를 정의했다고 해보자. 몇개의 다른 모니터가 있을까? A 라는 클래스 안에 키워드(this)가 인스턴스를 참조하는 키워드다. 메서드 방식 첫번째는 this 라는 객체의 모니터를 가지고있는 것이다. 두번째는 static이 있으므로 A 자체의 모니터를 가지고 있는 것이다. 위 2개의 모니터가 다른 것이다. block 방식 세번째는 this 위 첫번째 모니터와 동일한 모니터가 되는 것이다. 네번째는 A 자체 위 두번째 모니터링과 동일한 모니터가 되는 것이다. 첫번째-세번째, 두번째-네번째 모니터는 같다. 그러므로 다른 모니터는 2개가 있는 것이다. 모니터 안에 뮤텍스가 있음 T1이 첫번재 메서드의 모니터 lock을 얻었을때 T2가 두번째 메서드의 모니터 락을 얻을 수 있다. (서로 다른 모니터이기 때문에) T1, T2가 동시에 첫번째 메서드 수행이 불가능한것 (T1이 이미 lock을 소지하고있기 때문에) T1이 첫번쨰 메서드 lock을 가지고있을때 T2가 세번째 메서드 접근 불가능 (첫번째, 메서드 세번째 메서드가 모니터 가 같기 때문) 결국 모니터가 같으면 같은 모니터를 가진 메서드 접근이 안되는거임 만약에 B 클래스에서 인스턴스를 생성했을때 세번째 메서드에 b를 넣었고, 네번째에 B.class 를 넣으면 위에 첫번째 두번째가 A 클래스 기준일떄 서로 다른 모니터를 가지고있께되어 각 스레드가 접근 가능하다. 모니터와의 관련성 # A 클래스가 있고, A 클래스의 인스턴스 생성, 모니터의 특징 중에서 모든 객체는 모니터를 가진다고 했는데 모든 객체를 세부적으로 나누면 A라는 클래스 타입의 객체, A를 통해서 생성한 인스턴스 객체 2개로 나눠질 수 있다. 2개로 나눴다는 것은 A클래스 타입의 객체와 A 인스턴스는 각각 다른 모니터를 가지는것이다. 이렇게 각각 다른 모니터를 가지는 이유는 자바에서 이렇게 설계했다. 이게 모니터의 기준 (A 클래스 기준으로 모니터를 2개 가질 수 있다는 것이다.) synchronized 동기화 방식 # 메서드 동기화 방식 - synchronized method # 메소드 전체가 임계 영역(critical section)이 된다. 즉, 메소드 내의 모든 코드가 동기화 된다 동시성 문제를 한번에 편리하게 제어할 수 있는 장점은 있으나 메서드 내 코드의 세부적인 동기화 구조를 가지기 어렵다 메서드 전체를 동기화하기 때문에 동기화 영역이 클 경우 성능저하를 가져온다 인스턴스 메서드 동기화 와 정적 메서드 동기화 방식이 있다 블록 동기화 방식 - synchronized blcok # 특정 블록을 정해서 임계 영역(critical section)을 구성한다. 즉 블록 내의 코드만 동기화 된다 메서드 동기화 방식에 비해 좀 더 세부적으로 임계영역을 정해서 필요한 블록만 동기화 구조를 가질 수 있다 메서드 전체를 동기화 하는 것보다 동기화 영역이 작고 효율적인 구성이 가능하기 때문에 성능 저하가 덜하다 인스턴스 블록 동기화 와 정적 블록 동기화 방식이 있다 스레드 간 객체의 메서드를 동기화하기 위해서는 스레드는 같은 객체의 모니터를 참조하고 있어야 한다.\nReferences 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":37,"href":"/docs/parallel_programming/026_monitor/","title":"026 Monitor","section":"Parallel Programming","content":" 강의 메모 - Monitor - 모니터 -1,2 # 개요 # 자바가 동기화를 지원하기 위해 사용하는 메커니즘은 모니터(Monitor) 이며 뮤텍스나 세마포어보다 더 고수준의 동기화 기법이다 뮤텍스, 세마포어를 좀더 추상화한것 모든 자바 객체는 기본적으로 모니터를 가지며 여러 스레드가 객체의 임계 영역(critical section)에 진입하려고 할 때 JVM 은 모니터를 사용하여 스레드 간 동기화를 제공한다 자바의 모니터는 상호 배제(Mutual Exclusion) 및 협력(Cooperation)이라는 두 가지 동기화 기능을 제공하고 있으며 이를 위해 뮤텍스와 조건변수(Condition Variable)를 사용한다 상호배제 (Mutual Exclusion) # 객체가 가지고 있는 모니터 Lock 을 통해 여러 스레드가 동시에 공유 자원에 접근하는 것을 막아 데이터의 일관성과 안전성을 보장하는 메커니즘이다 JVM 은 \u0026lsquo;synchronized\u0026rsquo; 키워드를 이용하여 뮤텍스 동기화를 암묵적으로 처리해 주고 있으며 synchronized 는 메서드나 코드 블록에 적용할 수 있다. 내부적으로 보이지않게 처리 synchrnozied 키워드 안에서 내부적으로 처리하는것 synchronized (동기화) synchronized 블록은 해당 객체의 모니터를 획득 할 수 있으며 모니터를 획득한 스레드만이 임계영역에 접근 가능하고 그 외 다른 스레드들은 차단되어 대기 상태가 된다 synchronized 블록을 빠져 나오면 모니터 Lock 이 해제되고 대기 중인 다른 스레드 중 하나가 락을 얻고 임계 영역에 진입하여 작업을 수행하는 식으로 상호배제가 보장된다 협력 (Cooperation) # 협력은 모니터의 Condition Variable (조건 변수) 를 통해 스레드 간 공동의 목표를 위해 상호협력으로 데이터의 일관성과 안전성을 보장하는 동기화 메커니즘이다 Condition Variable (조건변수) 조건변수는 Object 클래스의 메소드인 wait(), notify(), notifyAll()과 함께 작용하며 특정 조건이 만족될 때까지 스레드를 대기시키는 기능을 제공한다 스레드가 특정 조건에 부합하지 않을 때 wait() 메소드를 호출하면 조건변수의 대기 셋(Wait Set)에 들어가 대기한다 다른 스레드가 특정 조건을 만족해서 notify() 또는 notifyAll() 메소드를 호출하면 해당 조건변수의 대기셋으로부터 스레드들을 깨워 실행시키게 된다. 조건변수를 통해 스레드 간 대기와 통지를 서로 조절하면서 경쟁 조건(race condition)과 같은 문제를 방지할 수 있다. 모니터 내부에는 여러개의 조건 변수를 가질 수 있지만 자바의 모니터에는 오직 한 개의 조건 변수만 가질 수 있다 t1이 대기하고있고, t2가 t1을 notify 할때의 실행영역의 조건 변수가 있고, 다른 t3은 조건변수가 또 있겠고, 조건 변수가 많아져서 선택될수록 구분해서 사용할 수 있겠지만, 모니터에는 1개의 조건 변수의 대기 큐에 다 들어가게된다. 그래서 일단 모든 쓰레드를 깨우고 실행영역을 수행하는 쓰레드가 있다. 모니터 대기 세트 구조 # 자바의 모니터 내부에는 EntrySet(진입셋) 과 WaitSet(대기셋) 이라는 대기 자료 구조가 있으며 이들은 멀티스레드 환경에서 스레드들 간의 상호작용을 조절하는 데 사용된다\nEntry Set\nEntry Set 은 모니터의 Lock 을 획득하기 위해 대기 중인 스레드들을 모아 놓은 자료 구조로서 스레드가 Lock 을 사용 중인 경우 그외 다른 스레드는 Entry Set 에 들어가게 된다 Entry Set 에 있는 스레드들은 Lock 이 반납될 때까지 기다리며 락이 반납되면 Entry Set 중 하나의 스레드가 락을 획득하고 임계 영역으로 진입하게 된다 Wait Set\nWait Set 은 모니터의 조건 변수(Condition variable)와 함께 사용하는 자료구조이며 스레드들이 특정한 조건이 만족할 때 까지 대기하고 있는 장소이다 스레드는 Wait Set 에 들어가 대기할 때 Lock 을 해제한다. 그리고 다른 스레드에 의해 깨어나게 되면 Entry Set 으로 이동해서 다시 Lock 을 획득 할 수 있다 누군가가 깨워줘야한다. 조건 변수 종류 # 조건변수를 통해 상호 협력하고 있는 두 스레드가 wait() 과 notify() 메서드 실행 후에 하나의 모니터를 두고 두 스레드 모두 소유가 가능한 상황이 발생한다 하나는 대기중인 스레드, 하나는 깨우는 스레드로서 어떤 스레드가 모니터를 먼저 소유할 것인가에 따라 두 종류의 조건변수로 나눌 수 있는데 Signal and Wait 와 Signal and Continue 이다 Signal and Wait # 현재 모니터를 소유하고 있는 스레드가 wait() 을 실행하면 모니터 내부에서 자신을 일시 중단하고 Lock 을 해제한 후 Wait Set 에 들어간다 깨우는 스레드가 notify() or notifyAll() 명령을 실행하면 Wait Set 에 있는 대기 스레드 중 하나 또는 모든 스레드를 깨우고 깨우는 스레드는 Lock 을 해제하고 대기한다 대기에서 깨어난 스레드가 Lock 을 획득한 후 모든 작업을 마치고 Lock 을 해제하면 깨운 스레드가 Lock 을 획득한 후 계속 작업을 진행한다 대기 스레드와 깨운 스레드 사이에 다른 스레드가 모니터를 소유할 수 없도록 원자적 실행이 보장되어야 한다 깨운 스레드가 락을 반납하고 대기하고있고, 대기 스레드가 수행 이후 깨운 스레드가 다시 락을 얻는다. 이 사이에 락을 다른 스레드가 가져갈 수 없다는것 Signal and Continue # 현재 모니터를 소유하고 있는 스레드가 wait() 을 실행하면 모니터 내부에서 자신을 일시 중단하고 Lock 을 해제한 후 Wait Set 에 들어간다 깨우는 스레드가 notify() or notifyAll() 명령을 실행하면 Wait Set 에 있는 대기 스레드 중 하나 또는 모든 스레드를 깨운다. 이때 일어난 스레드들은 Entry Set 으로 이동한다 깨우는 스레드는 Lock 을 계속 유지하면서 모든 작업을 완료하고 Lock 을 해제하면 Entry Set 에 대기하고 있는 모든 스레드가 Lock 을 획득하기 위해 경쟁한다 자바에서는 이 조건 변수 형식을 취하고있다 자바 모니터 동작 구조 # 스레드가 모니터 영역에 진입하기 위해 synchronized 메서드를 호출하면 모니터가 작동한다 스레드는 모니터 영역 진입을 위해 Entry Set 에 입장해서 모니터 Lock 을 획득하기 위해 시도한다 Entry Set 에서 이미 대기하고 있거나 현재 모니터를 소유한 스레드가 없으면 즉시 모니터의 소유자 되어 모니터 영역에 진입한다 만약 다른 스레드가 이미 모니터를 소유한 상태이면 모니터 영역에 진입하지 못하고 Entry Set 으로 들어가 대기한다 Wait Set 에서 대기하는 스레드는 모니터 스레드가 모니터를 해제할 때 까지 계속 대기 상태를 유지한다 모니터 스레드가 모니터를 해제할 수 있는 경우는 두 가지인데 실행 중인 모니터 영역을 완료하거나 wait 명령을 실행하는 것이다 (해당 스레드는 lock을 갖고있겠다) 모니터 스레드가 어떤 조건에 부합하지 않아서 wait() 를 실행하면 모니터가 해제되고 Wait Set 에 들어가 대기한다 (락 반납) 이 때 다른 스레드가 모니터를 소유한 상태에서 만약 notify 를 실행하지 않고 그냥 종료하는 경우 Wait Set 의 스레드가 깨어나지 않기 때문에 Entry Set 의 스레드만 모니터를 두고 경쟁하게 된다 만약 모니터 스레드가 notify 를 실행하게 되면 Wait Set 에서 대기하고 있는 모든 스레드를 깨운다 깨어난 스레드는 모두 Entry Set 으로 이동하게 된다. 하지만 스레드가 깨어나는 즉시 모니터를 소유하는 것이 아니다 모니터 스레드는 모니터를 계속 소유한 상태에서 모니터 영역을 실행하고 종료한다 이 때 Entry Set 과 Wait Set 에서 이동한 모든 스레드가 모니터를 두고 경쟁하게 되는데 만약 조건 변수 대기에서 깨어난 스레드 중에서 모니터를 소유했는데 그 시점에 또 조건이 맞지 않으면 다시 wait 를 실행하고 대기 상태로 들어갈 수 있다 Entry Set 과 Wait Set 에서 다음 스레드를 선택하는 기준은 오직 스케줄러에 의해 결정된다 모니터 구현 # ※ 일반적으로 모니터는 조건 변수를 여러개 가질 수 있으나 자바에서는 모니터당 오직 한개의 조건 변수를 가질 수 있다 (Lock 동기화 클래스에서 이 부분을 개선함)\nReferences 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":38,"href":"/docs/etc/004_sonarQube/","title":"004 Sonar Qube","section":"Etc","content":" 기술 블로그 정리 # 코드 분석 도구 적용기 - 3편, SonarQube 적용하기 # SonarQube란? # 정적 코드 분석 도구 # 정적 프로그램 분석(static program analysis) : 실제 실행 없이 컴퓨터 소프트웨어를 분석하는 것 정적 분석은 코드의 모든 부분을 확인할 수 있지만, 실행 환경에서의 상태를 정확히 알 수 없기 때문에 실행할 때에만 알 수 있는 데이터가 필요한 경우 정확히 분석하기 어렵다. SonarQube # 20개 이상의 프로그래밍 언어에서 버그, 코드 스멜, 보안 취약점을 발견할 목적으로 정적 코드 분석으로 자동 리뷰를 수행하기 위한 지속적인 코드 품질 검사용 오픈 소스 플랫폼 소나소스(SonarSource)가 개발 소나큐브는 중복 코드, 코딩 표준, 유닛 테스트, 코드 커버리지, 코드 복잡도, 주석, 버그 및 보안 취약점의 보고서를 제공 정적 코드 분석 도구 중 하나로, 레퍼런스가 많고, Github 또는 Jenkins와의 연동을 통해 자동 정적 코드 분석을 구성할 수 있다. SonarQube 장점 # 지속적인 인스펙션 지속적인 통합과 같이 빌드와 연동하여 지속적으로 코드에 대한 인스펙션을 수행 품질 중앙화 개발된 조직의 코드의 품질을 중앙 저장소에서 가시화하고 단일 위치에서 관리 DevOps와의 통합 다양한 빌드 시스템, CI 엔진과 통합되어 DevOps 실천을 지원 품질 요구사항 설정 품질 게이트를 통해 표준화된 코드 품질 요구사항을 설정 다중 언어 분석 20개가 넘는 프로그램 언어에 대한 코드 분석을 지원 플러그인을 통한 확장 다수의 플러그인을 통해 SonarQube의 기능을 확장가능 오픈소스 프로젝트 오픈소스 프로젝트로 특정 범위까지 무료로 사용 가능 SonarQube 적용모습 # SonarQube 사용예시 # 저희 프로젝트의 경우 PR을 통해 기능을 구현하고, 이를 develop에 merge하는(그리고 develop을 master로 merge하는) 전형적인 git-flow 브랜치 전략을 사용 따라서 PR이 생성되는 시점(브랜치가 새로 생성 \u0026amp; 새로운 커밋, 푸쉬), develop 또는 master에 머지되는 시점에 SonarQube analysis Stage가 실행\nSornarQube에서 관리해주는 소프트웨어 품질 # Code Smell : 심각한 이슈는 아니지만 베스트 프렉티스에서 사소한 이슈들로 모듈성(modularity), 이해가능성(understandability), 변경가능성(changeability), 테스트용의성(testability), 재사용성(reusability) 등이 포함 Bugs : 일반적으로 잠재적인 버그 혹은 실행시간에 예상되는 동작을 하지 않는 코드를 표시 Vulnerabilities : 해커들에게 잠재적인 약점이 될 수 있는 보안상의 이슈를 말한다. SQL 인젝션, 크로스 사이트 스크립팅과 같은 보안 취약성을 발견 Duplications : 코드 중복은 코드의 품질을 저해시키는 가장 큰 요인 중 하나 Unit Test : 단위테스트 커버리지를 통해 단위 테스트의 수행 정도와 수행한 테스트의 성공/실패 정보를 제공 Complexity : 코드의 순환 복잡도, 인지 복잡도를 측정 Size : 소스코드 사이즈와 관련된 다양한 지표를 제공 References https://seller-lee.github.io/static-code-analysis-part3 "},{"id":39,"href":"/docs/etc/005_rabbitMq/","title":"005 Rabbit Mq","section":"Etc","content":" 기술 블로그 정리 # RabbitMQ란? # RabbitMQ란? # AMQP를 따르는 오픈소스 메시지 브로커 메시지를 많은 사용자에게 전달하거나, 요청에 대한 처리 시간이 길 때, 해당 요청을 다른 API에게 위임하고 빠른 응답을 할때 많이 사용한다. AMQP # Advanced Message Queueing Protocol MQ의 오픈소스에 기반한 표준 프로토콜을 의미한다.\nRabbitMQ 개념 # 1. Producer # 메시지를 생성하고 발송하는 주체 이 메시지가 Queue에 저장된다. Producer는 Queue에 직접 접근하지 않고, 항상 Exchange를 통해 접근하게 된다. 2. Exchange # Producer들에게서 전달받은 메시지들을 어떤 Queue들에게 발송할지를 결정하는 객체 4가지 타입이 있으며, 일종의 라우터 개념 3. Binding # Exchange에게 메시지를 라우팅 할 규칙을 지정하는 행위 특정 조건에 맞는 메시지를 특정 큐에 전송하도록 설정할 수 있는데, 이는 해당 Exchange 타입에 맞게 설정되어야한다. Exchange와 Queue는 m:n binding이 가능하다. 4. Queue # Producer들이 발송한 메시지들이 Consumer가 소비하기 전까지 보관되는 장소 같은 이름과 같은 설정으로 Queue를 생성하면 에러 없이 기존 Queue에 연결되지만, 같은 이름과 다른 설정으로 Queue를 생성하려고 시도하면 에러 발생한다. 5. Consumer # 메시지를 수신하는 주체 Queue에 직접 접근하여 메시지를 가져온다. Exchange 타입 # Direct : Routing key가 정확히 일치하는 Queue에 메시지 전송 Topic : Routing key 패턴이 일치하는 Queue에 메시지 전송 Headers : [key:value] 로 이루어진 header 값을 기준으로 일치하는 Queue에 메시지 전송 Fanout : 해당 Exchange에 등록된 모든 Queue에 메시지 전송 Direct # 라우팅 키를 이용하여 메시지를 라우팅하는데, 하나의 큐에 여러개의 라우팅 키를 지정할 수 있다. 각 메시지에 따라 C1, C2에 저장 여러 큐에 같은 라우팅 키를 지정하여 Fanout처럼 동작하게 할 수도 있다. Topic # 라우팅 키 패턴을 이용하여 메시지를 라우팅한다. example.orange.rabbit -\u0026gt; 메시지가 Q1, Q2에 모두 전달 example.orange.turtle -\u0026gt; 메시지가 Q1에 전달 lazy.grape.rabbit -\u0026gt; 메시지가 Q2에 한번만 전달 라우팅 패턴이 여러개 일치하더라도 하나의 큐에는 메시지가 한번만 전달된다. Headers # Topic Exchange와 유사하지만 라우팅을 위해 header를 쓴다는 차이점이 있다. Fanout # exchange에 등록된 모든 queue에 메시지를 전송한다. Message Queue 및 Message 보존 # RabbitMq server가 종료 후 재기동하면, 기본적으로 Queue는 모두 제거된다. 이를 막기 위해서는 Queue를 생성할때, Durable 옵션에 true를 주고 생성해야한다. Producer가 메시지를 발송할 때, PERSISTENT_TEXT_PLAIN 옵션을 주어야 메시지가 보존된다.\ndispatching # 만약 여러 소비자가 1개의 Queue를 바라보고 있다면 RabbitMQ에서는 Round-Robin 사용해 메시지를 균등하게 분배한다. 즉, 중복 처리를 방지하기 위해 첫번째 메시지는 소비자 1에게 전달하고, 두번째 메시지는 소비자1이 아닌 소비자2에게 전달한다. 소비자 뿐만 아니라 여러 생산자도 같은 Queue에 메시지를 전달할 수 있다. 이러한 특성으로 메시지를 받아 처리하는 프로그램의 수평 확장이 가능하다.\nFair dispatching # 만약 소비자는 2개만 존재하고 홀수번째의 메시지의 크기는 항상 크고 짝수번째 메시지는 항상 작다면, Round-Robin 알고리즘을 사용해서 메시지 분배를 해도 공평하게 소비자에게 전달되지 않는다. 이러한 이유로 지연이 발생한 소비자에는 메시지를 전달하지 않도록 prefetch count라는 개념을 사용한다.\nprefetch count # 1로 설정 : 소비자로부터 act을 받지 못한 메시지가 1개라도 있으면 해당 소비자에게 메시지를 전달하지 않는다. 즉, prefetch count는 소비자에게 동시에 전달되는 메시지의 양이다. 소비자 서버가 죽었을 경우 # Queue는 소비자에게 메시지를 전달한 후 ACK를 받았을때 해당 메시지를 dequeue한다. 소비자가 ACK를 Queue에 전달하지 못하는 경우는 메시지가 너무 커서 아직 처리 중이거나 소비자 서버가 죽었을 때이다. RabbitMQ에서는 ACK을 받지 못한 메시지의 경우, 대기를 하고 있다가 전달한 소비자 서버의 상태를 확인한 후, Disconnected와 같은 신호를 받았을 경우 해당 소비자를 제외하고 다른 소비자에게 동일한 메시지를 전달한다.\nMessage Durability # 만약 메시지를 Queue에 넣은 다음 소비자에게 전달하기 전에 RabbitMq가 죽는다면, Queue는 메모리에 데이터를 쓰는 형식이므로 모든 데이터가 소멸하게 된다.\n영속성 # message durability는 메시지가 Queue에 저장될때, disk의 파일에도 동시에 저장하는 방법이다.\n해당 방법을 사용하면 서버가 죽었을때, Queue의 데이터가 어느 정도 복구가 되지만 메시지가 disk의 파일에 쓰는 도중에 서버가 죽는 경우도 있어서 일부 데이터의 소실이 발생할 수 있다.\nRabbitMQ 노드 # RAM : 모든 메타 정보를 메모리에 저장한다. disc보다 성능이 좋지만 해당 노드에 문제가 생겼을때 데이터가 유실될 수 있다. disc : 데이터를 디스크에 저장한다. RabbitMQ는 클러스터 내 모든 노드를 RAM 타입을 구성하는 것을 제한한다. 반드시 1개 이상의 disc 타입의 노드를 포함해야한다.\nReferences https://somaz.tistory.com/119#google_vignette "},{"id":40,"href":"/docs/kafka/002_kafka_intro/","title":"002 Kafka Intro","section":"Kafka","content":" 강의 메모 # 아파치 카프카의 탄생과 기본 구조 # 카프카의 탄생\n각각의 애플리케이션끼리 연결하여 데이터를 처리하는게 아닌, 한 곳에 모아 처리할 수 있도록 중앙집중화했다. 메시지 큐 구조를 그대로 살린 카프카 내부 구조 # 카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO(First In First Out) 방식의 큐 자료구조와 유사하다. 큐에 데이터를 보내는 것이 프로듀서 큐에서 데이터를 가져가는 것이 컨슈머 적재된 데이터를 하나하나 가져가더라도 파티션 내의 데이터는 삭제되지 않는다. 데이터를 읽는것 : 커밋 (데이터를 어디까지 읽었는지를 기록) 카프카가 데이터 파이프라인으로 적합한 4가지 이유 # 1. 높은 처리량 # 카프카는 프로듀서가 브로커로 데이터를 보낼때와 컨슈머가 브로커로부터 데이터를 받을때 모두 묶어서 전송한다. 동일한 양의 데이터를 보낼때 네트워크 통신 횟수를 최소한으로 줄인다면 동일 시간 내에 더 많은 데이터를 전송할 수 있다. 데이터를 묶음 단위로 처리하는 배치로 빠르게 처리할 수 있다. (대용량 실시간 데이터 처리에 적합) 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬 처리할 수 있다. 파티션 개수만큼 컨슈머 개수를 늘려서 동일 시간당 데이터 처리량을 늘리는 것이다. (sacle-out : 인스턴스 개수를 늘림 (컨슈머 개수를 늘림))\n2. 확장성 # 데이터가 얼마나 들어올지 예측하기 어렵다. 이러한 가변적인 환경에서도 안정적으로 확장 가능하다. 데이터가 적을때는 카프카 클러스터의 브로커를 최소한의 개수로 운영하다가 데이터가 많아지면 클러스터의 브로커 개수를 자연스럽게 늘려 scale-out할 수 있다.\n클러스터개념으로 운영하는데, 1개의 클러스터는 여러개의 브로커를 가지고있고, 각각의 브로커는 카프카 프로세서다. 데이터를 전송하면 각각의 브로커에 데이터가 저장된다. 각 브로커가 처리할 수 있는 데이터 처리량의 한계가 있고 데이터 처리량이 있을때 브로커를 추가하면 된다.\n반대로 데이터 개수가 적어지고 추가 서버들이 더는 필요없어지면 브로커 개수를 줄여 스케일 인(scale-in)할 수 있다. 카프카의 스케일 아웃, 스케일 인 과정은 클러스터의 무중단 운영을 지원한다.\n3. 영속성 # 영속성 : 데이터를 생성한 프로그램이 종료되더라도 사라지지 않은 데이터의 특성\n카프카는 데이터를 파일 시스템에 저장한다. 이는 보편적으로 느리다고 생각하겠지만, 카프카는 운영체제 레벨에서 파일 시스템을 최대한 활용하는 방법을 적용했다. 페이지 캐시(page cache) 영역을 메모리에 따로 생성하여 사용한다. 페이지 캐시 메모리 영역을 사용하여 한번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식이다. 카프카가 파일 시스템에 저장하고 데이터를 저장, 전송하더라도 처리량이 높은 것이다. 디스크 기반의 파일 시스템을 사용하므로 급작스럽게 종료되더라도 프로세스를 재시작하여 안전하게 데이터를 다시 처리할 수 있다.\n4. 고가용성 # 3개 이상의 서버들로 운영되는 카프카 클러스터 : 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다.\n클러스터 브로커1, 브로커2, 브로커3 (3개 이상 운영) -\u0026gt; Producer 에서 데이터를 보냈을때 브로커1에서 브로커2,3으로 복제하여 저장한다. 여러 브로커에 데이터를 모두 저장함으로써 1대의 브로커에 장애가 발생하더라도 복제된 데이터가 나머지 브로커에 있으므로 지속적으로 데이터 처리가 가능하다. 이 적재된 데이터를 바탕으로 컨슈머가 각 브로커들의 데이터를 지속적으로 가져갈 수 있다.\n빅데이터 아키텍처의 종류와 카프카의 미래 # 초기 빅데이터 플랫폼 각 서비스 애플리케이션으로부터 데이터를 배치로 모았다. 이런 방식은 end-to-end로 운영 데이터를 배치로 모으는 구조는 유연하지 못했고, 실시간으로 생성되는 데이터들에 대한 인사이트를 서비스 애플리케이션에 빠르게 전달하지 못했다. 원천 데이터로부터 파생된 데이터의 히스토리 파악도 어려웠다.\n람다 아키텍처 # 배치 레이어 : 배치 데이터를 모아서 특정 시간, 타이밍마다 일괄 처리 서빙 레이어 : 가공된 데이터를 데이터 사용자, 서비스 애플리케이션이 사용할 수 있도록 데이터가 저장된 공간 스피드 레이어 : 서비스에서 생성되는 원천 데이터를 실시간으로 분석하는 용도로 사용 배치 데이터에 비해 낮은 지연으로 분석이 필요한 경우는 스피드 레이어를 통해 데이터를 분석한다.\n한계 # 레이어가 2개로 나눠져서 불편함 존재 데이터를 분석, 처리하는데에 필요한 로직이 2벌로 각각의 레이어에 따로 존재해야한다. 배치 데이터, 실시간 데이터를 융합하여 처리할 때는 다소 유연하지 못한 파이프라인을 생성해야한다. 카파 아키텍처 # 스피드 레이어, 서빙 레이어로 구성된다. 람다 아키텍처의 단점으로 부각되었던 로직의 파편화, 디버깅, 배포, 운영 분리에 대한 이슈를 제거하기 위해 배치 레이어를 제거했다. 스피드 레이어에서 데이터를 모두 처리하여, 더욱 효율적으로 개발과 운영에 임할 수 있게되었다.\n카파 아키텍처의 활용 # 배치 데이터를 스트림으로 표현한다. 각 시점의 배치 데이터의 변환 기록(change log)을 시간 순서대로 기록함으로써 각 시점의 모든 스냅샷 데이터를 저장하지 않고도 배치 데이터를 표현할 수 있게 되었다.\n배치 데이터 vs 스트림 데이터 # 배치 데이터 한정된(bounded) 데이터 처리 (분/시간 단위 처리 등) 대규모 배치 데이터를 위한 분산 처리 수행 복잡한 키 조인 수행 분, 시간, 일 단위 처리를 위한 지연 발생 스트림 데이터 무한(unbounded) 데이터 처리 지속적으로 들어오는 데이터를 위한 분산 처리 수행 분 단위 이하 지연 발생 단순한 키 조인 수행 배치 데이터를 처리하는 방법 (in 하둡) # block 단위로 데이터를 저장하고, 이것을 각 데이터 노드에서 메모리로 올리는 방식으로 데이터를 처리하고, 다시 HDFS에 저장 스트림 데이터를 배치로 사용하는 방법 (in 카프카) # 스트림 데이터를 배치 데이터로 사용하는 방법은 로그에 시간을 남기는 것이다. 로그에 남겨진 시간을 기준으로 데이터를 처리하면 스트림으로 적재된 데이터도 배치로 처리할 수 있게 된다. 카프카는 로그에 시간(timestamp)를 남기기 때문에 이런 방식의 처리가 가능하다.\n스트리밍 데이터 레이크 # 카파 아키텍처랑 별개로 보면된다. 카파 아키텍처는 데이터를 사용하는 고객을 위해 스트림 데이터를 서빙 레이어에 저장하는 것을 알 수 있다. 스피드 레이어를 사용되는 카프카 분석과 프로세싱을 완료한 거대한 용량의 데이터를 오랜 기간 저장하고 사용할 수 있다면, 서빙 레이어는 제거되어도 된다. -\u0026gt; 서빙 레이어와 스피드 레이어가 이중으로 관리되는 운영 리소스를 줄일 수 있다. 달성하기에는 어렵다. (자주 사용하는 데이터, 자주 사용하지않은 데이터를 구분하는게 쉽지 않다.)\n개선을 위한 노력 (현재는 제공되고있지 않음) 카프카 클러스터에서 자주 접근하지 않는 데이터는 오브젝트 스토리지와 같이 저렴하면서도 안전한 저장소에 옮겨 저장하고, 자주 사용하는 데이터만 브로커에서 사용하는 구분 작업이 필요하다.\n오픈소스 아파치 카프카 생태계 # 카프카 생태계 프로듀서 -\u0026gt; 토픽으로 데이터를 넣는다. 토픽 \u0026lt;- 컨슈머가 데이터를 가져간다. 토픽에 저장된 데이터를 처리하고, 다시 토픽에 넣고 싶을때는 카프카 스트림즈라는 라이브러리를 사용한다. JAVA 라이브러리로 제공한다. 3rd party library (Go, js 등) 카프카에서 제공되는 여러 기능들을 완벽하게 쓴다는 보장이 없다. 오픈소스 카프카에 포함된 툴 (기본적인 release된 구조) 커넥트 : 데이터 파이프라인을 운영하는 툴 커넥트(싱크) : 컨슈머 역할 (타겟 애플리케이션으로 데이터를 보내는 역할) 커넥트(소스) : 프로듀서 역할 (특정 데이터베이스가 사용) 위 2개를 각 프로듀서, 컨슈머로 운영하면 되지않나? 템플릿 형태로 반복적으로 여러번 생성할 수 있고, 함께 운영하는게 특징이다. 커넥트에서 REST API로 보내게되면 파이프라인을 반복적으로 만들 수 있다. References 강의 : https://www.inflearn.com/course/%EC%95%84%ED%8C%8C%EC%B9%98-%EC%B9%B4%ED%94%84%EC%B9%B4-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D "},{"id":41,"href":"/docs/parallel_programming/024_mutual_exclusion/","title":"024 Mutual Exclusion","section":"Parallel Programming","content":" 강의 메모 - Mutual Exclusion - 상호 배제 # 개요 # 뮤텍스(Mutual Exclusion) 또는 상호 배제는 공유 자원에 대한 경쟁 상태를 방지하고 동시성 제어를 위한 락 메커니즘이다 스레드가 임계영역에서 Mutex 객체의 플래그를 소유하고 있으면(락 획득) 다른 스레드가 액세스할 수 없으며 해당 임계영역에 액세스하려고 시도하는 모든 스레드는 차단되고 Mutex 객체 플래그가 해제된 경우(락 해제)에만 액세스할 수 있다 이 메커니즘은 Mutex 락을 가진 오직 한개의 스레드만이 임계영역에 진입할 수 있으며 락을 획득한 스레드만이 락을 해제 할 수 있다 결론 : 뮤텍스는 락과 락해제를 통해 자원을 보호하는 락체계 동기화 도구이다 Mutex 문제점 # 데드락(Deadlock)\n데드락은 두 개 이상의 스레드가 서로가 가진 락을 기다리면서 상호적으로 블로킹되어 아무 작업도 수행할 수 없는 상태를 의미하며 잘못된 뮤텍스 사용으로 인해 데드락이 발생할 수 있다. 두 스레드의 lock이 서로 엮여있어서 모두 끝나지 않는 상황 우선 순위 역전(Priority Inversion)\n우선 순위 역전은 높은 우선 순위를 가진 스레드가 낮은 우선 순위를 가진 스레드가 보유한 락을 기다리는 동안 블록되는 현상으로 높은 우선 순위를 가진 스레드의 작업이 지연될 수 있다. 우선 순위 상속으로 해결할 수 있다 T1(10), T2(7), T3(5)가 있을때 T1은 대기하고있고, T3이 실행중일때 T2가 T3보다 우선순위가 높으므로 T2가 실행될 수 있다. 이때 T1이 T2보다 우선순위가 높음에도 T2가 실행되어 우선순위가 역전되는 현상이다. 오버헤드\n뮤텍스를 사용하면 여러 스레드가 경합하면서 락을 얻기 위해 스레드 스케줄링이 발생한다. 이로 인해 오버헤드가 발생하고 성능이 저하될 수 있다 성능 저하\n뮤텍스를 사용하면 락을 얻기 위해 스레드가 대기하게 되고, 스레드의 실행 시간이 블록되면서 성능 저하가 발생할 수 있다 잘못된 사용\n뮤텍스를 적절하게 사용하지 않거나 잘못된 순서로 락을 해제하는 경우 예기치 않은 동작이 발생할 수 있다 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":42,"href":"/docs/parallel_programming/025_semaphore/","title":"025 Semaphore","section":"Parallel Programming","content":" 강의 메모 - Semaphore - 세마포어 - 1,2 # 개요 # 세마포어는 공유 자원에 대한 접근을 제어하기 위해 사용되는 신호전달 메커니즘 동기화 도구이다 Mutex는 락을 획득한 쓰레드가 락을 해제 할 수 있음 세마포어 : 신호전달로, 쓰레드가 임계영역에 들어갈 수 있도록 락을 풀어주거나 하나가 아닌 여러 쓰레드가 락을 동시에 가질 수 있다. 세마포어는 정수형 변수 S 와 P(Proberen: try), V(Verhogen: increment)의 두 가지 원자적 함수로 구성된 신호전달 메커니즘 동기화 도구이다 P : 임계 영역을 사용하려는 스레드의 진입 여부를 결정하는 연산, Wait 연산이라고도 함 락을 획득하는것 V : 대기 중인 프로세스를 깨우는 신호(Wake-up)로 Signal 연산 락을 획득하고 연산한 이후, 락을 해제하는것 스레드가 임계영역에 진입하지 못할 경우 자발적으로 \u0026lsquo;대기(BLOCK)\u0026lsquo;상태에 들어가고 임계영역을 빠져나오는 스레드가 대기상태의 스레드를 실행대기상태로 깨워준다 자바에서는 java.util.concurrent 패키지에 세마포어 구현체를 포함하고 있기 때문에 직접 세마포어를 구현할 필요는 없다 세마포어는 특정 신호에 의해 공유자원의 접근을 제어하고 보호하는 신호체계 동기화 도구이다 세마포어 # S 의미 : 정수형 변수 설명 : 공유 자원의 개수로서, 이 개수 만큼 스레드의 접근이 허용된다. P (wait) 의미 : S가 1감소 (S\u0026ndash;) 설명 : 스레드가 임계 구역에 진입하기 전 실행되어 카운트의 값을 1 감소시킨다. V 의미 : S가 1 증가 (S++) 설명 : 스레드가 임계 구역에서 빠져나올때 실행되어 카운트의 값을 1 증가시킨다. 동기화\nS \u0026gt; 0 : 공유 자원 접근 허용 S = 0 : 공유 자원 접근 불가 같은 세마포어의 P, V 함수의 S 연산은 여러 스레드간 동시에 실행이 되지 않도록 원자적 실행이 보장되어야한다. P(wait) 동작 방식 # S = 2일 때 2개의 쓰레드가 임계영역에 진입될 수 있다 thread A 1개가 진입되면 S\u0026ndash; 수행되어 S = 1이 된다. thread B도 wait()을 통해서 임계영역 진입을 위해 세마포어를 얻는다. s\u0026ndash; 수행되어 S = 0 최종적으로 세마포어 S = 0 0이 된 이후로 다른 쓰레드가 최소한 하나라도 해제를 해야 세마포어를 얻을 수 있게된다. thread C가 wait() 호출하면 blocked 된다. V(signal) 동작 방식 # threadA, threadB가 세마포어를 얻어서 S = 0이고, thread C는 세마포어를 획득할 수 없으므로 blocked 상태 thread A가 세마포어를 릴리즈하여 S++ 수행되어 S = 1 thread C가 이제 진입 가능, 세마포어 S\u0026ndash; 수행되어 S = 0 (threadB, threadC가 수행중) 모든 스레드가 릴리즈 수행 그럼 다시 세마포어는 S = 2로 초기화 threadD, threadE가 wait() 호출하여 임계영역에 들어가기 위한 허가권을 얻음 S 는 최종적으로 0이됨 세마포어 유형 # 세마포어는 카운트 변수 S 가 1인 이진 세마포어(Binary Semaphore) 와 2 이상의 양수 값을 가진 카운팅 세마포어(Counting Semaphore)로 구분할 수 있다. 이진 세마포어(Binary Semaphore) : 뮤텍스랑 거의 동일 카운팅 세마포어(Counting Semaphore) : signal(카운터 변수)을 변경하는 쓰레드가 꼭 본인이 아니여도된다. 이진 세마포어 # 세마포어를 뮤텍스 처럼 락으로 사용하기 위해서는 카운트 변수를 1로 설정하고 한 스레드 안에서 세마포어를 회득하고 해제 할 수 있도록 구현한다 한 스레드만이 세마포어를 획득할 수 있기 때문에 그 외 다른 모든 스레드가 acquired()를 호출하게 되면 해당 스레드가 세마포어를 해제하기 전까지 블록된다 카운팅 세마포어 # 카운팅 세마 포어는 카운트 변수를 설정해서 스레드가 공유할 수 있는 자원의 최대치를 한정해서 운용하는 방식으로 자원 풀(pool)이나 컬렉션의 크기에 제한을 두고자 할 때 유용하다 DB Connection 개수 제한, 파일 다운로드 동시 실행 제한 등.. 락을 획득한 스레드와 락을 해제하는 스레드는 다를 수 있으며 스레드 간 락과 락해제를 위한 신호를 전달함으로 동기화를 구현한다 자바 세마포어 구조 # 공정성 여부가 true이면 원래 스케줄링해서 진행했던 부분을 wait 요청한 순으로 할당함 자바 세마포어 사용 # 인수로 전달하는 초기 허가 수는 런타임에서 동적으로 변경 가능하다 semaphore.acquire(); 임계영역; semaphore.release(); 뮤텍스와 세마포어 # 동작 방식 뮤텍스는 공유 자원에 대한 접근을 동시에 하나의 스레드만 가능하도록 보장한다. 즉, 뮤텍스는 상호 배제를 위한 동기화 기법이다 세마포어는 카운팅 기법으로, 특정 개수의 스레드가 동시에 공유 자원에 접근할 수 있도록 제어한다. 0 또는 1의 값을 가진 이진 세마포어는 뮤텍스와 유사한 역할을 하며 계수 세마포어는 양수 값을 가지며, 해당 개수만큼의 스레드가 동시에 접근을 허용한다 소유권 뮤텍스는 소유권이 있어서 락을 획득한 스레드만이 락을 해제할 수 있다. 즉, 락을 획득한 스레드가 락을 해제하지 않으면 다른 스레드는 해당 뮤텍스에 접근할 수 없다. 세마포어는 소유권이 없으며, 특정 개수의 스레드가 동시에 접근을 허용하는 카운팅 기법으로 작동한다. 따라서 세마포어를 사용하는 스레드들이 모두 세마포어를 해제할 수 있다. 초기값 뮤텍스는 기본적으로 잠겨있는 상태로 시작한다. 한 스레드가 뮤텍스를 획득하여 자원에 접근하면 다른 스레드들은 해당 뮤텍스를 획득하기 위해 블로킹된다 세마포어는 초기값을 설정할 수 있으며 초기값에 따라서 처음부터 스레드가 자원에 접근할 수 있는지 여부가 결정된다 사용 목적 뮤텍스는 주로 상호 배제를 위해 사용되며 하나의 자원에 하나의 스레드만 접근하도록 보장해야 하는 경우에 사용된다 세마포어는 주로 리소스의 한정적인 사용을 제어하는 데 사용되며 특정 개수의 스레드만이 동시에 자원에 접근하도록 제한하고자 할 때 사용된다 자바에서는 뮤텍스를 고도화한 모니터 객체와 세마포어 구현체를 제공하고 있다 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":43,"href":"/docs/parallel_programming/020_single_multi_thread/","title":"020 Single Multi Thread","section":"Parallel Programming","content":" 강의 메모 - 싱글스레드 \u0026amp; 멀티스레드 # 개요 # 프로세스는 오직 한개의 스레드로만 구성하는 싱글 스레드 프로세스와 하나 이상의 스레드로 구성하는 멀티 스레드 프로세스로 구분할 수 있다 작업 처리에 있어서 단일스레드와 멀티 스레드의 선택 기준은 어떤 방식이 자원을 더 효율적으로 사용하고 성능처리에 유리한가 하는 점이다 현대 CPU 는 대부분 멀티코어를 지원하기 때문에 병렬적 성능 및 동시적 자원 사용 관점에서는 싱글 스레드보다 멀티스레드 기반 프로그래밍이 유리한 점이 많다 싱글 스레드 혹은 아주 적은 스레드를 활용한 비동기 논블럭킹 프로그래밍은 많은 수의 멀티 스레드 기반 프로그래밍 보다 더 좋은 성능과 응답성을 보여줄 수 있다 단일스레드 # 장점 # 문맥교환이 없다 동기화 이슈가 없다 자원 비용이 적다 프로그래밍 난이도가 낮다 단점 # CPU 멀티코어 활용 못함　순차적 실행으로 응답성 및 전체 처리량이 낮다 I/O 처리 시 CPU 가 낭비된다　스레드에 오류가 발생하면 프로그램이 종료된다 멀티스레드 # 장점 # 동시성으로 사용자의 응답성 향상 CPU 멀티코어의 병렬성으로 성능 향상 CPU 낭비 없는 자원의 효율적인 사용　한 스레드 오류는 다른 스레드에 영향이 없다　단점 # 빈번한 문맥교환으로 성능이 저하 된다　스레드 간 동기화 이슈가 발생한다 스레드 생성 비용이 작지 않다 프로그래밍 난이도가 높다　멀티스레딩과 동시성 # CPU 의 동시적 작업 처리는 CPU 코어 개수보다 스레드의 개수가 많을 때 즉 , 멀티스레딩 환경에서 자원을 효율적으로 배분하고 사용하기 위해 설계된 방식이다 같은 프로그램 안에서 실행되는 여러 스레드가 읽기 및 쓰기 작업을 같은 메모리 영역에서 동시에 실행할 경우 동시성 문제가 대두된다 동시성 문제라 함은 하나의 스레드가 어떤 메모리 영역의 데이터를 쓰고 있는데 또 다른 스레드가 같은 메모리 영역의 데이터를 읽거나 쓸 경우 발생할 수 있는 문제이다 동시성 문제는 싱글스레드에서는 절대 발생하지 않으며 멀티 스레드를 운용하는 어플리케이션에서 나타나는 현상이다 a는 여러 쓰레드가 접근 가능하다. \u0026ldquo;멀티 스레딩 환경에서 공유 자원을 선점하기 위해 발생 하는 스레드 간의 동시성 문제는 자바 프로그램 개발에 있어서 반드시 숙지하고 이해해야 할 중요한 주제이다. 그리고 이 주제는 방대하고 복잡하며 난해하다.\u0026rdquo;\nReferences 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":44,"href":"/docs/parallel_programming/021_synchronized_cpu/","title":"021 Synchronized CPU","section":"Parallel Programming","content":" 강의 메모 - 동기화와 CPU 관계 # 동기화 (synchronization)란\n프로세스 혹은 스레드 간 공유 영역에 대한 동시접근으로 인해 발생하는 데이터 불일치(data inconsistency) 를 막고 데이터 일관성을 유지하기 위해 순차적으로 공유 영역을 수행하도록 보장하는 메카니즘이라 할 수 있다 CPU 연산 처리 이해\n모든 기계어 명령(machine instruction) 은 원자성(atimicity) 을 갖는데 이는 하나의 기계어 명령어가 실행을 시작할 경우 그 명령의 수행 종료 시 까지는 인터럽트(interrupt)를 받지 않는다. 분리 불가능(indivisible) 이라고도 한다 CPU 가 두 개 이상의 명령어를 처리할 경우에은 원자성이 보장되지 않는데 이는 각 명령을 수행하는 중에 OS 가 다른 스케줄링으로 CPU 에게 다른 명령을 수행하게 함으로써 현재 수행중인 명령을 인터럽트 즉 중단하게 된다는 의미이다 두 개 이상의 명령어를 원자성으로 묶기 위해서는 스레드 간 동기화 메카니즘이 필요하다. 즉 한 스레드가 모든 명령을 다 수행될 때까지 도중에 중단되지 않도록 해야 한다 ※ 결론적으로 CPU 에서 원자성을 보장하지 않는 모든 연산 처리는 스레드 간 동시적 접근에 의해 데이터 불일치가 발생할 수 있다\nReferences 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":45,"href":"/docs/parallel_programming/022_critical_section/","title":"022 Critical Section","section":"Parallel Programming","content":" 강의 메모 - ThreadLocal - 1,2 # Critical Section (임계영역, 공유변수영역) # Critical Section 이란 둘 이상의 스레드가 동시에 접근해서는 안되는 공유 자원(자료 구조 또는 장치) 에 접근하는 코드 영역를 말한다. 임계영역은 entry section, critical section, exit section, remainder section 으로 구성 된다 입장영역(entry section) : critical section 에 진입하기 위해 진입허가를 요청하는 영역입니다. 임계영역(critical section) : 하나의 스레드만 접근할 수 있는 영역이다 퇴장영역(exit section) : critical section 에서 빠져나올 때 신호를 알리는 영역이다 나머지영역(remainder section) : entry section, critical section, exit section 을 제외한 나머지 영역이다 lock을 획득해야만, 임계영역에 진입 가능하다. Critical Section Problem # 한 스레드가 critical section을 실행하고 있을 때 다른 스레드가 같은 critical section 을 사용함으로서 발생하는 문제이다 이 문제의 해결책을 위해서는 3가지 충족 조건이 요구된다. Mutual Exclusion (상호 배제)\n어떤 스레드가 critical section을 실행중이면 다른 스레드는 동일한 critical section 을 실행할 수 없다. Progress (진행)\n임계 구역에서 실행 중인 스레드가 없고 임계 구역에 진입하려는 스레드가 있을 때 어떤 스레드가 들어갈 것인지 적절히 선택해 줘야 하며 이러한 결정은 무한정 미뤄져선 안된다 Bounded Waiting (한정 대기)\n다른 스레드가 임계 영역에 들어가도록 요청한 후 해당 요청이 수락되기 전에 기존 스레드가 임계 영역에서 실행할 수 있는 횟수에 제한이 있어야 한다 thread1이 임계영역을 수행중이고, thread2번이 요청을 해서 thread1이 끝날때까지 대기한다. thread2는 대기중이기 때문에 thread1이 또다시 실행한다거나 하면 계속 대기하게될 수 있기 때문에 횟수 제한을 둔다는 뜻이다. (무한정 실행을 막는것) Starvation(기아상태) 이 발생하지 않도록 한다 동기화 도구들 # 뮤텍스, 세마포어, 모니터, CAS(Compare and Swap) 와 같은 동기화 도구를 통해 Critical Section Problem 이 발생하지 않도록 할 수 있으며 자바에서는 synchronized 키워드를 포함한 여러 동기화 도구들을 제공하고 있다 Race Condition (경쟁상태, 경쟁조건, 경합상태) # 여러 스레드가 동시에 공유 자원에 액세스하고 조작할 때 스레드 간 액세스하는 순서나 시점에 따라 실행결과가 달라질 수 있는데 이것을 경쟁 상태라고 한다 경쟁상태는 Critical Section Problem 이 해결되지 않은 상태에서 여러 스레드가 동시에 임계영역에 접근해서 공유 데이터를 조작함으로써 발생하는 상태라 할 수 있다 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":46,"href":"/docs/parallel_programming/023_thread_safe/","title":"023 Thread Safe","section":"Parallel Programming","content":" 강의 메모 - ThreadLocal - 1,2 # 개요 # 여러 스레드에서 클래스나 객체에 동시에 접근해서 계속 실행하더라도 지속적인 정확성이 보장되는 코드를 스레드 세이프(thread-safe) 즉 스레드에 안전하다고 한다. 동시에 실행해도 동일한 결과값 기본적으로 클래스 명세에 스레드 안정성을 헤치는 코드나 상태를 가지고 있지 않으면 스레드에 안전하다라고 정의할 수 있다 스레드에 안전한 코드에는 경쟁상태가 없으며 경쟁 상태는 다수의 스레드가 공유 자원에 쓰기 작업을 시도할 때 발생하기 때문에 스레드가 실행될 때 어떤 자원을 공유하게 되는지 아는 것이 중요하다 스레드에 안전한 구조 # 임계영역을 동기화 한다\n동시에 여러개의 스레드가 임계영역을 접근하지 못하도록 락(Lock) 메카니즘을 사용한다 동기화 도구를 사용한다\n세마포어, CAS, Atomic 변수, 동시성 자료구조 등의 동기화 도구들을 사용해서 스레드 안전성을 구현한다 스레드의 스택에 한정해서 상태를 관리한다\n스레드마다 할당된 스택 메모리 내에서 상태를 관리함으로서 다른 스레드와 상태를 공유할 수 없도록 한다’ ThreadLocal 을 사용한다\n스레드마다 가지고 있는 전용 저장소인 ThreadLocal 을 사용해서 상태를 관리함으로서 다른 스레드와 상태를 공유할 수 없도록 한다 불변 객체를 사용한다\n객체의 상태를 변경할 수 없는 클래스를 사용하거나 클래스를 설계할 때 상태를 변경할 수 없도록 만들어서 스레드에 안전하도록 한다 동기화 도구 사용 # 각 스레드가 동시에 동일한 Item 을 중복해서 읽어 올 수 있다 각 스레드가 대기하고 있다가 순차적으로 Item 을 읽어 온다 스레드의 스택 한정 # 지역 변수\n기본형 지역 변수는 스레드 마다 독립적으로 가지고 있는 스택에 저장되기 때문에 스레드간에 공유될 수 없다. 스레드에 안전한다 메서드로 전달되는 기본형 파라미터 변수도 스택에서만 관리되므로 스레드에 안전하다 지역 객체 참조\n지역 변수라 할지라도 객체 참조 변수는 기본형과 다른점이 있는데 객체는 스택에 저장되지 않고 메모리의 힙(heap) 영역에 저장된다는 점이다 지역적으로 생성된 객체가 해당 메서드에서 벗어나지 않고 사용 된다면 스레드는 자신만의 객체를 참조할 수 있게 되어 스레드에 안전하다 지역 참조 변수를 다른 클래스의 메소드에 파라미터로 넘겼을 때 해당 클래스가 파라미터 변수를 다른 스레드가 접근할 수 있는 멤버 변수로 저장했을 경우에는 스레드에 안전하지 않다 만약에 Member member1을 파라미터로 받는다면? 위 member1객체의 변경 행위를 하면 스레드 안정하지 않다. 모든 스레드가 이 객체를 바라보기 때문 (new 연산자로 생성한게 아니니깐) 동시 접근이 가능하기 때문에 발생 (멤버변수) 문자열 같이 불변 객체는 상태가 변경되지 않기 때문에 스레드에 안전하다 멤버 변수 참조\n멤버 변수 참조 역시 스레드 마다 객체를 생성하는 원리는 동일하다. 즉 스레드의 스택별로 객체가 생성되어 참조되도록 구현하면 된다 불변 객체 사용 # Setter 메서드가 없음, 생성자에서 멤버 변수 초기화, 멤버 변수는 final 로 선언 등으로 불변객체를 생성한다 불변객체는 어떠한 상황에도 상태가 변하지 않으므로 스레드에 안전하다고 할 수 있다 ※ 최초로 객체가 생성되는 시점 이후로 값이 절대 변하지 않는다\nReferences 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":47,"href":"/docs/parallel_programming/017_thread_type/","title":"017 Thread Type","section":"Parallel Programming","content":" 강의 메모 - 사용자 스레드 vs 데몬 스레드 # 개요 # 자바에는 크게 두 가지 유형의 스레드로 구분할 수 있는데 바로 사용자 스레드(user thread)와 데몬 스레드(daemon thread)이다. 사용자 스레드는 사용자 스레드를 낳고 데몬 스레드는 데몬 스레드를 낳는다.즉 자식 스레드는 부모 스레드의 상태를 상속 받는다 자바 어플리케이션이 실행이 되면 JVM 은 사용자 스레드인 메인스레드와 나머지 데몬 스레드를 동시에 생성하고 시작한다 main thread # 메인 스레드는 어플리케이션에서 가장 중요한 부분으로서 어플리케이션을 실행할 때마다 메인 스레드가 생성되어 실행된다 메인 스레드는 어플리케이션을 실행하는 최초의 스레드이자 어플리케이션 실행을 완료하는 마지막 스레드의 역할을 한다 메인 스레드에서 여러 하위 스레드를 추가로 시작할 수 있고 하위 스레드는 또 여러 하위 스레드를 시작할 수 있다 메인 스레드가 사용자 스레드이기 때문에 하위 스레드는 모두 사용자 스레드가 된다 user thread (사용자 스레드) # 사용자 스레드는 메인 스레드에서 직접 생성한 스레드를 의미한다 사용자 스레드는 각각 독립적인 생명주기를 가지고 실행하게 되며 메인 스레드를 포함한 모든 사용자 스레드가 종료하게 되면 어플리케이션이 종료하게 된다 사용자 스레드는 foreground 에서 실행되는 높은 우선순위를 가지며 JVM은 사용자 스레드가 스스로 종료될 때까지 어플리케이션을 강제로 종료하지 않고 기다린다 자바가 제공하는 스레드 풀인 ThreadPoolExecutor 은 사용자 스레드를 생성한다 daemon thread (데몬 스레드) # 데몬 스레드는 JVM 에서 생성한 스레드이거나 직접 데몬 스레드로 생성한 경우를 말한다 모든 사용자 스레드가 작업을 완료하면 데몬 스레드의 실행 여부에 관계없이 JVM 이 데몬 스레드를 강제로 종료하고 어플리케이션이 종료한다 데몬 스레드의 생명주기는 사용자 스레드에 따라 다르며 낮은 우선순위를 가지고 background 에서 실행된다 데몬 스레드는 사용자 스레드를 보조 및 지원하는 성격을 가진 스레드로서 보통 사용자 작업을 방해하지 않으면서 백그라운드에서 자동으로 작동되는 기능을 가진 스레드이다 자바가 제공하는 스레드 풀인 ForkJoinPool 은 데몬 스레드를 생성한다 데몬 스레드 생성 # void setDaemon(boolean on) 스레드를 데몬 또는 비데몬 스레드로 표시하며 이 메소드는 반드시 스레드가 시작되기 전에 호출되어야 한다 스레드가 실행 중인 동안 setDaemon()을 호출하려고 하면 IllegalThreadStateException 이 발생한다 true 이면 데몬스레드가 되며 false 이면 사용자 스레드가 된다. 기본은 false 이다 데몬 스레드 확인 # boolean isDaemon() 이 스레드가 데몬 스레드인지 아닌지 확인한다. References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":48,"href":"/docs/parallel_programming/018_thread_group/","title":"018 Thread Group","section":"Parallel Programming","content":" 강의 메모 - ThreadGroup # 개요 # 자바는 스레드 그룹(ThreadGroup)이라는 객체를 통해서 여러 스레드를 그룹화하는 편리한 방법을 제공한다 ThreadGroup은 스레드 집합을 나타내며 스레드 그룹에는 다른 스레드 그룹도 포함될 수 있고 그룹 내의 모든 스레드는 한 번에 종료하거나 중단할 수 있다 스레드는 반드시 하나의 스레드 그룹에 포함되어야 하며 명시적으로 스레드 그룹에 포함시키지 않으면 자신을 생성한 스레드가 속해 있는 스레드 그룹에 포함되어 진다 일반적으로 사용자가 main 스레드에서 생성하는 모든 스레드는 기본적으로 main 스레드 그룹에 속하게 된다 코드로 별도 스레드를 만들면 기본적으로 main 스레드 그룹에 속해진다. 대신 명시적으로 csustom thread group을 만들어서 셋팅하면 여기에 속하게된다. JVM의 스레드 그룹 생성 과정 # JVM 이 실행되면 최상위 스레드 그룹인 system 스레드 그룹이 생성된다 JVM 운영에 필요한 데몬 스레드들을 생성해서 system 스레드 그룹에 포함시킨다 system 스레드 그룹의 하위 스레드 그룹인 main 스레드 그룹을 만들고 main 스레드를 그룹에 포함시킨다 Thread Group 구조 # 스레드 그룹에 설정된 max priority를 개인 스레드 max priority가 넘을 수 없다. 스레드 그룹의 max priority가 5일때 개인 스레드 max priority를 7로 설정하더라도 5다. References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":49,"href":"/docs/parallel_programming/019_threadLocal/","title":"019 Thread Local","section":"Parallel Programming","content":" 강의 메모 - ThreadLocal - 1,2 # 개요 # 자바에서 스레드는 오직 자신만이 접근해서 읽고 쓸수 있는 로컬 변수 저장소를 제공하는데 이를 ThreadLocal 이라고 한다 각 스레드는 고유한 ThreadLocal 객체를 속성으로 가지고 있으며 ThreadLocal 은 스레드 간 격리되어 있다 스레드는 ThreadLocal 에 저장된 값을 특정한 위치나 시점에 상관없이 어디에서나 전역변수처럼 접근해서 사용할 수 있다. 변수 값을 전달하지 않아도 된다 모든 스레드가 공통적으로 처리해야 하는 기능이나 객체를 제어해야 하는 상황에서 스레드마다 다른 값을 적용해야 하는 경우 사용한다 (인증 주체 보관, 트랜잭션 전파, 로그 추적기 등) Thread 마다 ThreadLocal을 가지고있다. ThreadLocal은 각자 ThreadLocalMap이라는 객체를 가지고있어서 값을 가진다. ThreadLocal API # void set(T value) 스레드 로컬에 값을 저장한다 T get() 스레드 로컬에 저장된 값을 가져온다 void remove() 스레드 로컬에 저장된 값을 삭제한다 withInitial(Supplier\u0026lt;? extends S\u0026gt; supplier) 스레드 로컬을 생성하면서 특정 값으로 초기화한다 Thread \u0026amp; ThreadLocal # 스레드는 ThreadLocal 에 있는 ThreadLocalMap 객체를 자신의 threadLocals 속성에 저장한다. 스레드 생성 시 threadLocals 기본값은 null 이며 ThreadLocal 에 값을 저장할 때 ThreadLocalMap 이 생성되고 threadLocals 과 연결된다 스레드가 전역적으로 값을 참조할 수 있는 원리는 스레드가 ThreadLocal 의 ThreadLocalMap 에 접근해서 여기에 저장된 값을 바로 꺼내어 쓸수 있기 때문이다 ThreadLocalMap 은 항상 새롭게 생성되어 스레드 스택에 저장되기 때문에 근본적으로 스레드간 데이터 공유가 될 수 없고 따라서 동시성 문제가 발생하지 않는다 threadLocal은 1개인거고 여기 내부에서 ThreadLocalMap이 여러개 생기는것 주의사항 # ThreadLocal 에 저장된 값은 스레드마다 독립적으로 저장되기 때문에 저장된 데이터를 삭제하지 않아도 메모리를 점유하는 것 외에 문제가 되지는 않는다. 그러나 스레드 풀을 사용해서 스레드를 운용한다면 반드시 ThreadLocal 에 저장된 값을 삭제해 주어야 한다 스레드풀은 스레드를 재사용하기 때문에 현재 스레드가 이전의 스레드를 재사용한 것이라면 이전의 스레드에서 삭제하지 않았던 데이터를 참조할 수 있기 때문에 문제가 될 수 있다 ThreadLocal 작동원리 # ThreadLocal 은 Thread 와 ThreadLocalMap 을 연결하여 스레드 전용 저장소를 구현하고 있는데 이것이 가능한 이유는 바로 Thread.currentThread() 를 참조할 수 있기 때문이다 Thread.currentThread() 는 현재 실행중인 스레드의 객체를 참조하는 것으로서 CPU 는 오직 하나의 스레드만 할당받아 처리하기 때문에 ThreadLocal 에서 Thread.currentThread() 를 참조하면 지금 실행 중인 스레드의 로컬 변수를 저장하거나 참조할 수 있게 된다 ThreadLocal 에서 현재 스레드를 참조할 수 있는 방법이 없다면 값을 저장하거나 요청하는 스레드를 식별할 수 없기 때문에 Thread.currentThread() 는 ThreadLocal 의 중요한 데이터 식별 기준이 된다 데이터 저장 # 데이터 조회 # InheritableThreadLocal # InheritableThreadLocal은 ThreadLocal의 확장 버전으로서 부모 스레드로부터 자식 스레드로 값을 전달하고 싶을 경우 InheritableThreadLocal을 사용할 수 있다 값의 상속: 부모 스레드가 InheritableThreadLocal 변수에 값을 설정하면, 해당 부모 스레드로부터 생성된 자식 스레드들은 부모의 값을 상속받게 된다 독립성 자식 스레드가 상속받은 값을 변경하더라도 부모 스레드의 값에는 영향을 주지 않는다 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":50,"href":"/docs/parallel_programming/012_thread_interrupt/","title":"012 Thread Interrupt","section":"Parallel Programming","content":" 강의 메모 - interrupt() # 개요 # Interrupt 의 사전적 의미는 ‘방해하다’ 라는 뜻으로 어떤 주체의 행동이나 실행흐름을 방해한다는 의미로 해석 할 수 있다 자바 스레드에서 interrupt() 는 특정한 스레드에게 인터럽트 신호를 알려 줌으로써 스레드의 실행을 중단하거나, 작업 취소, 강제 종료 등으로 사용할 수 있다 interrupt() # interrupt() 는 스레드에게 인터럽트가 발생했다는 신호를 보내는 메카니즘이다 interrupt() 는 스레드가 현재 실행 흐름을 멈추고 인터럽트 이벤트를 먼저 처리하도록 시그널을 보내는 장치라 할 수 있다 interrupted 속성 스레드는 인터럽트 상태(Interrupt State )로 알려진 interrupted 를 가지고 있으며 인터럽트 발생 여부를 확인할 수 있는 상태 값이다. 기본값은 fasle 이다 인터럽트된 스레드가 처리해야 하는 특별한 규칙이나 정해진 기준은 없으나 일반적으로 인터럽트 상태를 사용해서 스레드를 중지하거나, 작업을 취소하거나, 스레드를 종료 하는 등의 기능을 구현할 수 있다 한 스레드가 다른 스레드를 인터럽트 할 수 있고 자기 자신을 인터럽트 할 수도 있다 interrupt() 하는 횟수는 제한이 없으며 인터럽트 할 때 마다 스레드의 인터럽트 상태를 true 로 변경한다 interrupt() 상태 확인 방법 # static boolean interrupted() # 스레드의 인터럽트 상태를 반환하는 정적 메소드이다 만약 현재 인터럽트 상태가 true 인 경우 true 를 반환하고 인터럽트 상태를 false 로 초기화 하므로 인터럽트를 해제하는 역할을 한다 인터럽트를 해제하는 경우 다른 곳에서 스레드에 대한 인터럽트 상태를 체크하는 곳이 있다면 별도의 처리가 필요할 수 있다 true에서 false로 바꾸기 때문에 다른 곳에서 영향을 받을 수 있다. 인터럽트를 강제로 해제했기 때문에 다시 인터럽트를 걸어서 인터럽트 상태를 유지할 수 있다 boolean isInterrupted() # 스레드의 인터럽트 상태를 반환하는 인스턴스 메서드이다 이 메서드는 스레드의 인터럽트 상태를 변경하지 않고 계속 유지한다. 인터럽트 상태를 확인하는 용도로만 사용할 경우 interrupted() 보다 이 메서드를 사용하는 것이 좋다 InterruptedException # InterruptedException 은 interrupt() 메카니즘의 일부이며 대기나 차단 등 블록킹 상태에 있거나 블록킹 상태를 만나는 시점의 스레드에 인터럽트 할 때 발생하는 예외이다 InterruptedException 이 발생하면 인터럽트 상태는 자동으로 초기화 된다. 즉 Thread.interrupted() 한 것과 같은 상태로 된다( interrupted = false) 다른 곳에서 인터럽트 상태를 참조하고 있다면 예외 구문에서 대상 스레드에 다시 interrupt() 해야 할 수도 있다 InterruptedException 이 발생하는 케이스는 다음과 같다 Thread.sleep(), Thread.join(), Object.wait() Future.get(), BlockingQueue.take() interrupted(true) 일 경우에 sleep(), join(), wait() 이 수행됬을때 InterruptedException이 발생하는 것이다. References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":51,"href":"/docs/parallel_programming/013_thread_info/","title":"013 Thread Info","section":"Parallel Programming","content":" 강의 메모 - name() / currentThread() / isAlive() # Thread Name # 멀티 스레드 환경에서 어떤 스레드가 실행 중인지 알아야 할 경우 스레드에 사용자 이름을 지정하면 실행 중인 스레드를 쉽게 찾을 수 있다 디버깅할 때 어떤 스레드가 무슨 작업을 하고 있는지 정확하게 파악하기 위해서 스레드 이름을 정하는 것이 큰 도움이 된다 자바에서 스레드가 생성되면 스레드 이름이 자동으로 주어진다. 이건 사용자가 정하는 것이 아니다 가장 먼저 생성되는 메인 스레드의 이름은 main 이다. 스레드 이름은 Thread-0, Thread-1, Thread-2, .. Thread-n 과 같이 0 부터 순차적으로 숫자를 증가하면서 이름이 만들어진다 자바에서 사용자가 스레드 이름을 정할 수 있으며 두 가지 방법으로 가능하다 스레드 객체 생성 시 인자로 전달 Thread myThread = new Thread([ThreadGroup],[Runnable],“myThread”); setName(String name) 으로 설정 myThread.setName(“myThread”); getName() 으로 스레드 이름 참조 myThread.getName(); currentThread() # Thread 클래스의 정적 메서드로서 현재 실행 중인 스레드 개체에 대한 참조를 반환한다 ex) Thread.currentThread().getName(), if(Thread.currentThread() == thread) isAlive() # 스레드가 살아 있는지 여부를 알 수 있다 스레드의 start() 메서드가 호출되고 스레드가 아직 종료되지 않은 경우 스레드가 활성 상태인 것으로 간주되어 true 를 반환한다 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":52,"href":"/docs/parallel_programming/014_thread_priority/","title":"014 Thread Priority","section":"Parallel Programming","content":" 강의 메모 - Priority # 스레드 우선순위 (Priority) # 단일 CPU에서 여러 스레드를 실행하는 것을 스케줄링이라고 하며 스레드는 스케줄링에 의해 선점되어 CPU 를 할당받는다\n자바 런타임은 고정 우선순위 선점형 스케줄링(fixed-priority pre-emptive scheduling ) 으로 알려진 매우 단순하고 결정적인 스케줄링 알고리즘을 지원한다\n이 알고리즘은 실행 대기 상태의 스레드 중에 상대적인 우선 순위에 따라 스레드를 예약한다\n우선순위 개념\nJava에서 스레드의 우선 순위는 1에서 10 사이의 정수이며 정수 값이 높을수록 우선순위가 높다 스레드가 생성될 때 우선순위 값이 정해지며 기본 우선 순위인 5 로 설정된다 스케줄러는 우선순위가 높은 스레드를 실행하다가 해당 스레드가 중지, 양보 또는 실행 불가능이 되는 경우 우선 순위가 낮은 스레드를 실행하기 시작한다 두 스레드의 우선순위가 같을 경우 라운드 로빈(순환 할당) 스케줄링 방식에 의해 다음 스레드를 선택한다 스케줄러가 반드시 우선순위가 높은 스레드를 실행한다고 보장 할 수 없다. 운영체제마다 다른 정책들이 있을 수 있으며 기아상태를 피하기 위해 스케줄러는 우선순위가 낮은 스레드를 선택할 수 있다 우선순위 유형\n최소 우선순위 – 1, public static int MIN_PRIORITY 기본 우선순위 - 5, public static int NORM_PRIORITY 최대 우선순위 - 10, public static int MAX_PRIORITY 우선순위 변경 및 확인\nvoid setPriority(int newPriority) 스레드에 대해 허용되는 우선 순위 값은 1에서 10 사이이며 이 외에 값을 설정하면 오류가 발생한다 스레드의 우선순위를 새롭게 변경한다 int getPriority()– 스레드의 우선순위를 반환 함 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":53,"href":"/docs/parallel_programming/015_thread_uncaughtExceptionHandler/","title":"015 Thread Uncaught Exception Handler","section":"Parallel Programming","content":" 강의 메모 - 스레드 예외처리 - UncaughtExceptionHandler # 개요 # 기본적으로 스레드의 run() 은 예외를 던질 수 없기 때문에 예외가 발생할 경우 run() 안에서만 예외를 처리해야 한다 RuntimeException 타입의 예외가 발생할 지라도 스레드 밖에서 예외를 캐치할 수 없고 사라진다 스레드가 비정상적으로 종료되었거나 특정한 예외를 스레드 외부에서 캐치하기 위해서 자바에서는 UncaughtExceptionHandler 인터페이스를 제공한다 UncaughtExceptionHandler # 캐치 되지 않는 예외에 의해 Thread가 갑자기 종료했을 때에 호출되는 핸들러 인터페이스 어떤 원인으로 인해 스레드가 종료되었는지 대상 스레드와 예외를 파악할 수 있다 예외가 발생하면 uncaughtException 이 호출되고 대상 스레드 t 와 예외 e 가 인자로 전달된다 Thread API # static void setDefaultUncaughtExceptionHandler\n모든 스레드에서 발생하는 uncaughtException 을 처리하는 정적 메서드 void setUncaughtExceptionHandler(UncaughtExceptionHandler ueh)\n대상 스레드에서 발생하는 uncaughtException 을 처리하는 인스턴스 메서드 setDefaultUncaughtExceptionHandler 보다 우선순위가 높다 흐름도 # References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":54,"href":"/docs/parallel_programming/016_thread_stop_flag/","title":"016 Thread Stop Flag","section":"Parallel Programming","content":" 강의 메모 - 스레드 중지 – flag variable vs interrupt() - 1,2 # 개요 # 자바에서는 무한 반복이나 지속적인 실행 중에 있는 스레드를 중지하거나 종료할 수 있는 API 를 더 이상 사용할 수 없다 (suspend(), stop()) 수행 중에 강제로 종료해버리면 이슈가 생길 수 있기 때문 스레드를 종료하는 방법은 플래그 변수를 사용하거나 interrupt() 를 활용해서 구현할 수 있다 Flag Variable # 플래그 변수의 값이 어떤 조건에 만족할 경우 스레드의 실행을 중지하는 방식 플래그 변수는 동시성 문제로 가능한 atomic 변수나 volatile 키워드를 사용하도록 한다 동시성 문제 때문 running 변수는 여러 쓰레드가 있다. running은 메모리에 있다. T1, T2, T3, T4, T5가 있을때 T1이 running에 false를 주면, T1이 아닌 그 외 쓰레들도 메모리를 보고 false로 보게된다. 따라서 모든 쓰레드가 메모리를 바라보도록 해야한다. T1 쓰레드가 running을 false로 바꾸고 메모리를 바꿔야 다른 쓰레드도 바라볼텐데, 본인의 캐시에만 업데이트 해버리면 다른 쓰레드는 true이다. 그럼 나머지 쓰레드는 계속적으로 작업을 수행하게된다. interrupted() \u0026amp; isInterrupted() # 실행 중인 스레드에 interrupt() 하게 되면 인터럽트 상태를 사용해서 종료기능을 구현할 수 있다 interrupt() 한다고 해서 스레드가 처리하던 작업이 중지되는 것이 아니며 인터럽트 상태를 활용하여 어떤 형태로든 스레드를 제어할 수 있다 interrupted() # 스레드가 실행되면 Thread.interrupted() 가 false 이므로 반복문을 계속 실행한다 인터럽트가 발생하면 Thread.interrupted() 은 true 이고 반복문을 빠져 나오면서 스레드는 종료된다 인터럽트 상태는 해제 된다 isInterrupted() # 스레드가 실행되면 Thread.currentThread().isInterrupted() 가 false 이므로 반복문을 계속 실행한다 인터럽트가 발생하면 Thread.currentThread().isInterrupted() 은 true 이고 반복문을 빠져 나오면서 스레드는 종료된다 인터럽트 상태는 계속 유지 된다 InterruptedException # 대기 중인 스레드에 interrupt() 하게 되면 InterruptedException 예외가 발생한다. 이 예외 구문에서 종료기능을 구현할 수 있다 인터럽트가 발생하면 InterruptedException 예외가 발생하고 예외 구문에서 반복문을 빠져 나오면서 스레드는 종료된다 인터럽트 상태는 해제 된다 인터럽트가 발생하면 강제로 InterruptedException 예외를 던지고 예외 구문에서 반복문을 빠져 나오면서 스레드는 종료된다 인터럽트 상태는 해제 된다 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":55,"href":"/docs/parallel_programming/011_thread_join/","title":"011 Thread Join","section":"Parallel Programming","content":" 강의 메모 - join() # 개요 # join() 메서드는 한 스레드가 다른 스레드가 종료될 때까지 실행을 중지하고 대기상태에 들어갔다가 스레드가 종료되면 실행대기 상태로 전환된다 T1, T2가 있을때, T1이 T2의 모든 작업이 종료될때까지 대기했다가, T2의 작업이 다 끝나고나서 T1이 본인의 작업을 이어서 나가야할 경우 T1 기준으로 T2.join()을 수행 대기는 T1이 하는것 스레드의 순서를 제어하거나 다른 스레드의 작업을 기다려야 하거나 순차적인 흐름을 구성하고자 할 때 사용할 수 있다 Object 클래스의 wait() 네이티브 메서드로 연결되며 시스템 콜을 통해 커널모드로 수행한다. 내부적으로 wait() \u0026amp; notify() 흐름을 가지고 제어한다 API 및 예외처리 # void join() throws InterruptedException # 스레드의 작업이 종료 될 때까지 대기 상태를 유지한다 void join(long millis) throws InterruptedException # 지정한 밀리초 시간 동안 스레드의 대기 상태를 유지한다 밀리초에 대한 인수 값은 음수가 될 수 없으며 음수 일 경우 IllegalArgumentException 이 발생한다 void join( long millis, int nanos) InterruptedException # 지정한 밀리초에 나노초를 더한 시간 동안 스레드의 대기 상태를 유지한다 나노초의 범위는 0 에서 999999 이다 InterruptedException # 스레드가 인터럽트 될 경우 InterruptedException 예외가 발생시킨다 다른 스레드는 join() 을 수행 중인 스레드에게 인터럽트, 즉 중단(멈춤) 신호를 보낼 수 있다 InterruptedException 예외가 발생하면 스레드는 대기상태에서 실행대기 상태로 전환되어 실행상태를 기다린다 T1, T2가 있을때, T1이 T2의 모든 작업이 종료될때까지 대기했다가, T2의 작업이 다 끝나고나서 T1이 본인의 작업을 이어서 나가야할 경우 T1 기준으로 T2.join()을 수행 대기는 T1이 하는것 -\u0026gt; T1에게 인터럽트를 거는 것 public static void main(String[] args) { Runnable r = new MyRunnable(); Thread thread = new Thread(r); thread.start(); try { // try-catch 문으로 예외 처리 해 주어야 한다 thread.join(); // 메인 스레드는 thread 의 작업이 종료될 때 까지 일시정지한다 } catch (InterruptedException e) { // 인터럽트 발생 시 예외 처리 필요 } } join의 작동방식 # wait() \u0026amp; notify() interrupt() 발생 join() 작동 방식 정리 # join() 을 실행하면 OS 스케줄러는 현재 스레드를 대기 상태로 전환하고 join() 을 수행중인 스레드에게 CPU 를 사용하도록 한다 현재 스레드는 대상 스레드.join() -\u0026gt; 현재 스레드가 대기로 빠지는 것임 join() 을 수행중인 스레드의 작업이 종료되면 현재 스레드는 실행 대기 상태로 전환 되고 CPU 가 실행을 재개할 때 까지 기다린다. 실행 상태가 되면 스레드는 남은 지점부터 실행을 다시 시작한다 join() 을 수행중인 스레드가 여러 개일 경우 각 스레드의 작업이 종료될 때 까지 현재 스레드는 대기하고 종료 이후 실행을 재개하는 흐름을 반복한다 join() 을 수행중인 스레드에게 인터럽트가 발생할 경우 현재 스레드는 대기에서 해제되고 실행상태로 전환되어 예외를 처리하게 된다 References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":56,"href":"/docs/parallel_programming/010_thread_sleep/","title":"010 Thread Sleep","section":"Parallel Programming","content":" 강의 메모 - sleep() # 개요 # 지정된 시간 동안 현재 스레드의 실행을 일시 정지하고 대기상태로 빠졌다가 시간이 지나면 실행대기 상태로 전환된다 native 메서드로 연결되며 시스템 콜을 통해 커널모드에서 수행 후 유저모드로 전환한다 이 간단한 메서드 자체도 jvm 자체에서 실행되지 못하고 커널모드에서 수행된다. 컨텍스트 스위칭 발생 API 및 예외 # staic sleep(long millis) throws InterruptedException # 지정한 밀리초 시간 동안 스레드를 수면 상태로 만든다 밀리초에 대한 인수 값은 음수가 될 수 없으며 음수 일 경우 IllegalArgumentException 이 발생한다\nstatic sleep( long millis, int nanos) InterruptedException # 지정한 밀리초에 나노초를 더한 시간 동안 스레드를 수면 상태로 만든다, 나노초의 범위는 0 에서 999999 이다\nInterruptedException # 스레드가 수면 중에 인터럽트(쓰레드 실행의 중단의 신호를 보냄) 될 경우 InterruptedException 예외를 발생시킨다 sleep() 메서드 사용시, try~catch로 InturrptedException 예외처리가 필요하다. 다른 스레드는 잠자고 있는 스레드에게 인터럽트, 즉 중단(멈춤) 신호를 보낼 수 있다 InterruptedException 예외가 발생하면 스레드는 수면상태에서 깨어나고 실행대기 상태로 전환되어 실행상태를 기다린다 -\u0026gt; sleep()에서 깨어나는 조건 : 예외 발생 (실행대기 상태로 전환, 다시 실행상태로 전환가능한 상태)\n예시) 쓰레드 T1, 쓰레드 T2 쓰레드 T1이 sleep() 메서드를 만나서 3초 대기를 하다가, T2가 T1에 인터럽트를 걸면 3초가 지나지않아도 catch문으로 이동된다. 또한 인터럽트를 건 이후, sleep() 메서드를 만나면 바로 catch문으로 이동된다.\nSleep() 작동 방식 # 지정된 시간이 지난 경우 # interrupt() 발생할 경우 # Sleep(0)과 Sleep(n)의 의미 # sleep(millis) 메서드는 네이티브 메서드이기 때문에 sleep(millis) 을 실행하게 되면 시스템 콜을 호출하게 되어 유저모드에서 커널모드로 전환된다 다른 스레드에게 명확하게 실행을 양보하기 위함이라면 sleep(0) 보다는 sleep(1) 을 사용하도록 한다 sleep(0)은 아래 내용과 같이 조건에 따라서 양보할수도 있고, 그대로 실행될수도 있기 때문이다. sleep(0) # 스레드가 커널 모드로 전환 후 스케줄러는 현재 스레드와 동일한 우선순위(Priority)의 스레드가 있을 경우 실행대기상태 스레드에게 CPU 를 할당함으로 컨텍스트 스위칭이 발생한다 기본적으로 sleep()은 컨텍스트 스위칭이 발생하는데, sleep(0)은 조건이 있어야 컨텍스트 스위칭이 발생한다. 조건 : 현재 스레드와 동일한 우선순위의 스레드가 있을 경우 (Runnable 상태인 스레드여야함) 만약 우선순위가 동일한 실행대기 상태의 다른 스레드가 없으면 스케줄러는 현재 스레드에게 계속 CPU 를 할당해서 컨텍스트 스위칭이 없고 모드 전환만 일어난다 sleep()은 커널모드로 왔으므로, 다시 유저모드로 전환만 발생한다는 뜻이다. user -\u0026gt; kernel kernel -\u0026gt; user sleep(n) # 스레드가 커널 모드로 전환 후 스케줄러는 조건에 상관없이 현재 스레드를 대기상태에 두고 다른 스레드에게 CPU 를 할당함으로 모든 전환과 함께 컨텍스트 스위칭이 발생한다 sleep() 작동 방식 원리 # sleep() 이 되면 OS 스케줄러는 현재 스레드를 지정된 시간 동안 대기 상태로 전환하고 다른 스레드 혹은 프로세스에게 CPU 를 사용하도록 한다 대기 시간이 끝나면 스레드 상태는 바로 실행상태가 아닌 실행 대기 상태로 전환 되고 CPU 가 실행을 재개할 때 까지 기다린다. 쓰레드가 sleep() 상태에서 깨어난다고 해서 바로 실행되는게 아닌 실행대기 상태로 전환되고 실행되는 것이다. 실행 상태가 되면 스레드는 남은 지점부터 실행을 다시 시작한다 동기화 메서드 영역에서 수면 중인 스레드는 획득한 모니터나 락을 잃지 않고 계속 유지한다 쓰레드A가 lock을 가지고 있다가 wait(대기)상태로 빠졌을때 lock을 풀어버린다. 하지만 sleep()은 lock을 가지고 있다가 sleep()을 만나게 되더라도 lock을 풀지 않는다. 계속 유지한다. sleep() 중인 스레드에게 인터럽트가 발생할 경우 현재 스레드는 대기에서 해제되고 실행상태로 전환되어 예외를 처리하게 된다 try~catch에서 catch()문 실행 스레드의 수면 시간은 OS 스케줄러 및 시스템 기능에 따라 제한되기 때문에 정확성이 보장되지 않으며 시스템의 부하가 많고 적음에 따라 지정한 수면 시간과 차이가 날 수 있다 sleep(3000)일때 2.9초일수도, 3초일수도 있다. 시스템 부하에 따라 조금씩 차이가 날 수 있다. References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":57,"href":"/docs/etc/003_huge_traffic_handling/","title":"003 Huge Traffic Handling","section":"Etc","content":" 우아콘 2023, 우아한 형제들 세션 정리 # 대규모 트랜잭션을 처리하는 배민 주문시스템 규모에 따른 진화 # 성장통들 # 개선 대상 리스트 단일 장애 포인트 대용량 데이터 대규모 트랜잭션 복잡한 이벤트 아키텍처 단일 장애 포인트 # 루비라 불리는 중앙 집중 저장소에 모든 시스템이 의존 중앙 저장소의 부하 발생 해결 # 중앙 저장소 -\u0026gt; 각 시스템을 분리하는 프로젝트 진행 시스템 간 통신은 Message Queue 기반으로 통신 특정 시스템의 장애는 메시지 발행의 실패로 끝 정리 # 중앙 집중 DB의 장애, 전체 시스템의 전파 -\u0026gt; MQ를 이용한 이벤트 기반 통신으로 시스템간 영향도를 분리\n대용량 데이터 # 주문 아키텍처 정규화된 주문 DB에서 저장 + 조회가 함께 발생 주문 내역 정보에는 많은 정보가 포함되어있음 해결 # 정규화된 주문 애그리거트는 수많은 조인이 발생 -\u0026gt; 조회 성능을 높이기위해 단일 도큐먼트로 역정규화 단순 단일 도큐먼트 조회만으로 주문 정보를 가져올 수 있음 (with MongoDB) 주문 도메인 생명주기 : 주문 도메인은 생명주기에서만 도메인 변경이 발생 주문 도메인 생명주기에 발생되는 도메인 이벤트를 통해 데이터 동기화 수행 CQRS 적용 아키텍처 : 저장과 조회를 분리한 아키텍처 정리 # 대용량 데이터 RDB 조회 성능 저하 -\u0026gt; 커맨드 모델과 조회 모델을 분리, 조회 모델 역정규화를 통해 조회 성능 개\n대규모 트랜잭션 # 주문 DB의 분당 쓰기 처리량 한계치 도달 쓰기 요청의 증가 : 스펙업으로 대응 (최고사양의 스펙으로도 처리량을 감당 불가능) 해결 # 샤딩\n애플리케이션 샤딩 구현\n고민 1) 샤드 클러스터 내 어느 샤드에 접근할지 결정하는 샤딩 전략 고민 2) 여러 샤드에 있는 데이터를 애그리게이트 하는 방법에 대한 고민 샤딩 전략\nKey Based Sharding : Shard Key를 이용하여 데이터 소스를 결정하는 방식 (Hash Based Sharding) Range Based Sharding : 값의 범위(Range) 기반으로 데이터를 분산시키는 방식 Directory Based Sharding : 샤드가 어떤 데이터를 가질지 look up table을 유지하는 방식 1. Key Based Sharding # 장점 : 구현 간단, 데이터를 샤드에 골고루 분배 가능 단점 : 장비를 동적으로 추가, 제거할때 데이터 재배치 필요 2. Range Based Sharding # 장점 : 특정 값의 범위 기반으로 샤드를 결정하면 되기 때문에 구현이 간단 단점 : 데이터가 균등하게 배분되지 않아 특정 샤드에 데이터가 몰리면 Hotspot이 되어 성능 저하 발생 3. Directory Based Sharding # 장점 : 샤드 결정 로직이 Look Up Table로 분리되어있어 동적으로 샤드 추가하는데 유리 단점 : 단일 장애 포인트 주문 시스템의 특징 주문이 정상 동작하지 않으면, 서비스 전체의 좋지않은 경험으로 이어지고, 동적 주문 데이터는 최대 30일만 저장한다. -\u0026gt; 단일 장애 포인트를 피하고, 샤드 추가 이후 30일이 지나면 데이터는 다시 균등하게 분배된다.\n샤드키 주문번호를 통해 주문 순번을 알 수 있다. -\u0026gt; B1MU00584X : 주문순번 % 샤드 수 = 샤드번호\n정리 # 주문번호 샤드키를 이용한 해싱은 주문 순번 순으로 샤드에 고르게 분배된다.\n다건 조회는 대용량 처리르 위해 몽고DB를 활용했고, 저장\u0026amp;조회 로직을 분리해둔 덕분에 애플리케이션 샤딩 적용시 큰 도움이 되었다.\nN개의 샤드에 분산 저장된 데이터를 조합하여 내려주는것을 해결할 수 있었음 쓰기 요청 증가 스케일 아웃 대응 샤딩으로 인해 증가하는 트랜잭션을 스케일 아웃으로 대응 가능\n샤딩이 적용된 아키텍처 모습 복잡한 이벤트 아키텍처 # 규칙성 없는 무분별한 이벤트 발행\n주요 도메인 로직과 서비스 로직을 이벤트 기반으로 관심사를 분리 주문 시스템의 이벤트 아키텍처 문제점1. 스프링 애플리케이션 이벤트는 로직을 수행하는 주체를 파악하기 어려움 문제점2. 이벤트 유실이 발생할 경우 재처리가 어려움 해결 # 내부 / 외부 이벤트 정리 ZERO Payload 이벤트 처리기가 모든 서비스 로직을 처리 필요한 데이터는 주문저장소(MongoDB) 조회 이벤트 처리 주체의 단일화 네트워크 비용보다 이벤트 처리 주체의 단일화에 이점이 있다.\n또다른 문제 # 이벤트 발행 실패 유형 트랜잭션 내부 외부에서 발행 실패 유무에 따라 처리가 달라진다. 해결 # 트랜잭션 아웃박스 패턴\n이벤트 발행 실패와 서비스 실패를 격리하여 재발행 수단을 보장한다. 이벤트 발행 실패시, OUTBOX 엔티티에 저장된 페이로드를 재발행한다.\n유실된 데이터를 배치로 재발행 정리 # 규칙성 없는 무분별한 이벤트 발행 -\u0026gt; 이벤트 로직을 단일 애플리케이션에 위임하여 관리 포인트를 집중\nReferences https://www.youtube.com/watch?v=704qQs6KoUk "},{"id":58,"href":"/docs/springbatch/001_springbatch_start/","title":"001 Springbatch Start","section":"Springbatch","content":" 강의메모 # 스프링 배치 소개 # 탄생 배경 # 자바 기반 표준 배치 기술 부재 스프링 배치는 SpringSource(현재는 Pivotal)와 Accenture(경영 컨설팅 기업)의 합작품 Accenture - 배치 아키텍처를 구현하면서 쌓은 기술적인 경험과 노하우 SpringSource - 깊이 있는 기술적 기반과 스프링의 프로그래밍 모델 배치 핵심 패턴 # Read(데이터 조회), Process(데이터 가공), Write(데이터 저장) 배치 시나리오 # 배치 프로세스를 주기적으로 커밋 동시 다발적인 Job 의 배치 처리, 대용량 병렬 처리 실패 후 수동 또는 스케줄링에 의한 재시작 의존관계가 있는 step 여러 개를 순차적으로 처리 조건적 Flow 구성을 통한 체계적이고 유연한 배치 모델 구성 반복, 재시도, Skip 처리 아키텍처 # 도서 - 스프링배치 완벽가이드 # 1장. 배치와 스프링 # 배치가 필요한 이유 # 필요한 모든 정보를 원하는 즉시 받아볼 수는 없다. 배치 처리를 이용하면 실제 처리가 시작되기 전에 필요한 정보를 미리 수집할 수 있다.\n즉시 처리할 필요가 없는 이벤트를 적정한 시간을 두고 이후 정해진 시간에 처리할 수 있다.\n자원을 더 효율적으로 활용할 수 있다. 일반적으로 데이터 모델 처리는 두단계로 나뉜다.\n첫번째 단계. 모델의 생성 모델을 생성하려면 대량 데이터를 수학적으로 집중 처리해야하므로 시간이 많이 걸릴 수 있다. 스트리밍 시스템이 사용할 데이터 모델을 외부에서 배치 처리르 수행해 생성 두번째 단계. 생성된 모델을 놓고, 새로운 데이터를 평가하거나 점수를 매기는 것이다. 매우 빠르다. 스트리밍 시스템이 해당 결과를 실시간으로 사용하게 하는 것이 합리적 배치 처리의 역사 # 2007년 : Accenture사는 자사의 풍부한 메인프레임 및 배치 처리 경험을 바탕으로 오픈소스 프레임워크를 만듬 (JVM에서 동작하는 배치 처리의 사실상 표준) 2008년 3월 말 : 스프링 배치 1.0.0 출시 209년 4월 : 스프링배치 2.0.0 (지원하는 JDK 버전 1.4 -\u0026gt; 1.5로 교체) 2014년 : 3.0.0 버전 (새로운 자바 배치 표준인 JSR-352 구현) 책 출시일 직전에 4.0.0 버전 출시 (스프링부트에서 자바기반 구성을 받아들였다.) 배치가 직면한 과제 # 배치 처리는 사용자가 추가로 개입하지 않아도 특정 완료 지점까지 실행할 수 있다. 배치는 확실한 로그와 피드백용 알림만을 사용해 신속하고 정확하게 에러를 발생하면 된다. 알림으로 어떤 오류는 해결할 수 있기 때문이다.\n소프트웨어 아키텍처는 사용성, 유지보수성, 확장성 등 여러 속성을 가지고있다. 배치 처리에서는 이러한 속성들이 기존과는 다른 측면으로 관련되어있다.\n사용성 배치에는 신경써야할 사용자 인터페이스가 없다. 배치에서 말하는 사용성은 GUI 등과는 관계가 없다. 배치 처리에서 사용성은 코드에 관한 것이다. 즉, 오류 처리 및 유지 보수성과 관련이 있다. 공통 컴포넌트를 쉽게 확장해 새로운 기능을 추가할 수 있는가? 기존 컴포넌트를 변경할때 시스템 전체에 미치는 영향을 알 수 있도록 단위 테스트가 잘 마련되어 있는가? 잡이 실패할때 디버깅에 오랜 시간을 소비하지 않고 언제, 어디서, 왜 실패했는지 알 수 있는가? 확장성 배치가 처리할 수 있어야하는 규모는 지금껏 개발해온 웹 애플리케이션 등 보다 몇자리수 이상 큰 경우가 많다.\n가용성 배치처리는 일반적으로 항상 실행되는 것이 아니다. 필요한 리소스를 언제 사용할 수 있는지 알고있는 상태에서 주어진 시간에 job이 실행되도록 예약한다.\n필요할때 바로 배치 처리를 수행할 수 있는가? 허용된 시간 내에 잡을 수행함으로써 다른 시스템에 영향을 미치지 않게 할 수 있는가? 보안 배치 처리에서 보안의 역할은 데이터를 안전하게 저장하는 것이다. 민감한 데이터베이스 필드는 암호화돼 있는가? 실수로 개인 정보를 로그로 남기지는 않는가? 외부 시스템으로의 접근은 어떠한가? 자격증명이 필요하며 적절한 방식으로 보안을 유지하고 있는가? 처리중인 데이터는 이미 검사된 데이터이기는 하지만 여전히 규칙을 잘 준수하고 있는가? 왜 자바로 배치를 처리하는가? # 배치 처리 개발에 자바 및 오픈소스를 사용해야하는 이유로, 유지보수성, 유연성, 확장성, 개발 리소스, 지원, 비용 등을 생각할 수 있다.\n유지보수성 배치 처리 코드는 일반적으로 다른 애플리케이션 코드보다 수명이 훨씬 길다. 그러므로 큰 위험 없이 쉽게 수정할 수 있도록 코드를 작성해야한다. 스프링은 테스트 용이성, 추상화와 같은 몇가지 이점을 얻을 수 있도록 설계됐다. 스프링 프레임워크의 의존성 주입을 통해 객체간 결합을 제거할 수 있고, 강력한 테스트를 구축할 수 있다. 이러한 스프링 프레임워크 기반인 스프링 배치는 트랜잭션 및 커밋횟수와 같은 것들을 제공하고, 처리가 어디까지 실행됐는가라든가 실패시 무슨일을 해야하는지 관리할 필요가 없다. 모두 스프링배치가 알아서 관리해주기 때문에 이러한 기능은 스프링 배치와 자바가 제공하는 유지보수성 측면에서의 강력한 이점이다.\n유연성 프레임워크가 없다면 환경에 맞춰 일일이 개발해야한다. 애플리케이션 서버, 도커 컨테이너, 클라우드에 배포하고 싶다면 필요에 맞는것을 선택하면 된다. WAR, JAR 등 어떤 것이든 스프링 배치를 사용하면 된다. 유연성의 또다른 측면은 시스템 간 코드를 공유할 수 있는 능력이다. 다른 플랫폼에 종속돼 동작하던 업무 로직을 재사용할 수도 있고, 이미 테스트 및 디버깅된 서비스를 배치 처리에서 동일하게 사용할 수도 있다.\n스프링 배치 프레임워크 # 일반적인 배치 패턴 및 패러다임을 구현하는 표준 기반 방법 Accenture사와 스프링소스(SpringSource)사 간의 협업으로 개발 데이터 유효성 검증, 출력 포매팅, 복잡한 비즈니스 규칙을 재사용 가능한 방식으로 구현하는 기능, 대규모 데이터셋 처리 기능이 구현돼있다. 애플리케이션 레이어가 최상위에 있는것이 아니라, 다른 두 레이어인 코어 레이어와 인프라스트럭처 레이어를 감싸고 있는 모습이다. why? 개발자가 개발하는 대부분의 코드가 코어 레이어와 함께 동작하는 애플리케이션 레이어이며, 때로는 커스텀 리더(reader), 커스텀 라이터(writer)와 같이 인프라스트럭처의 일부를 만들기도 한다.\n애플리케이션 레이어 배치 처리 구축에 사용되는 모든 사용자 코드 및 구성 포함 업무 로직, 서비스, 잡 구조화와 관련된 구성 포함 코어 레이어와 상호작용하는데 대부분의 시간을 소비한다. 코어 레이어 배치 도메인을 정의하는 모든 부분이 포함 잡, 스텝 인터페이스, 잡 실행에 사용되는 인터페이스(JobLauncher, JobParameters) 등 포함 인프라스트럭처 레이어 어떤 처리를 수행하려면 파일, 데이터베이스 등으로부터 읽고 쓸 수 있어야한다. 잡의 실패로 재시도될때 어떤일을 수행할지도 다룰 수 있어야한다. 이러한 부분은 공통 인프라스트럭처로 간주된다. References https://www.inflearn.com/course/%EC%8A%A4%ED%94%84%EB%A7%81-%EB%B0%B0%EC%B9%98 아키텍처 이미지 : https://docs.spring.io/spring-batch/docs/4.3.x/reference/html/images/spring-batch-layers.png "},{"id":59,"href":"/docs/parallel_programming/009_thread_status/","title":"009 Thread Status","section":"Parallel Programming","content":" 강의 메모 - 스레드 생명주기와 상태 # 개요 # 자바 스레드는 생성, 실행, 종료에 따른 상태를 가지고있다. OS 스레드 상태를 의미하지 않는다. 자바 스레드는 어떤 시점이던 6가지 상태 중 오직 1개의 상태를 가질 수 있다. 자바 스레드의 현재 상태를 가져오려면 Thread의 getState() 메서드를 사용하여 가져올 수 있다. Thread 클래스에는 스레드 상태에 대한 ENUM 상수를 정의하는 Thread.State 클래스를 제공한다. 스레드 상태 # NEW : 객체만 생성된 상태 RUNNABLE WAITING : 대기는 언제하는가? 대표적으로 쓰레드가 CPU에서 할당받아서 실행하다가, I/O 입출력 관련 작업이 발생되어 CPU를 사용하지 않는 대기 상태 TIMED_WAITING : 대기 시간이 지정된 상태 BLOCKED : 락(lock) : 쓰레드가 여러개일때 공유 데이터 접근시 여러 문제가 발생하는데, 동시에 접근하지 못하도록 막는것 TERMINATED 스레드 생명주기 # Running 상태 - 스레드에는 없는 상태정보, 전체 생명주기 흐름을 이해하기 위해 실행중인 상태로 표시한것일 뿐, Runnable 상태에 해당한다.\n객체 생성 상태 스레드 객체는 생성되었지만 아직 start() 하지 않은 상태 JVM에는 객체가 존재하지만 아직 커널로의 실행은 안된 상태\nRunnable start()를 실행하면 내부적으로 커널로의 실행이 일어나고 커널 스레드로 1:1 매핑된다. 스레드는 바로 실행 상태가 아닌 언제든지 실행할 준비가 되어있는 실행 가능한 상태가 된다. 스레드가 실행 상태로 전환하기 위해서는 현재 스레드가 어떤 상태로 존재하든지, 반드시 실행 대기 상태를 거쳐야한다.\n스케줄링 실행 가능한 상태의 스레드에게 실행할 시간을 제공하는 것은 OS 스케줄러의 책임이다 스케줄러는 멀티 스레드 환경에서 각 스레드에게 고정된 시간을 할당해서 실행 상태와 실행 가능한 상태를 오가도록 스케줄링한다 (스레드가 여러개있을때 스레드마다 번갈아가면서 CPU 할당 시간을 고정해서 그만큼 수행한다)\n실행상태 스레드는 스케줄러에 의해 스케줄링이 되면 실행 상태로 전환되고, CPU를 할당받아 run() 메서드를 실행한다. 스레드는 아주 짧은 시간동안 실행된 다음 스레드가 실행될 수 있도록 CPU를 일시 중지하고 다른 스레드에 양도하게된다 (컨텍스트 스위칭) 실행 상태에서 생성과 종료 상태를 제외한 다른 상태로 전환될 때 스레드 혹은 프로세스간 컨텍스트 스위칭이 일어난다고 할 수 있다.\n실행상태 -\u0026gt; 실행 대기 상태 실행 상태에서 스레드의 yield() 메서드를 호출하거나 운영체제 스케줄러에 의해 CPU 실행을 일시 중지하는 경우 실행 가능한 상태로 전환한다.\n실행 상태로 전환\n일시 정지 상태 (지정된 시간이 있는 경우 - Timed Waiting) 스레드는 sleep 및 time-out 매개변수가 있는 메서드를 호출할때 시간이 지정된 대기 상태가 된다 스레드의 대기 시간이 길어지고 CPU의 할당을 계속 받지 못하는 상황이 발생하면 기아 상태가 발생하게 되는데 이 상황을 피할 수 있다.\n실행 대기 상태 스레드가 대기 상태의 지정 시간이 완료되거나 다른 스레드에 의해 인터럽트가 발생하거나 대기가 해제되도록 통지를 받게되면 실행 대기 상태가 된다.\n임계 영역 동시적 접근 멀티 스레드 환경에서 각 스레드가 동기화된 임계 영역에 접근을 시도 (Critical Section 접근 시도 : 임계영역; 오직 하나의 쓰레드가 접근할 수 있도록 처리된 영역)\n일시 정지 상태 (차단됨 - Blocked) 스레드가 동기화된 임계 영역에 접근을 시도하다가 Lock을 획득하지 못해서 차단된 상태 스레드는 Lock을 획득할 때까지 대기한다.\n일시 정지 상태 -\u0026gt; 실행 대기 상태 스레드가 Lock을 획득하게 되면 실행 대기 상태가 된다.\n실행 상태로 전환 실행 대기상태에서 가능\n일시 정지 상태 (Waiting 상태) 스레드가 실행 상태에서 다른 스레드가 특정 작업을 수행하기를 기다리는 상태 (자기 스스로 빠져나오지못함) wait()은 다른 스레드에 의해 notify() 받을때까지, join()은 스레드의 실행이 종료되거나 인터럽트가 발생할때까지 대기한다. joing() : 쓰레드A, 쓰레드B가 있을때 쓰레드 A에서 B.joing() 실행하면 쓰레드B의 모든 작업(run 메서드 내부) 이 모두 완료될때까지 대기하는것\n일시 정지 상태 -\u0026gt; 실행 대기 상태 wait 상태의 쓰레드가 다른 쓰레드에 의해 notify() 혹은 notifyAll()이 일어나면 실행 대기 상태가 된다 다른 스레드에 의해 인터럽트가 발생할 경우 실행 대기 상태로 전환한다\n실행 상태로 전환\n실행 종료 상태 실행이 완료되었거나 오류 또는 처리되지않은 예외와 같이 비정상적으로 종료된 상태 종료된 스레드는 종료되어 더이상 사용할 수 없다\n정리 # 스레드 생명주기와 상태를 잘 알아야, 스레드를 효과적으로 잘 운용할 수 있다.\n스레드는 어떤 상황, 시점, 조건에 의해 상태 전이가 일어나는가? 스레드의 API를 사용함에 있어 해당 API가 어떤 상태를 일으키며 스레드간 영향을 미치게 되는가? 스레드의 실행 관점에서 보면 출발지가 스레드의 start() 메서드 실행이라면 목적지는 스레드의 run() 메서드 실행이 된다는 점이다. References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":60,"href":"/docs/parallel_programming/008_thread_start/","title":"008 Thread Start","section":"Parallel Programming","content":" 강의 메모 - 스레드 실행 및 종료 # 개요 # 자바 스레드는 OS 스케줄러에 의해 실행 순서가 결정되며, 스레드 실행 시점을 JVM에서 제어할 수 없다. kernel이 제어한다. 새로운 스레드는 현재 스레드와 독립적으로 실행되고, 최대 한번 시작할 수 있고, 스레드가 종료된 이후에는 다시 시작할 수 없다.\n스레드 실행 # start() : 스레드를 실행시키는 메서드로 시스템 콜을 통해서 커널에 커널 스레드 생성을 요청한다.\nstart() 호출 후 System Call -\u0026gt; (execute) -\u0026gt; Kernel -\u0026gt; (create) -\u0026gt; Kernel Thread 순서\n메인 스레드가 새로운 스레드를 생성한다. 메인 스레드가 start() 메서드를 호출해서 스레드 실행을 시작한다. 내부적으로 네이티브 메서드인 start0()을 호출해서 커널에게 커널 스레드를 생성하도록 시스템 콜을 요청한다. 커널 스레드가 생성되고 자바 스레드와 커널스레드가 1:1 매핑이 이루어진다. 커널 스레드는 OS 스케줄러로부터 CPU 할당을 받기까지 실행대기 상태에 있다. 커널 스레드가 스케줄러에 의해 실행상태가 되면 JVM에서 매핑된 자바 스레드의 run() 메서드를 호출한다. 유저 영역의 JVM이 하나의 프로세스 Kernel 스레드 제어/정보 관리하는 TCB 존재 JVM(프로세스)를 관리하는 PCB 존재 스레드 실행 # run()\n스레드가 실행이 되면 해당 스레드에 의해 자동으로 호출되는 메서드다. Thread의 run()이 자동 호출되고 여기서 Runnable 구현체가 존재할 경우 Runnable의 run()을 실행하게된다. public static void main(String[] args)가 메인 스레드에 의해 자동으로 호출되는 것과 비슷한 원리이다. 주의할점\nstart()가 아닌 run()을 직접 호출하면 새로운 스레드가 생성되지 않고, 직접 호출한 스레드의 실행 스택에서 run)이 실행될 뿐이다. public class ThreadStartRunExample { public static void main(String[] args) { MyRunnable myRunnable = new MyRunnable(); Thread thread = new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34; :스레드 실행중..\u0026#34;); } }); thread.start(); // Thread.java \u0026gt; run () : JVM이 실행해줌 // Thread.java \u0026gt; start() \u0026gt; start0() (native 메서드) 호출 // 그 다음 우리가 정의한 run() 메서드가 호출된다. // thread.run(); // run() 즉시 호출하면, 바로 쓰레드의 run()을 호출하게된다. (thread.java \u0026gt; run()) // 위 start()였을때의 차이점은 쓰레드 자체가 생기지 않은 것 // myRunnable.run(); // main 쓰레드에서 실행됨 // 출력 이후 쓰레드 run 로직의 출력 수행 // 쓰레드 추가할때마다 쓰레드 수행 순서는 다름 (독립적) // System.out.println(\u0026#34;main 쓰레드 종료\u0026#34;); } static class MyRunnable implements Runnable{ @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34;: 스레드 실행 중...\u0026#34;); } } } run() 메서드 스택만 1개 생성될 뿐이다.\n쓰레드가 새로 생성되어 Stack 안에서 JVM이 run()을 호출한다.\n스레드 스택(stack) # 스레드가 생성되면 해당 스레드를 위한 스택(stack)이 같이 만들어진다. 스택은 각 스레드마다 독립적으로 할당되어 작동하기 때문에 스레드간 접근하거나 공유할 수 없고, 이는 스레드로부터 안전하다 할 수 있다. 스택은 OS에 따라 크기가 주어지고 주어진 크기를 넘기게되면 java.lang.StackOverFlowError가 발생하게된다. 스택의 구성 정보 # 스택에 대한 메모리 접근은 Push \u0026amp; Pop에 의한 후입선출(LIFO; Last In First Out) 순서로 이루어지며 스택은 프레임(Frame)으로 구성되어있다. 프레임은 새 메서드를 호출할 때마다 로컬 변수(지역변수, 파라미터) 및 객체 참조 변수와 함께 스택의 맨 위에 생성(push)되고 메서드 실행이 완료되면 해당 스택 프레임이 제거(pop)되고 흐름이 호출한 메서드로 돌아가며 공간이 다음 메서드에 사용 가능해진다. 스택 메모리 상태 관리 # 스택 내부의 변수는 변수를 생성한 메서드가 실행되는 동안에만 존재한다. 스택 메모리에 대한 엑세스는 Heap 메모리와 비교할때 빠르다. 스레드 종료 # 스레드는 run() 메서드의 코드가 모두 실행되면 자동으로 종료한다. 스레드는 예외가 발생할 경우 종료된다. 이는 다른 스레드에 영향을 미치지 않는다. 어플리케이션은 싱글스레드인 경우와 멀티스레드인 경우 종료 기준이 다르다. 싱글스레드 # 싱글스레드는 사용자 스레드(user thread)가 없는 기본 main thread만 있는 상태이다. main thread만 종료되면 애플리케이션은 종료된다. /** * 싱글 스레드 * 모든 작업이 끝나면 main 스레드가 종료됨 */ public class SingleThreadAppTerminatedExample { public static void main(String[] args) { int sum = 0; for (int i = 0; i \u0026lt;1000; i++) { sum += i; } System.out.println(\u0026#34;sum : \u0026#34; + sum); System.out.println(\u0026#34;메인 스레드 종료\u0026#34;); } } 멀티스레드 # 멀티스레드인 경우 JVM에서 실행하고있는 모든 스레드가 종료되어야 어플리케이션이 종료된다. 동일한 코드를 실행하는 각 스레드의 종료 시점은 처리 시간 및 OS의 스케줄링에 의해 결정되므로 매번 다르게 나올 수 있다. /** * 멀티 스레드 * main 스레드가 종료된다해서 종료되는 것이 아님 * main 스레드는 가장 먼저 종료됨 * 그 이후, 각 쓰레드가 모두 수행되고, 모두 종료가 되어야 애플리케이션이 종료된다. * 멀티 스레드의 경우 모든 쓰레드가 모두 종료되어야한다. * 쓰레드 중 단 하나라도 종료가 안되면 애플리케이션이 종료가 안된다. */ public class MultiThreadAppTerminatedExample { public static void main(String[] args) { for (int i = 0; i \u0026lt; 3; i++) { Thread thread = new Thread(new ThreadStackExample.MyRunnable(i)); thread.start(); } System.out.println(\u0026#34;메인 스레드 종료\u0026#34;); } static class MyRunnable implements Runnable{ private final int threadId; public MyRunnable(int threadId) { this.threadId = threadId; } @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34;: 스레드 실행 중...\u0026#34;); firstMethod(threadId); } private void firstMethod(int threadId) { int localValue = threadId + 100; secondMethod(localValue); } private void secondMethod(int localValue) { String objectReference = threadId + \u0026#34;: Hello World\u0026#34;; System.out.println(Thread.currentThread().getName() + \u0026#34; : 스레드 ID : \u0026#34; + threadId + \u0026#34;, Value:\u0026#34; + localValue); } } } References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":61,"href":"/docs/parallel_programming/004_cpu_bound_io_bound/","title":"004 CPU Bound Io Bound","section":"Parallel Programming","content":" 강의 메모 - CPU Bound \u0026amp; I/O Bound # 개요 # 프로세스는 CPU 작업과 I/O 작업의 연속된 흐름으로 진행된다.\nCPU 작업 # I/O 작업 # 파일을 읽는 행위 등 CPU는 실제 데이터를 읽어들이는 일을 하진 않고, 이를 다른 디바이스에 맡긴다. CPU는 다시금 연산작업을 할 수 있는 쓰레드를 할당받고, CPU는 그 쓰레드에게 실제 연산작업을 시킨다. I/O 작업이 일어날 경우, CPU는 다른 쓰레드를 선택하는거고, 해당 I/O 작업을 하고있는 쓰레드는 I/O 작업이 끝날때까지 기다려야한다. CPU 작업은 계속해서 쓰레드가 running 상태인거고, I/O 를 만나면 할당을 다시 받을때까지 wait 해야한다.\n프로세스는 CPU 명령어를 수행하다가 I/O를 만나면 대기하고 I/O 작업이 완료되면 다시 CPU 작업을 수행한다. 특정한 task가 완료될 때까지 이를 계속 반복한다. Burst # 한 작업을 짧은 시간동안 집중적으로 연속해서 처리하거나 실행하는 것\nCPU Burst # CPU를 연속적으로 사용하면서 명령어를 실행하는 구간을 의미한다. 프로세스가 CPU 명령어를 실행하는데 소비하는 시간 프로세스의 RUNNING 상태를 처리한다. I/O Burst # 연속적으로 I/O를 실행하는 구간으로 I/O 작업이 수행되는 동안 대기하는 구간 프로세스가 I/O 요청 완료를 기다리는데 걸린 시간 프로세스의 WAITING 상태를 처리한다. CPU Bounded Process # CPU Burst 작업이 많은 프로세스 I/O Burst가 거의 없는 경우에 해당한다. 머신러닝, 블록체인, 동영상 편집 프로그램 등 CPU 연산 위주의 작업을 하는 경우를 의미한다. 멀티 코어의 병렬성을 최대한 활용해서 처리 성능을 극대화 하도록 스레드를 운용한다. 일반적으로 CPU 코어 수와 스레드 수의 비율을 비슷하게 설정한다. ex) CPU 코어가 3개라면 쓰레드를 3개 만들어서 각각을 담당하도록 한다. I/O Bounded Process # I/O Burst가 빈번히 발생하는 프로세스로서 CPU Burst가 매우 잛다. 파일, 키보드, DB, 네트워크 등 외부 연결이나 입출력 장치와의 통신 작업이 많은 경우에 해당한다. CPU 코어가 많을수록 멀티 스레드의 동시성을 최대한 활용하여 CPU가 idle 상태가 되지 않도록 하고, 최적화된 스레드 수를 운용해서 CPU의 효율적인 사용을 극대화한다. CPU 코어가 많을수록 쓰레드를 적절하게 관리해야 일을 하지 않는 CPU 를 감소시킬 수 있다. References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":62,"href":"/docs/parallel_programming/005_user_kernel_systemcall/","title":"005 User Kernel Systemcall","section":"Parallel Programming","content":" 강의 메모 - 사용자 모드 \u0026amp; 커널 모드 # 개요 # 운영체제 : 컴퓨터 시스템의 자원을 효율적으로 관리하는 소프트웨어 운영체제의 여러 기능 중 핵심 기능을 담당하는 부분을 커널(kernel) 이라고 한다.\n사용자가 운영체제 위에서 실행되는 프로그램을 편하고 효율적으로 사용할 수 있게 하드웨어와 소프트웨어 간 중개자 역할을 한다. CPU, I/O 장치, 메모리, 저장소와 같은 하드웨어 자원을 프로그램에 잘 할당하는 데 있다. 운영체제는 응용 프로그램이 하드웨어 자원에 직접 접근하는 것을 방지하여 자원을 보호한다. 운영프로그램이 하드웨어 자원에 접근하려고 할때는 반드시 운영체제를 통해서만 접근하도록 한다. CPU 권한 모드 # CPU는 명령어를 실행할때 크게 두가지 권한 모드로 구분해서 실행하는데 사용자 모드(user mode)와 커널 모드(kernel mode)로 구분한다. CPU는 동작하는 동안 두가지 모드를 번갈아 가면서 수행한다. 사용자모드 (Mode Bit = 1) # 사용자 응용프로그램의 코드가 실행 되는 모드로서 메모리의 유저 영역만 접근 가능 디스크, 메모리, Printer 및 여러 I/O 장치들과 같은 특정 리소스들에 접근이 안된다. 대부분의 응용 프로그램은 입출력 장치나 파일로의 접근이 필요하기 때문에 이때는 유저모드에서 커널모드로의 전환이 되어야한다. 커널모드 (Mode Bit = 0) # 커널 영역의 코드가 실행되는 모드로서 메모리의 유저 영역, 커널 영역 모두 접근 가능 하드웨어 자원에 직접 접근할 수 있음 시스템 호출 (System Call) # 시스템 호출은 응용 프로그램이 운영체제의 커널이 제공하는 서비스를 이용할 수 있도록 커널모드에 접근하기 위한 인터페이스 응용 프로그램이 파일 입출력이나, 화면에 메시지를 출력하는 등의 기능은 커널 모드일때 CPU가 실행하기 때문에 반드시 시스템 콜을 사용해서 커널모드로 전환해야한다.\nUser Mode의 Libraries를 통해서 System Call을 호출한다.\n시스템 호출 동작 과정 # 사용자 응용 프로그램은 작업 과정에서 커널의 기능을 사용하기 위해 매우 빈번하게 시스템 콜을 요청한다. (사용자모드, 커널모드를 상호간 전환하며 실행) I/O 처리를 위해 사용자 모드와 커널 모드를 번갈아 오가는 것은 컨텍스트 스위칭과 관련이 있으며 이는 멀티 스레드 환경에서 참고해야할 중요한 배경적 지식이다. References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":63,"href":"/docs/parallel_programming/006_user_mode_kernel_mode_thread/","title":"006 User Mode Kernel Mode Thread","section":"Parallel Programming","content":" 강의 메모 - 사용자 수준 스레드 \u0026amp; 커널 수준 스레드 # 개요 # 스레드는 사용자 수준 스레드(User Level Thread), 커널 수준 스레드(Kernel Level Thread) 로 구분된다. 사용자 수준 스레드 : 사용자 프로그램에서 관리하는 스레드 커널 수준 스레드 : OS에서 관리하는 스레드 ** CPU의 할당 단위는 쓰레드다. **\n사용자 수준 스레드(User Level Thread) # 스레드 라이브러리(Pthreads, WIndows Threads, Java Threads(JVM))에 의해 스레드의 생성, 종료, 스레드간 메시지 전달, 스케줄링 스레드 보관 등 모든 것을 관리한다. 커널은 사용자 수준 스레드에 대해 알지 못하며 단일 스레드 프로세스인 것처럼 관리한다. 커널은 프로세스 안에 스레드가 1개정도 있구나~ 하고 인식하는 정도로 안다. 커널의 간섭을 받지 않는다. 커널 수준 스레드(Kernel Level Thread) # 커널이 스레드와 관련된 모든 작업을 관리한다. (PCB, TCB 관리/유지) 커널은 커널 스레드의 모든 정보를 알고있으며, 커널 스레드는 OS 스케줄러에 의해 스케줄링 된다. CPU는 커널에 의해 생성된 커널 스레드의 실행만을 담당한다. 사용자 영역의 프로세스에서는 쓰레드를 생성하는데(JAVA 등) 이 쓰레드들은 커널의 쓰레드와 아무 관련이 없다. CPU는 커널 쓰레드만 인식하는데, 어떻게 우리가 만든 쓰레드를 인식해서 CPU가 동작할까? 라는 의문이 생긴다.\n멀티스레딩 모델 # CPU는 OS Scheduler가 예약하는 커널 스레드만 할당받아 실행시키기 때문에 사용자 수준 스레드는 커널 수준 스레드와의 매핑이 필요하다. 사용자 수준 스레드는 3가지 모델로 커널 수준 스레드와 매핑하여 구현할 수 있다. 다대일 스레드 매핑 일대일 스레드 매핑 다대다 스레드 매핑 다대일 스레드 매핑 (many to one thread mapping) # 다수의 사용자 수준 스레드가 커널 수준 스레드 1개에 매핑하는 유형으로 사용자 수준의 스레드 모델이라고 볼 수 있다.\nKernel 수준 쓰레드는 사용자 수준 스레드를 아예 알지 못한다. User Space, Kernel Space는 매핑 되어도 서로 알지 못한다는 것이다. CPU는 Kernel 쓰레드만 인식한다. 커널 개입 없이 사용자 스레드끼리의 스위칭이 발생하므로 오버헤드가 적다. 스케쥴링과 동기화를 하려고 커널을 호출하지 않으므로 커널 영역으로 전환하는 오버헤드가 줄어든다. 개별 스레드 단위가 아닌 단일 스레드의 프로세스 단위로 프로세서를 할당하기 때문에 멀티코어를 활용한 병렬처리를 할 수 없다. 커널 수준 쓰레드는 사용자 수준 쓰레드를 모르므로, 커널 영역은 프로세스 안에서 하나의 쓰레드가 있다고 일단 인식하는 구조다. 만약 CPU는 4개인데, 커널 영역 쓰레드는 2개이면, 나머지 2개의 CPU는 놀게된다. 유저 쓰레드는 여러개임에도 불구하고 놀게되는 단점이 있다. (병렬처리를 할 수 있음에도 불가능) 한 쓰레드가 Block I/O가 발생하면 모든 스레드들이 Block이 발생하는데 이는 프로세스 자체를 블록하기 때문이다. 1개의 쓰레드가 I/O Block이 되면 나머지 2개는 사용이 가능해야하는데, Kernel 수준 쓰레드 영역에서는 프로세스만 인지하므로, 이 프로세스 내에 Block된 스레드가 있다면 전체 프로세스가 Block 되버린다. 쓰레드가 1개밖에 없는 것처럼 인식하므로 1개가 Block되면 프로세스 전체가 Block되는 단점이 있다. 자바에서 초기 버전의 Green Thread가 이 모델에 해당한다고 볼 수 있다. 일대일 스레드 매핑 (one to one thread mapping) # 사용자 수준 스레드와 커널 수준 스레드가 1:1로 매핑된다. 커널 수준의 스레드 모델이다.\n커널이 전체 프로세스와 스레드 정보를 유지해야하기 때문에 컨텍스트 스위칭 시 사용자 모드에서 커널 모드로 전환해서 스케줄링 하는 등의 오버헤드가 발생 자원 한정으로 인해 스레드를 무한정 생성할 수 없다. 대안으로 스레드 풀을 활용한다. 이게 단점이다. 스레드 단위로 CPU를 할당하기 때문에 멀티 코어를 활용한 병렬 처리가 가능하다. CPU가 6개일때 하나씩 할당되므로 병렬 처리가 가능하다. 스레드 중 한개가 대기상태가 되더라도 다른 스레드를 실행할 수 있다. 즉, 멀티스레드의 동시성을 활용할 수 있다. 쓰레드 1개가 Block 되더라도 나머지는 CPU 할당받고 실행될 수가 있다. 자바에서 Native Thread가 이 모델에 해당한다고 볼 수 있다. 다대다 스레드 매핑 (many to many thread mapping) # 여러개의 사용자 수준 스레드를 같은 수 또는 그보다 작은 수의 커널 수준 스레드로 매핑하는 유형이다. 각 커널 수준의 스레드가 사용자 수준의 스레드 1개 이상과 매핑된다. 다대일, 일대일 모델의 단점을 어느정도 해결한다. 개발자는 필요한 만큼 많은 사용자 수준 스레드를 생성할 수 있고, 커널 수준 스레드가 멀티 프로세서에서 병렬로 수행될 수 있다. 사용자 수준 스레드가 I/O 시스템 콜을 발생시켰을 떄, 커널이 다른 스레드의 수행을 스케줄 할 수 있다. User 수준 쓰레드 1개가 Block 되더라도, 다른 User 수준 쓰레드와 커널 수준 쓰레드가 매핑될 수 있다. References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":64,"href":"/docs/parallel_programming/007_java_thread/","title":"007 Java Thread","section":"Parallel Programming","content":" 강의 메모 - Java Thread Fundamentals \u0026gt; 스레드 생성 # 개요 # 자바 스레드는 JVM에서 User Thread를 생성할때 시스템 콜을 통해서 커널에 생성된 Kernel Thread와 1:1 매핑이 되어 최종적으로 커널에서 관리된다. JVM에서 스레드를 생성할때마다 커널에서 자바 스레드와 대응하는 커널 스레드를 생성한다. 자바에서 Platform Thread으로 정의되어 있다. 즉, OS 플랫폼에 따라 JVM이 사용자 스레드를 매핑하게 된다. Platform Thread : 운영체제에서 스케줄링되는 Kernel 쓰레드와 1:1 매핑된 플랫폼 쓰레드의 생성을 지원한다. 사용자 수준 쓰레드처럼 쓰레드 관리, 스케줄링 등을 하지않고, 생성만 하고 커널 쓰레드와 매핑만 되어있음 (커널의 제어를 받는다.) JVM에서 Platform Thread를 생성하고 (User 수준 쓰레드) 생성할때마다 Kernel Thread도 생성되고, 각각이 1:1로 매핑된다. Kernel Thread가 OS Scheduler에게 스케줄링 제어를 받는다. 1:1 매핑되기 때문에 Platform Thread도 스케줄링 제어를 받게된다. 스케줄링된 스레드는 CPU에 할당받아서 작업이 수행된다.\nThread API # Thread 생성 # 2가지 방법이 있다.\nThread 클래스를 상속하는 방법 Runnable 인터페이스를 구현하는 방법 Thread 클래스를 상속하는 방법 # 작업 내용을 스레드 내부에 직접 재정의해서 실행된다.\nRunnable 인터페이스를 구현하는 방법 # 작업 내용을 Runnable에 정의해서 스레드에 전달하면 스레드는 Runnable을 실행한다.\n다양한 스레드 생성 패턴 # Thread를 상속한 클래스 public class ExtendThreadExample { public static void main(String[] args) { MyThread myThread = new MyThread(); myThread.start(); } } class MyThread extends Thread{ @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34; :스레드 실행 중.. \u0026#34;); } } Thread 익명 클래스\npublic class AnonymousThreadClassExample { public static void main(String[] args) { Thread thread = new Thread() { @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34; :스레드 실행 중..\u0026#34;); } }; thread.start(); } } Runnable 구현\npublic class ImplementRunnableExample { public static void main(String[] args) { MyRunnable task = new MyRunnable(); Thread thread = new Thread(task); thread.start(); } } class MyRunnable implements Runnable{ @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34;: 스레드 실행 중\u0026#34;); } } Runnable 익명 클래스\npublic class AnonymousRunnableClassExample { public static void main(String[] args) { Thread thread = new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34;: 스레드 실행 중..\u0026#34;); } }); thread.start(); } } Runnable 람다 방식\npublic class LambdaThreadExample { public static void main(String[] args) { Thread thread = new Thread(() -\u0026gt; System.out.println(Thread.currentThread().getName() + \u0026#34;: 스레드 실행 중..\u0026#34;)); thread.start(); } } References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":65,"href":"/docs/parallel_programming/003_context_switching/","title":"003 Context Switching","section":"Parallel Programming","content":" 강의 메모 - ContextSwitching # 개요 # 하나의 CPU는 동일한 시간에 1개의 task만 수행 가능, 여러 프로세스를 동시에 실행X 하나의 CPU에서 여러 프로세스를 동시성으로 처리하기 위해서는 한 프로세스에서 다른 프로세스로 전환해야하는데, 이것을 컨텍스트 스위칭이라고 한다.\nContext # 프로세스 간 전환을 위해서는 이전에 어디까지 명령을 수행했고, CPU Register에는 어떤 값이 저장되어있는지에 대한 정보가 필요하다. Context는 CPU가 해당 프로세스를 실행하기 위한 프로세스의 정보를 의미하며, 이 정보들은 운영체제가 관리하는 PCB라고 하는 자료구조의 공간에 저장된다.\nPCB (Process Control Block) # 운영체제가 시스템 내의 프로세스들을 관리하기 위해 프로세스마다 유지하는 정보를 담는 커널 내의 자료구조 컨텍스트 스위칭은 CPU가 프로세스간 PCB 정보를 교체하고 캐시를 비우는 일련의 과정이라 볼 수 있다. (A 프로세스가 있고, B 프로세스가 있을때, A가 캐시에 정보를 저장하는데, 이를 B로 전환하려면 캐시 메모리엔 이미 A 프로세스의 데이터가 있기 때문에 비워줘야한다.)\n프로세스 상태 # 프로세스는 New(생성), 준비(Ready), 실행(Running), 대기(Blocked), 종료(Exit) 상태를 가진다.\n컨텍스트 스위칭이 일어나는 조건 # 실행중인 프로세스에서 I/O 호출이 일어나 해당 I/O 작업이 끝날때까지 프로세스 상태가 running -\u0026gt; waiting 으로 전이된 경우 Round Robin 스케줄링 등 운영체제의 CPU 스케줄러 알고리즘에 의해 현재 실행중인 프로세스가 사용할 수 있는 시간 자원을 모두 사용했을때, 해당 프로세스를 중지하고(ready 상태로 전이) 다른 프로세스를 실행시켜주는 경우\n컨텍스트 스위칭이 발생하는 순간에 오버헤드 발생 (이시간 동안에 CPU는 일을 하지 않는다) CPU는 PCB를 관리하는 일을 할 뿐이다.\n스레드 컨텍스트 스위칭 # TCB (Thread Control Block) # Thread 상태 정보를 저장하는 자료구조이며, PC와 Register Set (CPU 정보), 그리고 PCB를 가리키는 포인터를 가진다. 스레드가 하나 생성될 때마다 PCB 내에서 TCB가 생성되며 컨텍스트 스위칭이 일어나면 기존의 스레드 TCB를 저장하고 새로운 스레드의 TCB를 가져와 실행한다.\n프로세스 vs 스레드 # 프로세스 : 컨텍스트 스위칭을 할 때, 메모리 주소 관련 여러가지 처리(CPU 캐시 초기화, TLB 초기화 등) 을 하기 때문에 오버헤드가 크다. 쓰레드 : 프로세스 내 메모리를 공유하기 때문에 메모리 주소 관련 추가적인 작업이 없어, 프로세스에 비해 오버헤드가 작아서 컨텍스트 스위칭이 빠르다. 스레드는 생성하는 비용이 커서 많은 수의 스레드 생성은 메모리 부족 현상이 발생하거나 빈번한 컨텍스트 스위칭으로 인해 어플리케이션의 성능 저하가 될 수 있다.\nReferences 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":66,"href":"/docs/etc/002_tps/","title":"002 Tps","section":"Etc","content":" 내가 만든 서비스는 얼마나 많은 사용자가 이용할 수 있을까? # 성능 테스트는 왜 해야 할까? # 성능 테스트 : 서비스의 성능적인 부분을 측정하기 위해 실행되는 작업 애플리케이션의 성능을 측정 : 점진적인 부하를 가하는 과정 속에서 더 이상 처리량이 증가하지 않을 때, 그 수치를 측정하고 해석하는 것 목적 현재 애플리케이션이 최대 몇 명의 사용자를 수용할 수 있는지 측정하고, 그 결과가 최초 목표한 성능에 부합하는지 알아내기 위함 목표 성능에 부합하지 않는다면 어떤 지점에서 병목이 발생하고, 이를 해결하기 위해 무엇을 해야 하는지 분석하여 개선함으로써 최종적으로 서비스가 중단되는 상황 없이 제공될 수 있도록 가용성을 높이는 것 서비스가 빠른지 느린지 어떻게 알 수 있을까? # Throughput : 시간당 처리량 TPS(Transaction Per Second), RPS(Request Per Second) 등으로도 불리며, \u0026lsquo;1초에 처리하는 단위 작업의 수\u0026rsquo; 혹은 \u0026lsquo;1초에 처리하는 HTTP 요청 수\u0026rsquo; 등으로 해석 1초에 최대한 많은 작업을 처리할 수 있는 서비스가 성능 측면에서 좋은 서비스라고 볼 수 있다. ex) A 서비스는 1초에 1000개의 작업을 처리하고 B 서비스는 1초에 2000개의 작업을 처리할 수 있는 능력을 가졌다면 B 서비스가 동일 시간 내에 더 많은 작업을 할 수 있으므로 성능면에서 더 좋다고 볼 수 있다. Throughput을 보면 내 서비스의 작업 처리 능력을 알 수 있으며, 이는 서비스 성능의 지표가 될 수 있다. Latency : 서버가 클라이언트로부터 요청을 받아서 응답을 보내주기까지 걸리는 시간을 의미 서비스가 작업을 얼마나 빠르게 처리할 수 있는지를 나타내는 성능 지표 서버가 클라이언트의 요청을 처리하는데 발생하는 지연시간으로도 생각해볼 수 있다. ex) A 서비스의 웹 서버가 WAS로부터 요청을 응답을 보내는데 걸리는 시간이 100㎳이고, B 서비스의 WAS가 동일 작업을 처리하는데 50㎳가 걸렸다면 B 서비스가 작업을 더 빨리 처리할 수 있음을 알 수 있고, 이에 따라 성능면에서 더 좋다고 볼 수 있다. 서비스 성능에 대한 기준이 생겼으니, 기준에 따라 해석하고 개선해보기 # 웹 서버 - WAS - DB로 구성된 서비스를 예로 살펴보자. 전체 서비스의 Throughput은 얼마나 될까? 500 TPS\n성능에 대한 비유는 흔히 고속도로 정체 상황을 비유한다. 하나의 작업을 차 한 대로 가정하고, 500 TPS는 Web 서버라는 고속도로를 1시간당 500대의 차가 통과한다고 가정해보자. 이러한 상황에서 만약에 2000대의 차량이 해당 도로를 통과하면 어떤 일이 벌어질까? 총 TPS의 합이 3500 TPS이니까 모든 도로가 원활하게 차를 수용할 수 있을까? 역시 불가능합니다. Web 서버에서 차들은 정체되고, WAS와 데이터베이스는 Web 서버를 통과한 차들만 수용할 수 있으므로 아무리 시간당 1000대, 2000대를 통과시킬 수 있는 수용력이 있어도 결국에는 시간당 500대만 통과하게 된다. 현재 상황을 웹 서비스로 다시 돌아와 보면 이러한 경우, 웹 서버에서 병목이 발생했다고 볼 수 있다.\n여기서 DB나 WAS 성능을 아래와 같이 개선한다고 해서 전체 서비스의 성능이 올라갈까? Web 서버의 Throughput은 500 TPS로 동일하기 때문에 해당 서비스는 여전히 초당 500개의 트랜잭션만 통과할 수 있다. 이러한 병목 현상과 같이 서비스의 성능 중 가장 큰 영향을 미치는 부분을 Critical Path라고 한다. 서비스의 전체 성능을 높이기 위해서는 (병목을) Critical Path를 찾아야 하고, 여기에 해당하는 Throughput이 증가해야만 해결할 수 있다.\n다시 돌아와서 데이터베이스와 WAS는 그대로 두고 Web 서버의 성능을 개선해보자. 그림을 보면 Web서버의 Throughput이 개선되자 Web서버와 WAS에서는 2000개의 트랜잭션을 처리할 수 있게 되었다. 하지만 데이터베이스에서는 초당 2000개의 처리 작업을 수행할 수 없으니, 서비스 전체의 Throughput은 데이터베이스의 Throughput인 1000 TPS로 변경되었다. 그래도 이전 500 TPS에서 1000 TPS로 Throughput이 개선되었으니 해당 서비스의 성능은 2배 빨라졌다고 할 수 있다.\n또한 병목이 Web서버에서 데이터베이스로 옮겨갔죠? 이와 같이 기존 구간의 성능 개선을 하게 되면 Critical Path는 이동하게 되고, 우리는 이러한 병목 구간을 Critical Path를 지속적으로 해결하면서 전체 서비스의 성능을 올려야만 한다.\n종합해보면 우리가 서비스의 Throughput이라고 말하는 것은 서비스의 하위 시스템들 중 가장 낮은 처리량을 의미한다. 또한 Throughput 관점에서 성능을 개선한다는 의미는 Critical Path를 찾아서 이를 개선하는 것이라고 할 수 있다.\n다음으로 서비스의 Latency는 얼마일까? 서비스의 Latency는 대기시간을 포함한 각 하위 시스템 Latency의 총합이다. [그림 1]을 다시 참고하면 현재 서비스의 Latency는 \u0026lsquo;350ms + 대기시간\u0026rsquo;으로 해석할 수 있다.\nLatency를 개선하기 위해서 고려할 요소들은 굉장히 많다. 기본적으로 하드웨어의 처리 성능, 애플리케이션 로직, 쿼리 인덱스 등 다양한 원인으로 작업의 Latency가 발생할 수 있다. 더 나아가 Throughput이 한계점에 도달하면 대기시간 또한 길어지므로 Latency 발생의 원인이 될 수 있다.\nThroughput을 분석할 때와 달리 Latency의 경우, 하나의 하위 시스템 Latency가 줄어들면 전체 서비스의 Latency도 줄어들게 되므로 병목 구간을 찾기보다 가장 Latency가 큰 하위 시스템을 개선하는 것이 서비스의 Latency를 큰 폭으로 줄일 수 있는 방법이다.\n[그림 2]의 경우, 병목 구간이 그대로 이기 때문에 Throughput은 개선 전과 동일했지만 Latency는 WAS 20ms + 데이터베이스 50ms = 70ms 줄어들어서 전체 서비스의 대기시간을 제외한 Latency가 \u0026lsquo;350ms\u0026rsquo;에서 \u0026lsquo;280ms\u0026rsquo;로 줄어드는 것을 확인할 수 있다. 이렇듯 서비스 Latency 개선의 경우, 병목 지점과 관계없이 각 하위 시스템들의 Latency 개선이 전부 영향을 줄 수 있음을 알 수 있다.\nThroughput을 개선하면 대기시간이 줄어들어 Latency를 줄일 수 있다. Throughput을 개선하는 것이 Latency 개선에도 영향을 줄 수 있다.\n결론 # 성능 테스트는 서비스가 목표하는 최대 사용자 수에 도달하기 위해 현재 성능을 파악하고, 개선하는 작업이다. 서비스의 성능을 알 수 있는 지표는 Throughput과 Latency가 있다. 전체 서비스의 성능을 개선하기 위해서는 하위 시스템의 병목 구간을 Critical Path를 찾아 개선하여 Throughput이 증가해야만 하고, 각 하위 시스템의 Latency를 줄여서 전체 서비스의 Latency를 줄여야만 한다. TPS # 지표 예시 : Whatap Application TPS Metric TPS 계산 # Transaction Per Second(TPS) : 초당 트랜잭션의 개수이다. 실제 계산하는 방식 : 일정 기간 동안 실행된 트랜잭션의 개수를 구하고 다시 1초 구간에 대한 값으로 변경한다. 위 이미지인 와탭의 경우 : 5초 구간으로 값을 수집하기 때문에 단위시간 동안 집계된 트랜잭션의 수를 5로 나눈 값이 표시된다. 위에 그림에 두 번째 열을 보면 5개의 트랜잭션이 실행 완료된 것을 볼 수 있다. 이런 경우 TPS를 구하는 방법은 5 transaction / 5 sec이므로 결과값은 1 TPS가 된다. (와탭의 TPS 지표는 좀 더 복잡하게 계산한다. 와탭은 차트의 추세를 보여주기 위해 5초 간격으로 30초 평균 TPS를 보여주고 있다.)\nSaturation Point 와 TPS가 낮은 이유 # 서비스에 사용자가 지속적으로 늘어나면 어느 순간부터 TPS가 더 이상 증가하지 않는 상황이 발생한다. 이렇게 증가하지 않는 지점을 Saturation Point라고 한다. 위 그림은 서비스가 이상적인 상황이다. 제대로 튜닝이 되지 않은 서비스는 Saturation Point를 지나면 오히려 TPS가 떨어지기도 한다. 위 글로 보면 서비스 사용자는 300명이 넘어가면 TPS가 고정되면서 상대적으로 트랜잭션의 응답시간이 길어질 것을 예상할 수 있다.\n위 그림을 보면 동시 접속 사용자가 300명이 넘어가면 TPS는 더 이상 올라가지 않으므로 서비스의 정체 시간은 증가하기 시작한다. 300명의 요청사항에 대한 TPS가 50이라면 해당 요청 사항을 다 처리하는데 6초가 걸린다고 생각할 수 있다. 이렇게 TPS와 동시 접속자를 미리 선정해봄으로써 서비스의 성능을 상상해 볼 수 있다.\nTPS 요약 # TPS는 초당 트랜잭션의 개수를 말한다. TPS는 서비스 성능의 기준이 된다. 평소 TPS 지표를 체크하자. TPS를 통해 무슨 요일에 또는 몇시에 최대치가 되는지 확인해야한다. TPS가 더 이상 증가하지 않은 지점을 Saturation Point라고 한다. Saturation Point가 넘으면서 사용자가 몰리면 TPS가 고정된 상태에서 응답시간이 길어지게 된다. References https://hyuntaeknote.tistory.com/10 https://www.whatap.io/ko/blog/14/ "},{"id":67,"href":"/docs/redis/008_spring_session_redis/","title":"008 Spring Session Redis","section":"Redis","content":" tech blog 글 읽고 정리하기 # 제목은 Spring Session 도입기로 하겠습니다. 근데 이제 Redis를 곁들인 # 개요 # 줌인터넷의 회원 서비스는 분산 환경에서 운영되고있다. 분산 환경에서 세션 동기화 문제를 해결하기 위해 사용하고 있는 세션 저장소를 Redis로 교체하게 된 이유와 기존 아키텍처를 유지하면서 안정적으로 Spring Session으로 도입할 수 있는 방법을 소개한다.\n도입 배경 # 기존 Aerospike -\u0026gt; Redis로의 전환을 결정하게 되었다. 전환을 통해 바라는 점\nSpring Session의 도입 가능 Spring Session은 Redis 이외에도 다양한 세션 저장소를 지원하여 세션 정보를 유연하게 관리할 수 있다. 다음에 세션 저장소를 변경해야할 경우에도 유연성을 제공한다.\n유지보수 용이성 향상 팀 내에 이미 사용중이며, 신규 인프라 구축 비용이 발생하지 않는다. Aerospike 관련 레퍼런스가 부족하여 러닝 커브가 큰 점을 고려할때 Redis로 전환하게되면 학습에서 발생하는 리소스 비용을 줄일 수 있다.\n위와 같은 이유로 세션 저장소를 Aerospike에서 Redis로 전환하면서 Spring Session을 도입하게 되었다.\nSpring Session # 스프링 기반 애플리케이션에서 세션 관리를 효과적으로 처리하기 위한 기술이다. 기본적으로 스프링 세션은 세션 데이터를 서버의 메모리에 저장하는 대신 외부 스토리지에 저장하고 관리한다. Spring Session은 세션 데이터를 외부 스토리지에 저장함으로써 여러 서버 간에 세션 데이터를 공유하고 로드밸런싱과 확장성을 지원한다. 이는 분산 환경에서 여러 서버가 같은 세션 데이터를 접근하고 처리할 수 있도록 해주는 장점을 제공한다.\nSpring Session은 사용자의 세션 정보를 관리하기 위해 API 및 구현체를 제공한다.\nspringSessionRepositoryFilter (Filter 인터페이스를 구현한 빈) 서블릿 컨테이너는 모든 요청에 대해 springSessionRepositoryFilter를 사용하도록 설정해야 하지만, 이 과정은 Spring Boot가 자동으로 처리해준다. Spring Session 적용 예시 # build.gradle\ndependencies { // Spring Boot에서 Redis를 사용하기 위한 의존성입니다. implementation \u0026#39;org.springframework.boot:spring-boot-starter-data-redis\u0026#39; // Spring Session을 Redis에 저장하기 위한 의존성입니다. implementation \u0026#39;org.springframework.session:spring-session-data-redis\u0026#39; } application.yml\nserver: servlet: session: cookie: path: / # 적용될 URL 경로를 나타냅니다. 예를 들어, path를 \u0026#34;/\u0026#34;로 설정하면 해당 도메인의 모든 경로에서 쿠키가 사용될 수 있습니다. name: JSESSIONID # 이름을 지정합니다. domain: zum.com # 유효 도메인을 지정합니다. 예를 들어, domain을 \u0026#34;zum.com\u0026#34;으로 설정하면 해당 도메인과 그 서브도메인에서 쿠키가 유효합니다. http-only: true # 브라우저에서 해당 쿠키에 대한 JavaScript 접근을 제한합니다. 이를 통해 XSS 공격을 방지할 수 있습니다. secure: true # 쿠키가 HTTPS(SSL/TLS) 연결을 통해서만 전송되어야 함을 나타냅니다. 즉, HTTPS로 암호화된 연결에서만 쿠키가 전송되어야 합니다. timeout: 3600 # 세션의 유효 시간을 지정합니다. 단위는 초입니다. spring: redis: host: 127.0.0.1 port: 6379 password: session: store-type: redis # 세션 저장소를 지정합니다. redis: namespace: zum:session # 세션을 저장하는 데 사용되는 키의 네임스페이스를 지정합니다. server.servlet.session.cookie를 통해 쿠키의 속성을 지정할 수 있다. spring.session.store-type을 지정해주면 별도의 설정 없이 Spring Boot의 AutoConfiguration으로 인해 @EnableRedisHttpSession 을 추가한 것과 같다. 또한 Spring Boot는 spring.session.store-type 속성을 기반으로 실제 사용할 구현체를 결정한다. 기본적으로 인메모리 저장소인 MapSessionRepository 클래스가 적용된다. 예를 들어, Redis를 사용하려는 경우 Redis 세션 저장소 구현체인 RedisIndexedSessionRepository 클래스가 Bean으로 등록된다. 아키텍처 # AS-IS 회원 서비스는 위와 같이 각 모듈을 독립적으로 서버에 배포하여 운영하고 있다. 기존 아키텍처에서는 모듈마다 같은 ID Generator를 사용하여 세션 아이디를 생성하고 있다. 생성된 세션 아이디를 통해 각 모듈은 세션 서버에 요청하여 세션 저장소에 접근한다. 이렇게 각 모듈마다 ID Generator가 존재하는 경우 만약 세션 아이디 생성 전략이 변경되었다면, 변경된 전략을 적용하기 위해 각 모듈을 전부 재배포해야한다. 또한 새로운 모듈이 추가된다면 매번 ID Generator를 추가해야한다. 매우 낮은 확률이지만 중복된 세션 아이디가 생성될 수 있다는 가능성도 고려했다.\nTO-BE 세션 아이디 생성 전략을 담당하는 ID Generator를 하나의 모듈에서 관리하도록 변경했다.\n사용자가 각 모듈에 접근합니다. 각 모듈은 세션 서버에 세션 아이디를 요청합니다. 세션 서버는 세션 아이디를 발행하고 세션 아이디와 함께 응답합니다. 각 모듈은 발급받은 세션 아이디를 클라이언트에 전송합니다. 와 같이 ID Generator를 한 곳에서 관리하면 모듈 간 응집도가 높아지며, 필요한 변경이 있을 때도 해당 모듈만 수정하여 유지보수와 서비스의 확장을 유연하게 할수있다. ID Generator를 하나의 모듈에서 관리하고, Redis를 통해 세션 정보를 관리하도록 변경하여, 아키텍처를 개선하고, 중복 코드를 제거할 수 있다.\n하지만 ID Generator를 한 곳에서 관리되기 때문에 SPOF(Single Point of Failure)가 발생할 수 있다. SPOF란 시스템에서 단일 실패 지점으로, 해당 지점에 장애가 발생하면 전체 시스템이 영향을 받는 상황이다. 이러한 문제를 해결하기 위해 세션 서버의 인스턴스를 분산하여 구성하였다.\n코드로 살펴보는 개선 과정 # 프로젝트 환경\nJava 8 Spring Boot 2.x Spring Security 5.x AS-IS 기존 아키텍처에서 사용자가 요청 시 세션 아이디가 발행되는 과정\nSessionIdFilter 해당 필터에서는 사용자의 요청마다 세션 쿠키가 존재하는지 확인하고, 만약 존재하지 않을 때에는 ID Generator를 통해 세션 아이디를 생성하고 쿠키에 저장한다. public class SessionIdFilter extends OncePerRequestFilter { private static final ThreadLocal\u0026lt;String\u0026gt; sessionIdHolder = new ThreadLocal\u0026lt;\u0026gt;(); private static final String SESSION_KEY = \u0026#34;JSESSIONID\u0026#34;; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException { String sessionId = CookieUtils.getCookieValue(request, SESSION_KEY); // 쿠키에서 세션 아이디를 조회합니다. if (Strings.isBlank(sessionId)) { // 세션 아이디가 존재하지 않는 경우 새로운 세션 아이디를 생성합니다. sessionIdHolder.set(SessionIdGenerator.generate()); CookeUtils.addCookie(response, SESSION_KEY); } sessionIdHolder.set(sessionId); // 세션 아이디를 ThreadLocal에 저장합니다. filterChain.doFilter(request, response); // 다음 필터로 요청을 전달합니다. sessionIdHolder.remove(); // ThreadLocal에 저장된 세션 아이디를 제거합니다. } } CustomSecurityContextRepository Spring Security에서 기본적으로는 HttpSession을 사용하여 SecurityContext를 저장하고 로드한다. 다른 세션 저장소를 사용하기 위해 SecurityContextRepository를 구현한 클래스이다. SessionAdapter를 통해 세션 모듈에서 세션 정보 가져오고, SecurityContext를 생성하여 반환한다. public class CustomSecurityContextRepository implements SecurityContextRepository { private final SessionAdapter sessionAdapter; // 세션 서버와 통신하기 위한 SessionAdapter public ZumContextRepository(SessionAdapter sessionAdapter) { this.sessionAdapter = SessionAdapter; } @Override public SecurityContext loadContext(HttpRequestResponseHolder requestResponseHolder) { // 세션 아이디를 통해 세션 정보를 조회합니다. return SessionAdapter.getSession() .map(this::createSecurityContext) .orElseGet(this::emptyContext); } private SecurityContext createSecurityContext(final User user) { // 세션 정보를 통해 SecurityContext를 생성합니다. UsernamePasswordAuthenticationToken usernamePasswordAuthenticationToken = new UsernamePasswordAuthenticationToken(user, user.getAuthorities()); return new SecurityContextImpl(usernamePasswordAuthenticationToken); } private SecurityContext emptyContext() { // 세션 정보가 존재하지 않는 경우 빈 SecurityContext를 생성합니다. return SecurityContextHolder.createEmptyContext(); } } SpringSecurityConfiguration Spring Security 설정 클래스다. SecurityContextPersistenceFilter는 SecurityContextRepository를 사용하여 SecurityContext의 저장 및 로드를 처리한다. SecurityContextPersistenceFilter 이전에 실행되며, CustomSecurityContextRepository를 SecurityContextRepository로 설정한다. @EnableWebSecurity public class SpringSecurityConfiguration extends WebSecurityConfigurerAdapter { private final SessionAdapter sessionAdapter; public SecurityConfig(SessionAdapter sessionAdapter) { this.sessionAdapter = SessionAdapter; } @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .and() .securityContext() // SecurityContext를 설정합니다. .securityContextRepository(new CustomSecurityContextRepository(SessionAdapter)) // SecurityContextRepository를 설정합니다. .and() .addFilterBefore(new SessionIdFilter(), SecurityContextPersistenceFilter.class); // SessionIdFilter를 SecurityContextPersistenceFilter 이전에 실행합니다. } } TO-BE\nSessionAdapter public class SessionAdapter { @Cacheable(value = \u0026#34;session\u0026#34;, key = \u0026#34;#sessionId\u0026#34;) public ZumSessionDto getSession(String sessionId) { ResponseEntity\u0026lt;User\u0026gt; response = restTemplate.exchange( \u0026#34;http://localhost:8080/api/v2/session\u0026#34;, HttpMethod.GET, createRequestEntityWithHttpHeader(sessionId), User.class); // 세션 서버에서 세션 정보를 조회합니다. String responseSessionId = getSessionIdFromResponseHeaders(response.getHeaders()); // 세션 서버에서 응답받은 세션 ID를 가져옵니다. return ZumSessionDto.of(response.getBody(), responseSessionId); // 세션 정보를 Dto 변환합니다. } private String getSessionIdFromResponseHeaders(HttpHeaders headers) { String cookie = headers.getFirst(\u0026#34;X-SESSION-ID\u0026#34;); if (Strings.isNotBlank(cookie)) { return cookie; } return null; } private HttpEntity\u0026lt;Object\u0026gt; createRequestEntityWithHttpHeader(String sessionId) { HttpHeaders requestHeaders = new HttpHeaders(); requestHeaders.add(\u0026#34;X-Session-ID\u0026#34;, sessionId); return new HttpEntity\u0026lt;\u0026gt;(null, requestHeaders); } } SessionAdapter는 각 모듈에서 세션 서버에 요청하기 위해 존재하는 Adapter이다. 기존 아키텍처는 세션 정보를 조회하기 위해 매번 네트워크 I/O 비용이 발생하는 문제가 있었다. 여러 가지 방법을 고민해보았지만, 세션 서버 요청의 90% 이상이 GET 요청으로 이루어지고 있어, 캐싱 시스템을 도입하여 세션 정보를 조회하는 비용을 줄이기로 했다. Look-aside 전략은 캐시 시스템에서 사용되는 하나의 전략으로, 데이터를 캐시에 저장하고 검색할 때 데이터베이스 또는 백엔드 시스템에 대한 추가 작업을 최소화하는 것을 목표로 한다.\nLook-aside 전략 # 데이터를 읽거나 검색하기 전에, 먼저 캐시에서 데이터를 찾는다. 데이터가 캐시에 존재하면, 해당 데이터를 반환하고 추가적인 백엔드 작업을 수행하지 않는다. 데이터가 캐시에 존재하지 않으면, 데이터를 백엔드 시스템에서 검색한 후, 해당 데이터를 캐시에 저장하고 반환한다. 이후 같은 데이터에 대한 요청이 들어올 때는 캐시에서 데이터를 반환한다. 이러한 방식으로 Look-aside 전략은 세션 서버에 대한 요청을 최소화하고, 데이터를 빠르게 반환하여 성능을 향상시킬 수 있다. 만약 데이터의 업데이트가 발생하면, @CacheEvict를 통해 캐시를 업데이트 하도록 구현했다. 세션 정보가 필요할 때 마다 네트워크 I/O 비용이 발생하는 문제는 캐싱 시스템을 도입함으로써 해결하였으며 캐시 정합성 문제는 @CachePut를 통해 해결했다.\n코드를 보면 특이하게도 세션 서버에 요청할 때 세션 아이디를 쿠키가 아닌 헤더에 담아 요청하고 응답을 받고있다. 아래 사진처럼 Spring Session은 세션 아이디를 기본적으로 쿠키를 통해 발행하도록 구현되어 있지만 세션 아이디를 헤더에 담아서 요청과 응답을 하는 것이 현재 아키텍처에서 더욱 적합하다고 생각했다. 이유는 아래와 같다.\nSet-Cookie 헤더는 브라우저가 응답받고 쿠키를 저장하는 과정을 거치기 때문에 브라우저가 없는 서버 간 요청에서는 쿠키를 저장할 수 없습니다. 쿠키는 클라이언트 보안 정책에 따라 다르지만, 기본적으로 같은 도메인 간에만 공유된다. 하지만 헤더는 타사 도메인 간 요청에서 커스텀 헤더를 통해 데이터를 전달할 수 있다. 이는 추후 줌인터넷에서 제공하는 서비스가 만약 다른 도메인을 가질 때 유연하게 대응할 수 있다고 판단했다. 따라서 세션 아이디를 헤더에 발행하는 방법으로 Spring Session을 커스터마이징하여 구현하였다.\nCustomSessionIdResolver import org.springframework.session.web.http.HeaderHttpSessionIdResolver; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.util.List; public class CustomSessionIdResolver extends HeaderHttpSessionIdResolver { public CustomSessionIdResolver(String headerName) { super(headerName); } @Override public List\u0026lt;String\u0026gt; resolveSessionIds(HttpServletRequest request) { List\u0026lt;String\u0026gt; sessionIds = super.resolveSessionIds(request); if (sessionIds.isEmpty()) { String sessionId = request.getHeader(\u0026#34;X-Session-ID\u0026#34;); // 헤더에서 세션 아이디를 읽어옵니다. if (sessionId != null) { sessionIds.add(sessionId); } } return sessionIds; } @Override public void setSessionId(HttpServletRequest request, HttpServletResponse response, String sessionId) { super.setSessionId(request, response, sessionId); response.setHeader(HEADER_SESSION_ID, sessionId); // 헤더에 세션 아이디를 추가합니다. } } HeaderHttpSessionIdResolver 클래스는 Spring Session에서 제공하는 HttpSessionIdResolver 인터페이스의 구현체다. 이 클래스를 사용하면 세션 아이디를 HTTP 헤더에 포함하여 전달할 수 있다. 해당 클래스의 생성자는 세션 아이디를 담을 헤더의 이름을 인자로 받을 수 있다.\nHttpSessionIdResolverConfiguration import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession; import org.springframework.session.web.http.HttpSessionIdResolver; @Configuration public class HttpSessionIdResolverConfiguration { @Bean public HttpSessionIdResolver httpSessionIdResolver() { return new CustomSessionIdResolver(\u0026#34;X-Session-ID\u0026#34;); // 세션 아이디를 담을 헤더 이름을 지정합니다. } } 위의 코드를 통해 CustomSessionIdResolver 클래스가 Bean으로 등록된다.\nSessionController @RestController @RequestMapping(\u0026#34;/api/v2/session\u0026#34;) public class SessionController { private final ObjectMapper objectMapper; public SessionController(ObjectMapper objectMapper) { this.objectMapper = objectMapper; } @GetMapping User session(HttpSession httpSession) { return objectMapper.convertValue(httpSession.getAttribute(\u0026#34;user\u0026#34;), User.class); // 세션에서 user attribute를 꺼내서 응답합니다. } } 세션 서버의 역할은 간단하다. SessionController 에서는 각 모듈에서 요청한 attribute를 세션에서 꺼내서 응답한다. Spring Session을 적용하였기 때문에 기존의 HttpSession 객체는 Spring Session이 제공하는 세션 객체로 대체되며, 데이터는 Redis에 저장되고 관리된다.\n마무리 - 확인 # 세션 정보를 조회하는 별도의 모듈이 존재하고 각 서비스는 특정 모듈을 통해 세션 정보를 조회하도록 API를 제공하고 있지만, 세션 서버를 직접 호출하는 서비스가 존재할 수 있어서 이를 확인해야 했다. 아니나 다를까 우려했던 것처럼 세션 서버를 직접 호출하는 서비스가 존재했다. 회원 서비스는 전사 서비스이기 때문에 다른 서비스에 장애 전파가 발생하지 않도록 특히 주의해야한다. 기존 서비스의 수정 없이 세션 서버에서 대응할 수 있는 방법을 찾아야 했고, Nginx를 통해 세션 서버로의 요청을 가로챈 후 헤더에 세션 아이디를 담아서 요청을 보내는 방법을 선택했다.\nnginx.conf\nhttp { server { listen 80; server_name example.com; location /api/v1/session { # 특정 URL로 요청이 들어오면 set $session_id \u0026#34;\u0026#34;; # 세션 아이디를 담을 변수를 선언합니다. if ($args ~* \u0026#34;(?:^|\u0026amp;)JSESSIONID=([^\u0026amp;]+)\u0026#34;) { # URL에 세션 아이디가 담겨있다면 set $session_id $1; # 세션 아이디를 변수에 담습니다. } proxy_set_header X-Session-ID $session_id; # 세션 아이디를 헤더에 담습니다. proxy_pass http://backend; # 나머지 프록시 설정 } # 나머지 서버 설정 } 결과적으로, 서비스에 장애가 전파되지 않도록 기존에 사용하던 /api/v1/session 엔드포인트를 유지하면서 세션 저장소를 성공적으로 교체할 수 있었다.\nReferences https://zuminternet.github.io/spring-session/ "},{"id":68,"href":"/docs/parallel_programming/001_process_thread/","title":"001 Process Thread","section":"Parallel Programming","content":" 강의 메모 - Process \u0026amp; Thread # Process # File down -\u0026gt; .exe 파일 실행 -\u0026gt; 설치된 상태 : 프로그램 (!= 프로세스) 프로세스는 프로그램의 실제 실행. =\u0026gt; 프로그램 데이터들이 메모리에 올라와 CPU를 할당받고 명령을 수행하고있는 상태\n각각의 프로세스는 RAM(메모리)의 각각의 영역을 할당받음\n4GB 정도 할당 받는다고 해보자.\n1GB 정도는 운영체제를 위한 커널(Kernel) 서비스를 위해 차지한다. 나머지 3GB가 Stack, heap, data, code 등 영역을 차지한다. 프로세스는 자식 프로세스를 가질 수 있다. 운영체제는 프로세스 마다 각각 독립된 메모리 영역을 할당해준다. (Code/Data/Stack/Heap 영역) 각 프로세스 간에는 데이터 접근 안된다. 프로세스 1개에 오류가 나도 다른 프로세스에 영향을 주지 않는다.\nThread # 하나의 프로세스는 하나 이상의 스레드를 갖는다.\n프로세스 : 운영체제로부터 메모리 자원을 할당받는 것 스레드 : CPU를 할당받아서 우리의 프로그램 코드를 실행하는 것 CPU를 할당받는 주체는 즉, 프로세스가 아닌 쓰레드이다. Java의 main() 메서드\nmain()를 가진 클래스를 수행 -\u0026gt; JVM이 실행된다. (JVM이 하나의 프로세스) JVM이 실행될때 main thread가 반드시 생성된다. 각 쓰레드마다 할당받는 영역 : Stack 각 스레드가 공유하는 영역 : Heap, Code, Data Thread \u0026amp; CPU # 쓰레드가 CPU에 할당되어 우리가 짜놓은 코드를 실행한다. 쓰레드는 CPU의 최소 실행 단위가 된다. CPU는 동시에 2개의 스레드를 수행시킬 수 없고, 1개의 스레드를 스케줄러로 선점하여 수행한다. 많은 여러개의 쓰레드가 있다면, 어떤 쓰레드를 먼저 CPU가 할당할 것인지를 운영체제 스케줄러 알고리즘에 의해 결정된다. 선점이 일어날때 CPU의 실행 흐름(문백)이 전환되는 컨텍스트 스위칭이 발생한다.\nReferences 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":69,"href":"/docs/parallel_programming/002_parallel_concurrent/","title":"002 Parallel Concurrent","section":"Parallel Programming","content":" 강의 메모 - Parallel \u0026amp; Concurrent # 동시성 # 특정한 순서 없이 겹치는 기간에 시작, 실행 및 완료되는 여러 작업에 관한것 ex) 사람이 있다. 작업1, 작업2가 있다. 이 사람은 작업1, 작업2를 모두 해야한다. 작업1을 하고 작업2를 하는데, 시간적으로 동시에하는건 아니고 계속 번갈아가면서 한다. 이게 짧은 찰나로 번갈아가면서 하기 때문에 동시에 하는것처럼 보인다. (순차적이지 않다. 순서가 없다.) (시간적인 동시성이 아님)\n작업의 갯수 \u0026gt; CPU 갯수 Thread1, Thread2가 번갈아가면서 Task를 수행 빠른게 목적이 아닌, CPU의 효율적인 사용이 목적이다. 스레드가 작업을 처리할때 I/O 블록(이미지 삽입, 파일 읽기, DB 조회 등)에 걸렸을 경우 CPU 자체가 관여하지 않고 별도의 I/O 컨트롤러가 관여함 I/O 블록에 걸린 스레드를 대기시키고 다른 쓰레드를 CPU가 점유하여 작업한다. (효율성 상승) 병렬성 # 멀티 코어 프로세서에서 동시에 실행되는 동일한 작업의 여러 작업에 관한 것 ex) 사람이 2명 있다. 작업 1은 사람A가, 작업 2는 사람B가 수행한다. 이를 동시에 수행한다. (시간적으로도 동시가 맞다.) 동일한 시간에! 동시에 수행하는것 (시간적인 동시성임)\nCPU가 동시에 많은 일을 수행, 최대한 바쁘게 동작해야함 CPU \u0026gt;= 작업갯수 1개의 코어에서는 절대 병렬성을 구현할 수 없다. 동시성이 작업 처리 방식에 대한 설계에 관한것이라면, 병렬성은 하드웨어에서 계층에서 작업 수행 방식에 관한것이다. 병렬성과 동시성 조합 # ThreadPoolExecutor\n병렬성으로 처리 성능을 극대화하고 동시성으로 CPU 자원을 효율적으로 운용한다. 병렬성(Parallelism - Divide and COnquer) # ForkJoinPool\n하나의 Task를 서브 태스크로 분할하여 병렬처리함으로써 전체 작업 성능을 높인다. References 강의 : https://www.inflearn.com/course/%EC%9E%90%EB%B0%94-%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%A6%AC%EC%95%A1%ED%8B%B0%EB%B8%8C-part1 "},{"id":70,"href":"/docs/redis/007_redis_cluster_migration/","title":"007 Redis Cluster Migration","section":"Redis","content":" tech blog 글 읽고 정리하기 # 초보 개발자를 위한 Redis Cluster Migration 가이드라인 # 요청상황 # Master/Slave 구조의 Redis에서 Cluster 구조의 Redis로 migration 되니, 관련 코드 작업을 진행해야한다. 즉, Master/Slave에는 모든 key가 한 node에 있고 Cluster는 그렇지 않은 상황이다.\nMaster/Slave 구조와 Cluster 구조 # Master/Slave 구조\nMaster의 내용을 Slave에 복제하여 read/write 권한을 나눠 사용하는 구조 어떤 key가 들어오든 간에 하나의 master에서 처리를 진행 Cluster 구조\n여러대의 Master를 두어 가용성을 높인 구조 하나의 Master가 fail되면 짝을 이루고있던 Slave가 Master로 승격되어 가용성을 보장하는 구조 일반적으로 Cluster 구조에서는 3개의 node를 구성해서 사용 (경우에 따라 node의 개수 변경 가능) key를 hash한 값에 따라 들어가는 master node가 달라진다. 동시에 여러 key에 접근하지 않도록 하기 # Master/Slave에는 모든 key가 한 node에 있고 Cluster는 그렇지 않은 상황\nCluster 구조에서는 key가 한 node에 있지 않고 여러 node에 분산되어 있는데, 그렇게 되면 동시에 여러 key에 접근하는 것이 불가능하다. 여러 key에 접근한다는 의미는 Redis 연산을 수행할 때 인자로 여러 개의 key를 넘기는 상황을 의미한다. (명령어 mget, mset 등)\n# redis의 현재 상황 # {key: key1, value: \u0026#34;first\u0026#34;} # {key: key2, value: \u0026#34;second\u0026#34;} redis-cli \u0026gt; SET key1 \u0026#34;first\u0026#34; redis-cli \u0026gt; SET key2 \u0026#34;second\u0026#34; # MGET으로 2개의 key에 대한 값을 동시에 가져옴 redis-cli \u0026gt; MGET key1 key2 1) \u0026#34;first\u0026#34; 2) \u0026#34;second\u0026#34; mget의 파라미터로 2개의 key를 넘겨주고 있는데, 코드에서 이러한 부분이 보이면 수정을 진행해야한다.\nlettuce.io 에서는 Redis 연산자명 != 메서드명이다. 아래 코드를 보면 multiGet=mget 임을 알 수 있다.\n// redis 연결 RedisTemplate\u0026lt;String, Integer\u0026gt; redisTemplate = new RedisTemplate\u0026lt;\u0026gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); // keys : key들이 들어있는 ArrayList redisTemplate.opsForValue().multiGet(keys); 코드를 바꾸는 간단한 방법 for(반복문)을 돌면서 하나의 key에 대한 연산을 개별적으로 진행한다.\n// redis 연결 부분 생략 // keys : key들이 들어있는 ArrayList for(var key : keys) { redisTemplate.opsForValue().get(key); } delete의 경우도 봐야함\nvar keys = new ArrayList\u0026lt;String\u0026gt;(); // 생성 후 element를 넣었다고 가정 var key = \u0026#34;key\u0026#34;; redisTemplate.delete(key); // cluster 구조에서 문제 없음 -\u0026gt; 변경대상 아님 redisTemplate.delete(keys); // cluster 구조에서 문제 있음 -\u0026gt; 변경대상 트랜잭션 전면 재검토 # 동시에 여러 key에 접근할 수 없게 만들었던 원흉인 Cluster의 key에 따른 node 분배 때문에 \u0026lsquo;트랜잭션\u0026rsquo;을 신경써야한다. 트랜잭션은 같은 node 안에서만 실행할 수 있기에, key의 hash값에 따라 들어가는 node가 달라지는 Cluster 구조에서는 트랜잭션을 적용하는 것이 불가능하다. Redis는 트랜잭션을 할 수 있도록 나름의 방법인 hash tag를 제공하고있다.\n하나의 node에 몰아넣는 방법 - hash tags # Redis의 key를 중괄호로 묶으면, 중괄호 내의 key를 hash한 결과값을 바탕으로 node에 할당한다.\nkey:{group}:test key:group:test key:{group}:hello hash tag : 여기서 group이라는 문자열이 중괄호로 묶여 있는 것 hash tag가 없는 key는 key 전체를 hash하는 반면, hash tag를 포함한 key는 hash tag 내부에 있는 문자열에만 hash를 진행한다. 따라서 같은 hash tag를 가진 다른 key도 같은 node에 들어가는 것을 보장할 수 있다. References https://dev.gmarket.com/71 "},{"id":71,"href":"/docs/etc/001_jmeter/","title":"001 Jmeter","section":"Etc","content":" 설치 # Jmeter 설치 터미널에서 수행 brew install jmeter jmeter 실행 터미널에서 수행 open /opt/homebrew/bin/Jmeter 사용방법 # Request Number Of Thread (users) : 몇 개의 쓰레드(유저 수)로 테스트할 지 Ramp-up period (seconds) : {Number of Thread} 만큼의 쓰레드를 몇초에 걸쳐서 만들 지 Loop Count : 요청을 몇번을 반복할 지 (설정된 값에 따라 Number of Threads * Ramp-up period 만큼 요청을 다시 보낸다.) Response Label : Sampler 명 Samples : 샘플 실행 수 (Number of Threads X Ramp-up period) Average : 평균 걸린 시간 (ms) Min : 최소 Max : 최대 Std. Dev. : 표준편차 (요청에 대한 응답시간의 일정하고 안정적인가를 확인, 값이 작을수록 안정적이다.) Error % : 에러율 Throughput : 분당 처리량 (또는 초당 처리되는 요청 수) Received KB/sec : 초당 받은 데이터량 Sent KB/sec : 초당 보낸 데이터량 Avg. Bytes : 서버로부터 받은 데이터 평균 References https://jaeyeong951.medium.com/kotlin-lambda-with-receiver-5c2cccd8265a https://0soo.tistory.com/220 "},{"id":72,"href":"/docs/rdbms/004_window_function/","title":"004 Window Function","section":"Rdbms","content":" Analytic SQL - 집계(Aggregate) Analytic과 Window 상세 # Aggregate Functions vs Window Functions # Aggregate Functions Group by는 원본 데이터 집합의 레벨을 변경하여 적용 Window Functions Analytic SQL은 원본 데이터 집합의 레벨을 그대로 유지하면서 적용 Window를 이용하여 Row 단위의 집합 연산 수행 가능 원본 데이터의 레벨을 그대로 유지하면서, 그룹핑 레벨에서 자유롭게 Window의 이동과 크기를 조절하면서 Analytic을 수행 Analytic SQL 적용 로직 # \u0026lt;Analytic function\u0026gt; (인자1, ...) OVER ( [Partition절] [Sorting절] [Window절] ) 자유로운 window 설정에 따른 analytic 구사가 가능하므로, SQL의 Analytic 함수를 window 함수로도 지칭한다. 순위 Analytic # rank 공동 순위가 있을 경우 다음 순위는 공동 순위 개수만큼 밀려서 정함 1,2,2,4 또는 1,2,2,2,5 dense_rank 공동 순위가 있더라도 다음 순위는 바로 이어서 정함 1,2,2,3 또는 1,2,2,2,3 row_number 공동 순위가 있더라도 반드시 unique한 순위를 정함 1,2,3,4,5 NULL 처리\nNulls first : NULL을 최우선 순위로 (default) Nulls las : NULL을 가장 마지막 순위로 rank() OVER (\u0026lt;Partition절\u0026gt; order by column [nulls first/last]) 집계(aggregate) 계열 analytic 함수 # sum(), max(), min(), avg(), count() 등 order by절이 있을 경우 window 절은 기본적으로 range unbounded preceding and current row이다. order by절이 없다면 window는 해당 partition의 모든 row를 대상으로 한다. partition, order by절 모두 없다면 window는 전체 데이터의 row를 대상으로한다. range와 rows의 차이 # range : range unbounded preceding and current row rows : rows between unbounded preceding and current now 동일한 order by컬럼 값이 있을 경우 range : 중복되는 값들 모두를 대상으로 한다. current row를 자신의 row가 아닌 동일 값이 있는 전체 row를 동일 그룹으로 간주하여 집계에 적용한다. rows : 중복이 되더라도 current row를 대상으로 한다. 실습코드 # 05_1_window_함수실습.sql 05_2_rank_함수실습.sql 05_3_avg_이동평균_실습.sql 05_4_불연속_일자데이터_유의사항.sql 05_5_range_rows_유의사항.sql 06_1_lead_lag절_실습.sql 06_2_first_value_last_value.sql "},{"id":73,"href":"/docs/rdbms/005_subquery/","title":"005 Subquery","section":"Rdbms","content":" 섹션 8. 서브 쿼리(Sub-query) # 서브쿼리(Sub-query) # 서브쿼리는 하나의 쿼리 내에 또다른 쿼리가 포함되어있는 쿼리를 의미 서브 쿼리는 메인 쿼리(Main Query) 내에 포함되어있는 관계 Where절에 사용될 경우 복잡한 업무적인 조건을 직관적인 SQL로 표현하여 필터링하는데 주로 사용됨 서브쿼리 유형 # Where절에 사용되는 서브쿼리 -- 평균 급여 이상의 급여를 받는 직원 select * from hr.emp where sal \u0026gt;= (select avg(sal) from hr.emp); -- 가장 최근 급여 정보 select * from hr.emp_salary_hist a where todate = (select max(todate) from hr.emp_salary_hist x where a.empno = x.empno); Select 절에 사용되는 서브쿼리 = 스칼라 서브쿼리 select ename, deptno, (select dname from hr.dept x where x.deptno=a.deptno) as dname from hr.emp a; From 절에 사용되는 서브쿼리 = 인라인 뷰 select a.deptno, b.dname, a.sum_sal from ( select deptno, sum(sal) as sum_sal from hr.emp group by deptno ) a join hr.dept b on a.deptno = b.deptno; 서브쿼리 특징 # 서브쿼리는 메인 쿼리에 where 조건으로 값을 전달하거나 메인쿼리와 연결되어 메인 쿼리의 필터링 작업을 수행 서브쿼리 컬럼은 메인쿼리로 전달 불가능 메인쿼리의 컬럼은 서브쿼리에서 사용 가능 서브쿼리와 메인쿼리로 연결시 메인쿼리의 집합 레벨이 바뀌지않음 메인쿼리와 서브쿼리 연결시 서브쿼리는 연결 컬럼으로 무조건 유니크한 집합, 즉 1의 집합이 되므로 메인쿼리의 집합 레벨을 변경하지않음 Order와 Order_Items는 order_id를 기준으로 하면 1:M 연결관계를 가진다. 하지만 서브쿼리는 메인 쿼리 집합을 변화시키지 않으므로 결과 집합은 orders 집합 레벨이다. select * from nw.orders a where order_id in (select order_id from nw.order_items where amount \u0026gt; 100); 서브쿼리 활용 # 비상관(non-correlated) 서브쿼리 서브쿼리 자체적으로 메인쿼리에 값을 전달 상관(correlated) 서브쿼리 서브쿼리 내에 메인쿼리의 연결 컬럼을 가지고 있음 Exists 연산자 # 메인쿼리의 레코드 별로 서브쿼리의 결과가 한건이라도 존재하면 true가 되어 메인쿼리의 결과를 반환한다. 문맥적으로 메인쿼리의 레코드가 서브쿼리에서 존재하는지를 체크하기 위해 활용 Not in vs Not Exists # IN 연산자 여러개의 값이 입력될 경우 개별 값의 = 조건들의 OR 연산이 적용된다. SELECT * FROM HR.EMP WHERE DEPTNO IN (20, 30, NULL); SELECT * FROM HR.EMP WHERE DEPTNO = 20 OR DEPTNO=30 OR DEPTNO = NULL; NOT IN 연산자\n여러개의 개별 = 조건들이 NOT(조건=) AND NOT(조건=)로 변환되어 Null 적용시 직관가 다른 결과가 발생 SELECT * FROM HR.EMP WHERE DEPTNO NOT IN (20, 30, NULL); SELECT * FROM HR.EMP WHERE DEPTNO != 20 AND DEPTNO != 30 AND DEPTNO != NULL; AND DEPTNO != NULL; 조건 때문에 결과가 없다. (자체로 NULL이다.) IS NOT NULL 이랑은 다름 여러조건 결합시, True and Null 은 Null 여러조건 결합시, True or Null은 True NOT EXISTS 연산자\nSELECT A.* FROM NW.REGION A WHERE NOT EXISTS (SELECT SHIP_REGION FROM NW.ORDERS X WHERE X.SHIP_REGION = A.REGION_NAME --AND A.REGION_NAME IS NOT NULL ); 서브쿼리 반환값이 NULL이므로(반환값이 없다) NOT EXISTS 문은 true가 된다. NOT EXISTS의 경우 NULL 값을 제외하려면 서브쿼리가 아닌 메인쿼리 영역에서 제외해야한다.\n틀린 예제\nSELECT A.* FROM NW.REGION A WHERE NOT EXISTS (SELECT SHIP_REGION FROM NW.ORDERS X WHERE X.SHIP_REGION = A.REGION_NAME AND A.REGION_NAME IS NOT NULL ); 올바른 예제\nSELECT A.* FROM NW.REGION A WHERE NOT EXISTS (SELECT SHIP_REGION FROM NW.ORDERS X WHERE X.SHIP_REGION = A.REGION_NAME ) AND A.REGION_NAME IS NOT NULL; Null 조합 # -- TRUE SELECT 1=1; -- FALSE SELECT 1=2; -- NULL SELECT NULL = NULL; -- NULL SELECT 1=1 AND NULL; -- NULL SELECT 1=1 AND (NULL = NULL); -- TRUE SELECT 1=1 OR NULL; -- FALSE SELECT NOT 1=1; -- NULL SELECT NOT NULL; 스칼라 서브쿼리 # Select 절에 사용할 수 있는 서브쿼리로 상관 서브쿼리와 유사하게 동작 단 한 개의 로우, 단 한 개의 컬럼값만 반환할 수 있음. 조인과 유사하게 from절의 집합과 연결되어 원하는 결과를 추출할 수 있으며, from절의 집합 레벨을 변화 시키지 않음. From 절의 집합에서 한 건씩 스칼라 서브쿼리의 연결되어 값을 반환하므로 from절 집합이 커질 경우 수행 속도가 저하됨 스칼라 서브쿼리는 Left Outer Join으로 대체 될 수 있음 SELECT A.*, (SELECT DNAME FROM HR.DEPT X WHERE X.DEPTNO=A.DEPTNO) AS DNAME FROM HR.EMP A; 실습코드 # 07_1_서브쿼리_유형.sql 07_2_where절_서브쿼리.sql 07_3_non-correlated_서브쿼리.sql 07_4_correlated_서브쿼리.sql 07_5_서브쿼리_실습.sql 07_6_not_in_not_exists_차이.sql 07_7_스칼라_서브쿼리.sql "},{"id":74,"href":"/docs/clean_code/001_argument/","title":"001 Argument","section":"Clean Code","content":" tech blog 글 읽고 정리하기 # 인자가 많은 메서드는 왜 나쁠까? # 상황 # 재전송, 메일 수신자 필터링, SMS 전송(fallback) 등 다양한 기능을 제공하는 메일 발송 기능이 있을때. 이 기능을 하는 메서드의 인자가 11개 정도 있다면?\nclass Mail( // ... ) { fun send( phoneFallback: Boolean?, phoneNumber: String?, isForceSend: Boolean?, recipient: String, id: Long, mailDomainFilterService: MailDomainFilterService?, mailRetryService: MailRetryService?, title: String, body: String, param: Map\u0026lt;Any, Any\u0026gt;, reservedAt: Instant?, ) } 위 클래스를 다음과 같이 호출한다. mail.send( phoneFallback = null, phoneNumber = null, isForceSend = null, recipient = \u0026#34;jaeeun.na@tosspayments.com\u0026#34;, id = -1, mailDomainFilterService = null, mailRetryService = null, title = \u0026#34;안녕하세요\u0026#34;, body = \u0026#34;메일 본문입니다\u0026#34;, param: emptyMap(), reservedAt: null, ) 각각의 파라미터가 어떤 역할을 하는지 알겠는가? 메일 제목, 내용, 받는사람만 채워서 메일 한통을 보내고 싶을 뿐인데, 이렇게 인자가 많고 의미를 파악하기 어려우면 메서드 사용성이 떨어진다. 샌상성 악마의 미소 # 비슷한 역할을 하는 Gmail UI를 한번 보자. 받는 사람만 입력하면 본문 내용이 없어도 메일이 발송된다. 어떤 값이 필수인지도 바로 알 수 있다. class Mail( // ... ) { fun send( phoneFallback: Boolean?, phoneNumber: String?, isForceSend: Boolean?, recipient: String, id: Long, mailDomainFilterService: MailDomainFilterService?, mailRetryService: MailRetryService?, title: String, body: String, param: Map\u0026lt;Any, Any\u0026gt;, reservedAt: Instant?, ) } Mail.send() 메서드에는 입력해야 하는 \u0026lsquo;필수\u0026rsquo; 인자가 너무 많다. 잘 모르는인자, 사용할 필요 없는 인자에 null을 채워 넣는 것도 불편하다. 다른 선택지를 고민\nsend 메서드는 다른 곳에서 어떻게 쓰고있을까? 비슷하게 따라해야겠다. 아무 인자나 넣고 일단 실행해볼까? 실행에 성공하면 좋고, 예외가 생긴다면 그때 확인해야지. send 메서드 구현은 어떻게 되어있지? 각 인자의 의미를 파악해보자. 어떤 선택지를 고르더라도 모두 생산성은 낮다. 어떤 선택지 하나를 고르는 순간, 생산성 악마는 이런 질문을 한다.\nsend 메서드는 다른 곳에서 어떻게 쓰고있을까? 비슷하게 따라해야겠다. 메서드 실행에 과연 성공할까? 올바른지 검증할 수 있을까? 따라한 코드의 맥락과 사용하는 맥락이 같은가? 아무 인자나 넣고 일단 실행해볼까? 실행에 성공하면 좋고, 예외가 생긴다면 그때 확인해야지. 이 상태로 라이브 배포를 할 수 있을까? 내가 만드는 코드를 정확히 이해하지 못해도 괜찮은가? send 메서드 구현은 어떻게 되어있지? 각 인자의 의미를 파악해보자. 내부 구현을 정말로 이해할 수 있을까? 그 시간에 다른 기능을 만드는게 낫지 않을까? 질문에 답을 하다보면 세가지 선택지 모두 좋은 접근이 아니다. 문제를 해결하기 위해서는 근본적이고 장기적인 해결책이 필요하다.\n함께 사용하는 인자는 하나로 묶는다. # 이해하기 어려운 인자부터 하나씩 봐보자.\nphoneFallback 인자가 뭘까? 메일 발송이 실패했을때 메일이 있는 내용을 문자로 대신 보내기 위한 인자 phoneNumber 은? 위 phoneFallback이 true라면 phoneNumber에 적힌 번호로 메일 내용이 전송된다. 즉, 1) phoneFallback과 2) phoneNumber은 항상 같이 사용되는 인자다.\n개선한 코드\n// 아래 두 변수가 둘 다 null이거나, 둘 다 null이 아니어야 한다는 사실을 // 더 이상 기억하고 있을 필요가 없다 data class FallbackFeatureOption( val phoneFallback: Boolean, val phoneNumber: String, ) class Mail( // ... ) { fun send( fallbackFeatureOption: FallbackFeatureOption?, isForceSend: Boolean?, recipient: String, id: Long, mailDomainFilterService: MailDomainFilterService?, mailRetryService: MailRetryService?, title: String, body: String, param: Map\u0026lt;Any, Any\u0026gt;, reservedAt: Instant?, ) { sendInternal( phoneFallback = fallbackFeatureOption?.phoneFallback, phoneNumber = fallbackFeatureOption?.phoneNumber, isForceSend = isForceSend, recipient = recipient, id = id, mailDomainFilterService = mailDomainFilterService, mailRetryService = mailRetryService, title = title, body = body, param = param, reservedAt = reservedAt, ) } @Deprecated(\u0026#34;이 메서드는 너무 인자가 많아서 코드를 이해하기 어렵습니다. 다른 send() 메서드를 사용하세요.\u0026#34;) fun sendInternal( phoneFallback: Boolean?, phoneNumber: String?, isForceSend: Boolean?, recipient: String, id: Long, mailDomainFilterService: MailDomainFilterService?, mailRetryService: MailRetryService?, title: String, body: String, param: Map\u0026lt;Any, Any\u0026gt;, reservedAt: Instant?, ) { ... } } 아직 완벽한 코드는 아니지만, 코드 사용자가 기억할 내용을 하나 줄였다.\nfallbackFeatureOption에 null을 넣어야한다는게 맘에 안든다. 문자 발송은 더이상 신경쓰기 싫다.\nfun send( // fallbackFeatureOption 인자를 삭제했다 // ... ) { sendInternal( phoneFallback = null, phoneNumber = null, // ... ) } // 메서드 이름으로 의도를 표현했다 fun sendWithFallback( fallbackFeatureOption: FallbackFeatureOption, // non-nullable // ... ) { sendInternal( phoneFallback = fallbackFeatureOption.phoneFallback, phoneNumber = fallbackFeatureOption.phoneNumber, // ... ) } send() 메서드가 노출하고 있던 phoneFallback 맥락이 사라졌다. -\u0026gt; sendWithFallback() 별도 메서드를 추가함 phoneFallback이 false 인데 phoneNumber 값을 넣어야해서 여전히 어색하다. 문자를 보내지 않을건데 휴대폰 번호를 넣어야하니깐. (조금 뒤에서 다시 개선해보자.)\n관련 없는 것은 분리한다. # isForceSend은? 메일 수신을 거부한 이메일 주소에도 발송 기록을 남겨야하는 정책 때문에 사용하는 인자다. 즉, isForceSend를 true로 설정하면 수신 거부한 주소에도 메일을 발송할 수 있다. 이메일 수신 거부 목록은 데이터베이스에 들어있고, MailDomainFilterService에서 데이터베이스에 접근해서 발송할지 결정한다. class MailDomainFilterService { fun isFiltered(domain: String): Boolean { // database 에서 목록을 가져온 뒤, 발송 여부 결정 } } isForceSend가 true이면 mailDomainFilterService 인자가 사용되지 않는다. -\u0026gt; isForceSend가 true여도 메일을 보내야하기 때문\nclass Mail( private val mailDomainFilterService: MailDomainFilterService, ) { fun send( // ... ) fun sendWithFallback( // 기존과 동일 ) // 수신 거부 주소에도 메일 발송 fun sendWithFallbackAndForced( fallbackFeatureOption: FallbackFeatureOption?, recipient: String, id: Long, mailRetryService: MailRetryService?, title: String, body: String, param: Map\u0026lt;Any, Any\u0026gt;, reservedAt: Instant?, ) { sendInternal( phoneFallback = fallbackFeatureOption?.phoneFallback, phoneNumber = fallbackFeatureOption?.phoneNumber, isForceSend = true, recipient = recipient, id = id, mailDomainFilterService = null, mailRetryService = mailRetryService, title = title, body = body, param = param, reservedAt = reservedAt, ) } } sendWithFallbackAndForced() 메서드를 추가해서, isForceSend가 true다.\n문제 발생 # fallbackFeatureOption 기능과 isForceSend 기능이 서로 겹치기 때문에, 다음과 같이 5개의 메서드를 만들어야하는 상황이다.\n1) send() 2) sendInternal() 3) sendWithFallbackAndForced() 4) sendWithFallback() 5) sendWithForce() 연관이 없는 메서드들은 독립적으로 사용하도록 해보자.\nclass Mail( private val mailDomainFilterService: MailDomainFilterService ) { private var enableForceSendFeatureForCompliancePurpose: Boolean = false private var smsFallbackFeatureOption: SmsFallbackFeatureOption? = null fun enableSmsFallbackFeature(smsFallbackFeatureOption: SmsFallbackFeatureOption): Mail { this.smsFallbackFeatureOption = smsFallbackFeatureOption return this } // isForceSend 라는 이름에서, 조금 더 서술적인 이름으로 변경했다. fun enableForceSendFeatureForCompliancePurpose(): Mail { this.enableForceSendFeatureForCompliancePurpose = true return this } fun send( id: Long, recipient: String, mailRetryService: MailRetryService?, title: String, body: String, param: Map\u0026lt;Any, Any\u0026gt;, reservedAt: Instant?, ) { // 이제 인자가 아닌 객체 필드에서 값을 얻어올 수 있다. sendInternal( phoneFallback = this.smsFallbackFeatureOption?.phoneFallback, phoneNumber = this.smsFallbackFeatureOption?.phoneNumber, isForceSend = this.enableForceSendFeatureForCompliancePurpose, recipient = recipient, id = id, mailDomainFilterService = if (this.enableForceSendFeatureForCompliancePurpose) { this.mailDomainFilterService } else { null }, mailRetryService = mailRetryService, title = title, body = body, param = param, reservedAt = reservedAt, ) } } SmsFallbackFeatureOption 클래스를 마저 개선해보자. # // sms fallback 기능을 사용하겠다고 했는데 true를 입력하는게 어색하다. mail .enableSmsFallbackFeature( SmsFallbackFeatureOption(true, \u0026#34;010-1234-5678\u0026#34;) ) .send(...) // sms fallback 기능을 사용하겠다고 했는데 false를 입력하면 어떻게 되는거지...? mail .enableSmsFallbackFeature( SmsFallbackFeatureOption(false, \u0026#34;???\u0026#34;) ) .send(...) 불필요한 인자를 삭제하고 이름을 개선하자. # class Mail( private val mailDomainFilterService: MailDomainFilterService ) { private var enableForceSendFeatureForCompliancePurpose: Boolean = false private var smsFallbackFeaturePhoneNumber: String? = null fun enableSmsFallbackFeature(smsFallbackFeaturePhoneNumber: String): Mail { this.smsFallbackFeaturePhoneNumber = smsFallbackFeaturePhoneNumber } fun enableForceSendFeatureForCompliancePurpose(): Mail { this.enableForceSendFeatureForCompliancePurpose = true } fun send( id: Long, recipient: String, mailRetryService: MailRetryService?, title: String, body: String, param: Map\u0026lt;Any, Any\u0026gt;, reservedAt: Instant?, ) { sendInternal( phoneFallback = this.smsFallbackFeaturePhoneNumber != null, phoneNumber = this.smsFallbackFeaturePhoneNumber, isForceSend = this.enableForceSendFeatureForCompliancePurpose, recipient = recipient, id = id, mailDomainFilterService = if (this.enableForceSendFeatureForCompliancePurpose) { this.mailDomainFilterService } else { null }, mailRetryService = mailRetryService, title = title, body = body, param = param, reservedAt = reservedAt, ) } } 이제 좀더 편리하게 사용할 수 있게 되었다. 메서드 이름을 보고 충분히 기능을 유추할 수 있게 되엇고, send() 메서드의 주의사항을 더 기억하지 않아도 된다. 사용하고 싶은 기능을 호출하기만 하면 된다.\nmail.enableSmsFallbackFeature(\u0026#34;010-1234-5678\u0026#34;) // 이 메서드를 호출하지 않아도 괜찮다 mail.enableForceSendFeatureForCompliancePurpose() // 이 메서드를 호출하지 않아도 괜찮다 .send(...) 가장 중요한 인자만 남긴다. # class Mail( private val mailDomainFilterService: MailDomainFilterService, private val mailRetryService: MailRetryService, private val param: Map\u0026lt;Any, Any\u0026gt;?, ) { private var enableForceSendFeatureForCompliancePurpose: Boolean = false private var smsFallbackFeaturePhoneNumber: String? = null private var scheduleSendReservedAt: Instant? = null private var enableRetryFeature: Boolean = false fun enableSmsFallbackFeature(smsFallbackFeaturePhoneNumber: String): Mail { this.smsFallbackFeaturePhoneNumber = smsFallbackFeaturePhoneNumber } fun enableForceSendFeatureForCompliancePurpose(): Mail { this.enableForceSendFeatureForCompliancePurpose = true } fun enableScheduleSendFeature(reservedAt: Instant): Mail { this.scheduleSendReservedAt = reservedAt } fun enableRetryFeature(): Mail { this.enableRetryFeature = true } fun send( recipient: String, title: String, body: String, ) { sendInternal( phoneFallback = this.smsFallbackFeaturePhoneNumber != null, phoneNumber = this.smsFallbackFeaturePhoneNumber, isForceSend = enableForceSendFeatureForCompliancePurpose, id = Random.nextLong(), // 내부 구현에서 랜덤으로 생성해서 사용함 mailDomainFilterService = if (this.enableForceSendFeatureForCompliancePurpose) { this.mailDomainFilterService } else { null }, mailRetryService = if (this.enableRetryFeature) { this.mailRetryService } else { null }, title = title, body = body, param = emptyMap(), reservedAt = this.scheduleSendReservedAt, ) } fun templateSend( recipient: String, title: String, body: String, param: Map\u0026lt;Any, Any\u0026gt; ) { sendInternal( // ... param = param // ... ) } } sendInternal 메서드 구현을 전혀 건드리지 않고도 이젠 코드에 내가 원하는 값만 넣을 수 있게 되었다. 코드를 읽기도, 사용하기도 편해졌다.\n// 가장 간단하게 사용할 때 mail.send( recipient = \u0026#34;jaeeun.na@tosspayments.com\u0026#34;, title = \u0026#34;안녕하세요\u0026#34;, body = \u0026#34;메일 본문입니다\u0026#34;\t) // 지원하는 모든 기능을 사용할 때 mail.enableSmsFallbackFeature(\u0026#34;010-1234-5678\u0026#34;) .enableForceSendFeatureForCompliancePurpose() .enableScheduleSendFeature(Instant.now().plus(Duration.ofHours(2))) .enableRetryFeature() .send( recipient = \u0026#34;jaeeun.na@tosspayments.com\u0026#34;, title = \u0026#34;안녕하세요\u0026#34;, body = \u0026#34;메일 본문입니다\u0026#34; ) References https://toss.tech/article/engineering-note-4 "},{"id":75,"href":"/docs/rdbms/001_basic_join/","title":"001 Basic Join","section":"Rdbms","content":" 조인(Join) - 조인 기반 메커니즘 # 조인 # 2개 이상의 테이블을 서로 연결하여 데이터 추출 관계형 DB에서는 조인을 통해 서로 다른 테이블간의 정보를 원하는 대로 가져올 수 있음 중요한 부분 # 1:M 조인시, 결과 집합은 M 집합의 레벨을 그대로 유지한다. 조인 결과 : M 집합 Left Outer Join # Right Outer Join은 사용하지 말자. -\u0026gt; 테이블 위치를 바꾸면 Left Outer join처럼 사용 가능 Left Outer Join 또다른 표기 (+) # SELECT * FROM TEST1 A, TEST2 B WHERE A.COL1 = B.COL1(+) -- SELECT * FROM TEST1 A LEFT OUTER JOIN TEST2 B ON A.COL1 = B.COL1 실습코드 # 01_조인실습.sql 02_1_outer_조인실습.sql 02_2_full_outer_조인실습.sql 02_3_non_equi_조인_cross_조인실습.sql "},{"id":76,"href":"/docs/rdbms/002_date_interval/","title":"002 Date Interval","section":"Rdbms","content":" Date, Timestamp, Interval 다루기 # Date 일자로서 년, 월, 일 정보를 가짐. YYYY-MM-DD Timestamp 일자를 시간 정보까지 같이 가짐. YYYY-MM-DD HH24:MI:SS Time 오직 시간 정보만 가짐. HH24:MI:SS Interval N days HH24:MI_SS pattern # hh24 하루중 시간(00-23) hh12 하루중 시간(01-12) mi 분(00-59) ss 초(00-59) yyyy 년도 mm 월(01-12) dd 일(월중 일자 01-31) month 월 이름 day 요일 이름 w 월의 주(1-5) ww 년의 주(1-52) d 요일. 일요일(1) ~ 토요일(7) am 또는 pm AM 또는 PM 표시 tz 시간대 interval 활용 # Date 타입에 숫자값을 더하거나/빼면 숫자값에 해당하는 일자를 더하거나/빼서 날짜 계산 곱셈/나눗셈은 불가능 -- DATE 타입에 곱하기나 나누기는 할 수 없음. SELECT TO_DATE(\u0026#39;2022-01-01\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;) * 10 AS DATE_01; Timestamp타입에 숫자값을 더하거나 빼면 오류 발생 -- TIMESTAMP 연산. +7을 하면 아래는 오류를 발생. SELECT TO_TIMESTAMP(\u0026#39;2022-01-01 14:36:52\u0026#39;, \u0026#39;YYYY-MM-DD HH24:MI:SS\u0026#39;) + 7; Timestamp는 interval 타입을 이용하여 연산 수행 DATE 타입에 INTERVAL을 더하면 TIMESTAMP로 변환됨 SELECT TO_DATE(\u0026#39;2022-01-01\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;) + INTERVAL \u0026#39;2 DAYS\u0026#39; AS DATE_01; JUSTIFY_INTERVAL(일자) # 출력 형식 : 43 years 7 mons 15 days 9 hours 3 mins 37.040751 secs AGE(일자) # 나이 계산 출력 형식 : 43 years 0 mons 0 days 0 hours 0 mins 0.0 secs 실습코드 # 03_1_date_형변환실습.sql 03_2_extract_추출실습.sql 03_3_interval_실습.sql 03_4_date_truc_함수실습.sql "},{"id":77,"href":"/docs/rdbms/003_groupby/","title":"003 Groupby","section":"Rdbms","content":" Group by 와 집계 함수(Aggregate Function) # Group by절 # Group by 절에 기술된 컬럼 값(또는 가공 컬럼값)으로 그룹화 한 뒤 집계(Aggregation) 함수와 함께 사용되어 그룹화된 집계 정보를 제공 Group by 절에 기술된 컬럼 값으로 반드시 1의 집합을 가지게 됨 Select 절에는 Group by 절에 기술된 컬럼(또는 가공 컬럼)과 집계 함수만 사용될 수 있음 Null을 계산하지 않음 case~when 사용한 pivoting # Group by 시 행 레벨로 만들어진 데이터를 열 레벨로 전환할 때 Aggregate와 case when을 결합하여 사용 -- DEPTNO로 GROUP BY하고 JOB으로 PIVOTING SELECT SUM(CASE WHEN JOB = \u0026#39;SALESMAN\u0026#39; THEN SAL END) AS SALES_SUM , SUM(CASE WHEN JOB = \u0026#39;MANAGER\u0026#39; THEN SAL END) AS MANAGER_SUM , SUM(CASE WHEN JOB = \u0026#39;ANALYST\u0026#39; THEN SAL END) AS ANALYST_SUM , SUM(CASE WHEN JOB = \u0026#39;CLERK\u0026#39; THEN SAL END) AS CLERK_SUM , SUM(CASE WHEN JOB = \u0026#39;PRESIDENT\u0026#39; THEN SAL END) AS PRESIDENT_SUM FROM HR.EMP; count 집계함수 사용시 주의할점 # GROUP BY PIVOTING시 조건에 따른 건수 계산 유형(COUNT CASE WHEN THEN 1 ELSE NULL END) SELECT DEPTNO, COUNT(*) AS CNT , COUNT(CASE WHEN JOB = \u0026#39;SALESMAN\u0026#39; THEN 1 END) AS SALES_CNT , COUNT(CASE WHEN JOB = \u0026#39;MANAGER\u0026#39; THEN 1 END) AS MANAGER_CNT , COUNT(CASE WHEN JOB = \u0026#39;ANALYST\u0026#39; THEN 1 END) AS ANALYST_CNT , COUNT(CASE WHEN JOB = \u0026#39;CLERK\u0026#39; THEN 1 END) AS CLERK_CNT , COUNT(CASE WHEN JOB = \u0026#39;PRESIDENT\u0026#39; THEN 1 END) AS PRESIDENT_CNT FROM HR.EMP GROUP BY DEPTNO; 잘못된 사례\ncount는 1이라해서 1건을 세는게 아니라, 1이든 0이든 세는거임 !!!! null 이여야 안센다! -- GROUP BY PIVOTING시 조건에 따른 건수 계산 시 잘못된 사례(COUNT CASE WHEN THEN 1 ELSE NULL END) SELECT DEPTNO, COUNT(*) AS CNT , COUNT(CASE WHEN JOB = \u0026#39;SALESMAN\u0026#39; THEN 1 ELSE 0 END) AS SALES_CNT , COUNT(CASE WHEN JOB = \u0026#39;MANAGER\u0026#39; THEN 1 ELSE 0 END) AS MANAGER_CNT , COUNT(CASE WHEN JOB = \u0026#39;ANALYST\u0026#39; THEN 1 ELSE 0 END) AS ANALYST_CNT , COUNT(CASE WHEN JOB = \u0026#39;CLERK\u0026#39; THEN 1 ELSE 0 END) AS CLERK_CNT , COUNT(CASE WHEN JOB = \u0026#39;PRESIDENT\u0026#39; THEN 1 ELSE 0 END) AS PRESIDENT_CNT FROM HR.EMP GROUP BY DEPTNO; Rollup vs Cube # rollup, cube는 group by와 함께 사용된다. grup by절에 사용되는 컬럼들에 대해서 추가적인 group by를 수행한다. rollup : 계층적인 방식으로 group by 추가 수행 Group by 시 Rollup을 함께 사용하면 Rollup에 적용된 컬럼의 순서대로 계층적인 Group by 를 추가적으로 수행 Group by 절의 나열된 컬럼수가 N개 이면 Group by는 N+1회 수행 cube : group by절에 기재된 컬럼들의 가능한 combination으로 group by 수행 Group by 시 Cube를 함께 사용하면 Cube에 나열된 컬럼들의 가능한 결합으로 Group by 수행 Group by 절의 나열된 컬럼수가 N개 이면 Group by는 2의 n제곱 수행 실습코드 # 04_1_groupby_실습.sql 04_2_groupby_count_distinct_실습.sql 04_3_groupby_case_when_실습.sql 04_4_groupby_case_when_privoting_실습.sql 04_5_groupby_rollup_실습.sql 04_6_groupby_cube_실습.sql "},{"id":78,"href":"/docs/batch/002_batch_performance/","title":"002 Batch Performance","section":"Batch","content":" [Youtube 세미나 보기] Batch Performance 극한으로 끌어올리기: 1억 건 데이터 처리를 위한 노력 / if(kakao)2022 # 다루고자 하는 내용 # 개발자들은 언제 Batch를 개발할까? 특정 시간에 많은 데이터를 일괄 처리 배치를 사용하는 상황 일괄 생성 일괄 수정 통계 무관심한 Batch Performance # Batch 개발을 쉽게 생각하는 경향 배포후 관리 소홀 배치를 지원하는 APM Tool의 부재 많은 데이터 처리량 # 2017년 : 하루 평균 25만번 현재 : 1억번 그럼에도 Batch 수행시간은 1시간으로 동일하다. 어떻게 Batch Performance 개선이 가능했을까? # Batch 처리의 순수한 성능 개선 관점 대량 데이터 처리 Batch 개발자를 위함 개발 환경 # 효과적인 대량 데이터 Reader 개선 # Batch 성능에서 차지하는 비중은 Reader \u0026gt; Writer Reader의 복잡한 조회 조건 대부분의 상황은 아래와 같이, 필요한 데이터만 골라내야한다. Batch에서 데이터를 읽는 절대적인 방법 - Chunk Processing # 1000만개의 데이터를 한번에 처리 가능한 application은 거의 없다. 나누어 처리하는 방식 : chunk Processing Pagination Reader 데이터를 한번에 받지 않고, Page 단위로 받음 이런 방식은 대량 처리에 매우 부적합하다. Limit Offset이 가지는 태생적인 한계 때문이다. Offset이 커질수록 느려진다. (mysql이 5000000번째 데이터 찾는것에 상당한 부담을 느낀다.) 위 문제를 해결한 Reader : ZeroOffsetItemReader (항상 offset을 0으로 유지한다.) PK를 기준으로 오름차순 정렬 생성한 쿼리에 자동으로 PK 조건을 추가해서 offset을 0으로 유지한다. 3번째 page는 2번째의 마지막 id보다 +1로 두면 된다. 추가적으로 QueryDSL 적용 (Query Domain Specific Language) 다른 해결방안- Cursor을 사용하자. # JpaCursorItemReader은 OOM 발생 가능성이 있다. 더 안전항 방식은? Exposed 사용 새로운 방식의 쿼리 구현, Exposed # Exposed DSL 도입 이유: Kotlin 언어적 특성을 활용해서 세련되게 쿼리를 구현 가능 얼마나 개선되었을까? # ItemReader 성능 비교 데이터가 많아질수록 그 차이는 더 심해진다. 300만건 Heap Space Monitoring 기존의 ItemReader 정리 # 개선한 ItemReader 정리 # 데이터 Aggregation 처리 # 데이터가 많아지고 쿼리가 복잡해져도 문제가 없을까? SUM 쿼리에 의존하는 배치의 문제점 Join, Groupby, Sum 쿼리를 사용하면 성능 저하 해결 - groupby 를 포기한다. # 직접 aggregation을 한다. 1000만개의 데이터를 합산해서 50만개 데이터를 만들때, 최소한 50만개의 데이터 공간이 필요하다. OOM 발생 가능성 존재 새로운 Architecture, Redis를 활용한 Sum # 충분한 저장공간과 빠른 연산 -\u0026gt; Redis Redis를 도입한 이유\n연산 명령어 (hincrby, hincrbyfloat 지원) 메모리 수준에서 합산 50만개는 쉽게 저장하는 넉넉한 메모리 In-Memory DB 빠른저장O, 영구저장X 해결되지 않은 문제\n네트워킹 지연성 전체 성능은 오히려 저하된다. Redis pipeline으로 처리하자.\n다수 command를 한번에 묶어서 처리한다. 대량데이터 Write # Writer 개선 방법 # Batch Insert : 일괄로 쿼리 요청 명시적 쿼리 : 필요한 컬러만 Update, 영속성 컨텍스트 사용하지 않기 JPA를 사용하지 않는다. Batch에서 JPA Write에 대한 고찰 Dirty Checking과 영속성 관리 불필요한 check 로직으로 인한 큰 성능 저하 Read할 때부터 Dirty Checking과 영속성 버리기 UPDATE 할때 불필요한 컬럼도 UPDATE 불필요한 컬럼 update로 인한 소폭 성능 저하 JPA Batch Insert 지원이 어려운 부분 batch insert 불가한 경우, 매우 큰 성능 저하 Batch에서 Batch Insert 사용을 해야한다. 결론 : Writer에서 JPA를 포기하고 Batch Insert 할 것 성능 비교 # Batch 구동 환경 # 보통의 환경 기존 스케줄 tool의 아쉬운점 자원관리(Resource Control)의 어려움 배치 상태파악(Monitoring)의 어려움 Batch에서는 동작 하나하나가 매우 길다. 대부분 스케줄 Tool에서 로그를 볼 수 있지만 로그가 빈약하다. 서비스 상태를 로그로 판단하는것 자체가 전혀 시각적이지 않다. Spring Cloud Data Flow 도입 Spring Cloud Data Flow # 데이터 수집, 분석, 데이터 입/출력과 같은 데이터 파이프라인을 만들고 오케스트레이션하는 툴 데이터 파이프라인 종류 : Stream, Task(Batch) 동작과 역할 flow Task 스케줄 생성 Monitoring References https://www.youtube.com/watch?v=L9K0l65wMbQ "},{"id":79,"href":"/docs/msa/003_circuit_breaker/","title":"003 Circuit Breaker","section":"Msa","content":" Spring cloud circuit breaker # Circuit breaker # 전기 회로의 차단기와 유사한 역할을 하는 소프트웨어 디자인 패턴 특정 서비스가 과부하 상태가 되었을 때 추가적인 요청을 차단하여 시스템의 안정성을 유지 해당 서비스가 복구될 수 있는 시간 확보 Reactive systems의 Resilient(복원력)를 지원 Spring Cloud Circuit Breaker # Spring에서 circuit breaker를 지원하기 위해 만든 라이브러리 resilience4j와 spring retry를 추상화하여 Circuit breaker를 지원 reactive 환경에서는 resilience4j 만 사용 가능 Resilience4j # java에서 circuit breaker를 지원하는 라이브러리 Circuit breaker, Rate Limiter, Retry, Bulkhead, Time Limiter, Cache 등의 기능을 제공 Circuit breaker 준비 # org.springframework.cloud:springcloud-starter-circuitbreaker-reactorresilience4j 를 추가 mavenBom에 cloud-dependencies 버전을 명시 circuit breaker에는 별도의 버전 명시 없이 사용 가능 Circuit breaker 상태 # Circuit breaker의 상태는 finite state machine으로 표현 가능 Closed, Open, Half Open으로 구성 Closed: 정상적으로 요청을 받을 수 있는 상태 Open: Circuit breaker가 작동하여, 목적지로 가는 트래픽, 요청을 막고 fallback 반환 Half Open: 트래픽을 조금 흘려보고 Open을 유지할지 Closed로 변경할지 결정 Circuit breaker 상태 변화 # Closed에서 Open으로 변경 호출하는 대상에 문제가 생긴 경우 Open에서 Half Open으로 변경 일정 시간이 지나거나 강제로 상태 변경 Half Open에서 Open 혹은 Close로 변경 half open 동안 실제 대상을 호출하여 문제가 없는지 검증 Reactor transform 연산자 # 원본 publisher를 인자로 전달 받고 변환된 publisher를 반환 통째로 넘기기 때문에 다른 연산자보다 더 유연하게 데이터 스트림을 처리 가능 Reactor transform 연산자 테스트 # 함수형 인터페이스를 직접 구현 apply에서 인자로 현재 mono를 받고 각각의 item에 1을 더하는 연산자를 추가하여 반환 error를 반환하는 Flux를 무조건 1을 반환하는 Flux로 변환하여 반환 Circuit breaker closed # 기본 상태 들어오는 모든 요청을 대상 메소드, 서비스에 전달 서비스에 전달 후 응답이 느리거나 error가 발생한다면 fallback을 실행하여 결과 전달 closed 테스트 서비스 # ReactiveCircuitBreakerFactory bean 주입 doGreeting을 호출하여 특정 delay 이후 인사를 반환하는 Mono 획득 transform을 사용하여 해당 publisher를 circuit breaker에 전달 “normal” id를 갖는 circuit breaker를 생성 fallback으로 fallbackMessage를 반환 closed 테스트 어노테이션 # GreetingCircuitBreakerServiceTest를 생성 AutoConfigureReactiveCircuitBreaker를 만들어서 제공 closed 테스트 설정 # 로깅을 위한 customizer 추가 요청이 성공하거나 문제가 발생하거나 circuit breaker의 상태가 바뀌거나 slowCallRate, failureRate 등이 바뀌면 로깅 eventLogger 외에는 모두 기본 설정 configureDefault를 통해 별도의 설정을 갖지 않는 다른 모든 circuit breaker에 해당 설정이 적용 “normal”에 대한 별도의 설정을 제공하지 않기 때문에 normal circuit breaker는 default config를 사용 closed 테스트 # 테스트 생성 Greeter를 @SpyBean으로 설정 GreetingCircuitBreakerService와 CircuitBreakerRegistry를 @Autowired로 successMessage과 fallbackMessage를 지정 success 예제\ndelay를 0ms로 제공 바로 기대한 결과인 “Hello grizz!” 를 반환 circuitBreaker id를 normal로 설정했기 때문에 logging에서도 normal로 남는다 greeter의 generate는 한번 실행 GreetingCircuitBreakerServiceTest fallback 예제\ndelay를 5000ms로 제공 withVirtualTime을 사용해서 시간이 지난 것처럼 시뮬레이션 1초가 지나는 시점에 “Hello world!” 반환 5초를 기다리게 해도 결과는 동일 circuit breaker에서 TimeoutException 발생시키고 fallback 실행 greeter는 중간에 cancel되어 실행되지 않음 exception이 발생한 경우도 fallback 실행\nit 이 Mono의 결과 (error) Circuit breaker sliding window # circuit breaker는 대상이 되는 서비스 호출과 관련된 성공, 실패 여부를 sliding window 형태로 저장 count-based sliding window에서는 특정 개수 n개만큼의 측정 결과를 저장 저장된 측정 결과의 개수가 n개에 도달하면 최초에 저장했던 결과를 제거하고 새로운 결과를 저장 circular array를 사용 각각의 과정에서 전체 개수 대비 실패 비율을 계산 이를 failure rate 라고 부른다 sliding window와 failure rate # failure rate = 실패한 호출 수 / sliding window 크기 item 하나가 새로 추가되면 failure rate 계산에 O(1) 만큼의 시간복잡도가 필요 sliding window 크기는 고정이기 때문에 총 합에서 새로운 값을 더하고 없어진 값을 제거하여 다시 계산 failure rate이 설정한 임계치에 도달하는 순간 closed 상태에서 open 상태로 변경 closed에서 open으로 전환 # closed 상태에서 open 상태로 바뀌면서 circuit breaker는 state transition 이벤트를 발행 closed 전환 테스트 # circuit breaker id를 외부에서 바꿀 수 있게 추가 sliding window size를 4\n기본값은 100 failure rate threshold를 50\nfailure rate 임계치를 퍼센티지로 표현 기본값은 50이고 0에서 100 사이 “mini” 라는 이름을 갖는 circuit breaker 생성\nsliding window size가 4이고 failure rate threshold가 50이기 때문에 전체 호출 중 절반이 실패하는 순간 open으로 전환 처음 4개가 성공하여 failureRate이 0.0\n이후 2개가 실패하고 failureRate이 50.0이 되면서 open 상태로 전환 Circuit breaker open # open 상태에서는 기존에 호출하던 대상을 더이상 호출하지 않는다 fallback이 실행되어 반환 호출하는 서비스를 보호하고 복구할 수 있는 시간 확보 fallback이 실행되기 때문에 서비스의 장애가 전파되지 않는다 open 테스트 # open 상태에서는 delay를 0으로 전달해도 무조건 fallback 메세지 반환 closed 상태였다면 무조건 성공 100번을 호출했지만, spyGreeter는 처음 성공했던 4번 호출 이후 한번도 호출되지 않는다 open에서 half open으로 # open 상태에서 half open 상태로 바뀌면서 circuit breaker는 state transition 이벤트를 발행 open 상태에서 half open으로 가기 위해서는 2가지 방법이 존재 circuit breaker api를 사용해서 명시적으로 변경 옵션을 지정하여 특정 시간이 지나면 자동으로 변경 open 전환 테스트 # 처음 4번 실패하게 만들어 open 상태로 전환 CircuitBreakerRegistry를 이용해서 circuitBreaker에 접근하고 transitionToHalfOpenState를 호출 하지만 직접 상태를 관리해야 하기 때문에 어쩔 수 없는 겨우가 아니라면 가급적 권장 하지 않는다 open 전환 테스트 # enableAutomaticTransitionFromOpenToHalfOpen: 일정 시간이 지나면 자동으로 open에서 half open으로 변경되게 설정 waitDurationInOpenState: 얼마만큼의 시간이 지난 후 half open으로 변경할지 설정 예제\n처음 4개를 실패 상태로 만들어 open 상태로 waitDurationInOpenState를 5초로 설정했기 때문에 6초동안 대기 이후 자동으로 half open 상태로 변경 Circuit breaker half open # half open 상태에서 정해진 개수만큼 요청을 전달 half open에서 측정한 결과에 따라서 open 혹은 closed로 변경 대상 서비스가 복구된 경우, 자동으로 장애 복구하기 위한 목적 half open에서 closed 혹은 open # half open 상태에서 sliding window와 같이 특정 개수(permittedNumberOfCalls)에 대해서 측정 결과 저장 half open의 failure rate = 실패한 호출 수 / permittedNumberOfCalls open으로 전환 : failure rate이 임계점보다 높거나 같다면 close로 전환 : failure rate이 임계점보다 낮다면 half open 전환 테스트 # 3초가 지나면 자동으로 open에서 half open으로 전환 half open 상태에서 6개의 요청을 분석하고 3개 이상 실패하면 open으로 2개 이하 실패하면 closed로 permittedNumberOfCallsInHalfOpenState가 6이라서 6개의 요청을 분석 failureRateThreshold가 50이기 때문에 3개가 기준이 된다 close로 변경 예제\ncircuit breaker를 half open 상태로 변경 4번은 성공, 2번은 실패 6번 (permittedNumberOfCalls) 요청이 끝나는 시점에 failureRate은 33퍼센트 50퍼센트 (failureRateThreshold) 보다 작기 때문에 closed로 상태 변경 open으로 변경 예제\ncircuit breaker를 half open 상태로 변경 3번은 성공, 3번은 실패 6번 (permittedNumberOfCalls) 요청이 끝나는 시점에 failureRate은 50퍼센트 50퍼센트 (failureRateThreshold) 보다 크거나 같기 때문에 open으로 상태 변경 Circuit breaker 설정 # ReactiveResilience4JCircuitBreakerFactory에 CircuitBreakerConfig와 TimeLimiterConfig 제공 CircuitBreakerConfig는 circuit breaker 설정 제공 TimeLimiterConfig는 timeout과 관련된 설정 제공 CircuitBreakerConfig 설정 # slidingWindowSize: 호출 결과를 저장할 sliding window 크기. 기본 100 failureRateThreshold: 실패 비율 임계점. 기본 50 enableAutomaticTransitionFromOpenToHalfOpen: open에서 half open으로 자동 전환할지 여부. 기본 false waitDurationInOpenState: open에서 half open로 전환될 때까지 필요한 시간. 기본 60초 permittedNumberOfCallsInHalfOpenState: half open 상태에서 허용할 호출 수. 기본 10 ignoreExceptions: 서비스에서 exception을 던지는 경우, 허용할 exceptions 목록. 기본 empty maxWaitDurationInHalfOpenState: half open 상태에서 대기할 수 있는 최대 시간. 기본 0 TimeLimiterConfig 설정 # cancelRunningFuture: future가 진행중인 경우, cancel 할지 여부. 기본 true timeoutDuration: timeout 기준 시간. 기본 1초 factory.configure에 circuit breaker id를 전달하여 특정 id를 갖는 circuit breaker들에 대한 설정 가능 일반적인 모든 circuit breaker에 설정을 적용하려면 configureDefault를 사용 Circuit breaker yaml 설정 # yaml을 통해서도 설정 가능 resilience4j.circuitbreaker.instances.로 특정 circuit breaker instance의 설정을 주입 resilience4j.circuitbreaker.instances.example을 앞선 @Bean 설정과 동일 resilience4j.circuitbreaker.configs.default를 설정하여 정확히 일치하는 instance가 없는 circuit breaker에 대한 설정 제공 Circuit breaker group # circuit breaker instance를 만들면서 group을 제공할 수 있다 아래의 순서로 설정을 찾게 된다 instance id와 정확히 일치하는 설정 사용 없다면, group과 정확히 일치하는 설정 사용 없다면, default 설정 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":80,"href":"/docs/operating_system/001_thread/","title":"001 Thread","section":"Operating System","content":" 스레드 종류 - 하드웨어 스레드, OS 스레드, 네이티브 스레드, 커널 스레드, 유저 스레드, 그린 스레드 # 우리가 작성한 프로그램의 동작방식 # 하드웨어 OS Kernel User Program : 운영체제를 통해 하드웨어를 사용 Hardware thread # 코어(core)의 고민 : 메모리에서 데이터를 기다리는 시간이 꽤 오래 걸린다. 코어에서 프로그램이 실행될때 각종 연산 작업이 있을테고, 이를 위해 메모리에 접근하는 작업들이 있다. 이 코어에서 실행되는 연산 작업에 비해 메모리에서 데이터를 기다리는 시간이 더 오래 걸린다. 이는 코어를 낭비하고있다는 말과 같다. 메모리를 기다리는 동안 다른 쓰레드를 실행하는건 어떨까? 예시) 코어 -\u0026gt; Computing 작업 -\u0026gt; 메모리에 접근하여 데이터 처리 -\u0026gt; Computing 작업 -\u0026gt; 메모리에 접근\u0026hellip; 코어가 쉬고있을때마다 코어를 낭비하는 것과 같다. 이를 해결하기 위해, 메모리에 접근하는 동안 독립적으로 또다른 무언가를 실행하면 어떨까? 1개의 코어에서 2개의 쓰레드를 실행하는 것 -\u0026gt; 이 각각의 쓰레드를 Hardware Thread라고 한다. 이를 인텔의 Hyper-threading 이라고 한다. 물리적인 코어마다 하드웨어 스레드가 2개다. Hardware Thread : OS 관점에서는 가상의 코어 싱글코어 CPU에 하드웨어 스레드가 2개라면, OS는 이 CPU를 듀얼 코어로 인식하고, 이에 맞춰서 OS 레벨의 스레드들을 스케줄링한다. OS Thread # OS Kernel 영역에 해당되는 부분 커널(kernel) 운영체제의 핵심 시스템의 전반을 관리/감독하는 역할 하드웨어와 관련된 작업을 직접 수행 OS Thread OS 커널 레벨에서 생성되고 관리되는 스레드 CPU에서 실제로 실행되는 단위, CPU 스케줄링의 단위 OS 스레드들의 컨텍스트 스위칭은 커널이 개입 -\u0026gt; 비용 발생 사용자 코드와 커널 모드 모두 OS 스레드에서 실행된다. 우리가 작성한 코드가 OS 스레드로 실행되고, system call을 사용하면 커널 모드로 전환되고, 여기서 커널 코드가 실행되는데 이도 OS 스레드에서 실행된다. native 스레드, 커널 스레드, 커널-레벨 스레드, OS-레벨 스레드 라고 불리기도한다. OS 스레드 8개가 하이퍼 스레딩이 적용된 인텔 듀어코어 위에서 동작한다면 OS 스레드들을 어떻게 코어에 균등하게 할당할까? 코어마다 하드웨어 쓰레드가 2개씩 있고, OS 입장에서는 하드웨어 쓰레드 각각이 코어로 인식되고, 이 코어 당 쓰레드 2개씩 할당한다. User Thread # 유저-레벨 쓰레드라고 불리기도한다. 스레드 개념을 프로그래밍 레벨에서 추상화 한것이다. Thread thread = new Thread(); thread.start(); 자바에서 제공하는 Thread라는 Class 사용 - start()의 구현부에서 start0() 메서드를 호출하는데, 이는 JNI라는 기술을 통해 OS SystemCall을 호출한다. 리눅스라면 clone 이라는 system call 호출 -\u0026gt; 리눅스에서는 OS 레벨의 쓰레드를 1개 생성하게된다. 이 쓰레드는 User-level의 쓰레드와 연결이 된다. 유저 스레드가 CPU에서 실행되려면 OS 스레드와 반드시 연결되어야한다. (= 기본적 CPU에서 실행되는 단위는 커널 스레드이기 때문! 따라서 유저 스레드는 반드시 커널 스레드와 연결이 되어야 하고, 그 후에 커널 스레드가 CPU에서 실행이 된다.) 유저 스레드와 OS 쓰레드를 어떻게 연결할까? One-to-One model : start() 하는 순간, OS 레벨의 쓰레드를 생성하여 1:1로 연결한다. 쓰레드 관리를 OS에 위임한다. (스케줄링 포함) 스케줄링도 커널이 수행한다. CPU가 만약에 멀티코어를 가진다고 해도, 이 멀티코어에 OS 쓰레드를 잘 분배하여 동작시키면, 1:1로 분배되는 유저 쓰레드도 멀티코어를 잘 활용하게된다. 한 쓰레드가 block이 되도, 나머지 쓰레드에는 영향없이 잘 동작한다. race condition 발생 가능성 있음 Many-to-One model : User 레벨의 쓰레드가 여러개있고, OS 레벨의 쓰레드는 1개만 있다. N:1로 연결한다. Context Switching이 더 빠르다. (User 레벨에서 발생하기때문에 커널의 개입이 없다.) OS 쓰레드만 보면 싱글 쓰레드이기 때문에 OS 레벨에서 race condition이 발생할 가능성이 거의 없다. (User Thread는 있다.) 멀티코어를 활용할 수 없다. 실제로 코어에서 실행되는 쓰레드는 OS 쓰레드이기 때문이다. 첫번째 User-Thread가 blocki I/O를 호출했을때, OS 쓰레드에서 block I/O를 호출하게되어, 다른 User-Thread 모두 block된다. Many-to-Many model : User Thread, OS Thread 가 모두 N:N으로 연결된다. 위 1), 2) 방법의 장점을 살린 방법 구현이 복잡하다. 기술 문서에서는 OS와는 독립적으로 유저 레벨에서 스케줄링되는 스레드라고 한다. Many-to-One model, Many-to-Many model 로 볼 수 있다. Green Thread # Java 초창기 버전은, Many-to-One 스레딩 모델을 사용, 이때 유저 스레드들을 그린 쓰레드라고 호칭했다. OS와는 독립적으로 유저 레벨에서 스케줄링되는 스레드 Many-to-One model, Many-to-Many model 로 볼 수 있다. Kernel Thread # OS 커널의 역할을 수행하는 스레드 References Youtube 강의 : https://www.youtube.com/watch?v=vorIqiLM7jc https://velog.io/@chanyoung1998/%EC%8A%A4%EB%A0%88%EB%93%9C%EC%9D%98-%EC%A2%85%EB%A5%98%ED%95%98%EB%93%9C%EC%9B%A8%EC%96%B4-%EC%8A%A4%EB%A0%88%EB%93%9C-OS-%EC%8A%A4%EB%A0%88%EB%93%9C-%EC%9C%A0%EC%A0%80-%EB%A0%88%EB%B2%A8-%EC%8A%A4%EB%A0%88%EB%93%9C "},{"id":81,"href":"/docs/msa/001_reactive/","title":"001 Reactive","section":"Msa","content":" 1. Reactive Mircroservice의 Reactive # Spring cloud # Reactive systems의 핵심 가치 # Responsive(응답성) # 요구사항 문제를 신속하게 탐지하고 효과적으로 대처 신속하고 일관성 있는 응답 시간 제공 신뢰할 수 있는 상한선을 설정하여 일관된 서비스 품질을 제공 결과 가능한 한 즉각적으로 응답 사용자의 편의성과 유용성의 기초 오류 처리를 단순화 일반 사용자에게 신뢰를 조성하고, 새로운 상호작용 촉진 Resilient(복원력) # 요구사항 복제, 봉쇄, 격리, 위임에 의해 실현 장애는 각각의 구성 요소에 포함 (봉쇄) 구성 요소들은 서로 분리 (격리) 복구 프로세스는 다른(외부의) 구성 요소에 위임 (위임) 필요한 경우 복제를 통해 고가용성이 보장 (복제) 결과 장애에 직면하더라도 응답성을 유지 시스템이 부분적으로 고장이 나더라도, 전체 시스템 을 위험하게 하지 않고 복구 할 수 있도록 보장 구성 요소의 클라이언트는 장애를 처리하는데에 압박을 받지 않습니다 Elastic(유연성) # 요구사항 경쟁하는 지점이나 중앙 집중적인 병목 현상이 존재하지 않도록 설계 구성 요소를 샤딩하거나 복제하여 입력을 분산 실시간 성능을 측정하는 도구를 제공 응답성 있고 예측 가능한 규모 확장 알고리즘을 지원 결과 작업량이 변화하더라도 응답성을 유지 입력 속도의 변화에 따라 이러한 입력에 할당된 자원을 증가시키거나 감소 상품 및 소프트웨어 플랫폼에 비용 효율이 높은 방식으로 유연성을 제공 Message Driven(메시지 기반) # 비동기 통신: 구성 요소는 서로 비동기적으로 메시지를 주고 받으며, 독립적인 실행을 보장한다. 메시지 큐: 메시지 큐를 생성하고 배압을 적용하여 부하를 관리하고 흐름을 제어한다. 복원력: 구성 요소 사이에 경계를 형성하여 직접적인 장애의 전파를 막고 장애를 메시지로 지정해서 위치와 상관 없이 동일하게 장애를 관리한다. 탄력성: 구성 요소 사이에 경계를 형성하여 각각의 구성 요소를 독립적으로 확장 가능하게 만들고, 자원을 더 쉽게 추가하거나 제거한다. Reactive systems # 핵심 가치: 가능한 한 즉각적으로 응답 첫 번째 형태: 장애에 직면하더라도 응답성을 유지 두 번째 형태: 작업량이 변화하더라도 응답성을 유지 방법: 비동기 non-blocking 기반의 메시지 큐를 사용해서 통신한다 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":82,"href":"/docs/msa/002_reactive_microservice/","title":"002 Reactive Microservice","section":"Msa","content":" 2. Reactive Mircroservice의 Microservice # Monolithic 아키텍쳐 # 모든 컴퍼넌트가 하나의 서버 내에 존재 단순성: 코드 저장소가 하나로 존재하는 경우가 많아서 관리 포인트가 줄어든다 일관성: 각각의 서비스는 하나의 기술 스택으로 제공되기 때문에 접근성이 높다 높은 효율: 서비스 사이에 네트워크 통신이 없기 때문에 네트워크 지연이나 데이터 직렬화에 의한 오버헤드가 없다 Monolithic 아키텍쳐의 단점 # 확장성의 한계: 모든 서비스가 하나의 서버로 구성되어 있기 때문에 특정 서비스가 부하를 받더라도 서버 전체를 늘려야 한다 장애의 전파: 서비스 중 한 곳에서 문제가 발생하면 시스템 전체에 영향을 줄 수 있다 배포의 단일화: 하나의 서비스를 변경하더라도 전체 시스템을 다시 배포해야 한다 기술의 정체: 모든 서비스가 하나의 코드베이스에 결합되어 있어서 새로운 기술을 도입하거나 아키텍쳐의 변경이 전체에 큰 영향을 줄 수 있다 유지보수의 어려움: 모든 구성요소가 직접 연결 되어 있기 때문에 작은 부분의 수정이 전체에 영향을 미칠 수 있다 Microservice 아키텍쳐 # 이러한 단점을 극복하기 위해 서비스와 저장 소, 데이터베이스 등을 분리 독립적인 배포: 각 서비스는 독립적으로 배포 유연한 확장: 각 서비스는 각각 탄력적으로 확장 가능. 트래픽이 몰리는 특정 서비스만 서버를 늘려서 유연하게 대응 가능 장애 격리: 서비스 하나에서 장애가 발생해도 전체 어플리케이션에 영향을 주지 않는다 기술의 유연성: 각 서비스는 다른 언어, 프레임워크, 기술 스택 사용 가능. 심지어 서버리스 또한 도입 가능 Microservice 아키텍쳐 단점 # 복잡성 증대: 각각의 서비스가 네트워크를 통해서 통신. 네트워크 지연, 실패, 데이터 일관성 등의 문제가 발생 데이터 일관성: 각각의 서비스가 별도의 데이터베이스를 유지하기 때문에 이를 일치하는 것이 어렵다 배포와 운영의 복잡성: 각 서비스가 독립적인 배포 프로세스를 갖추며, 모니터링, 알림 시스템이 필요 테스트의 어려움: 서비스가 분리되어 있고 데이터베이스도 각자 가지고 있기 때문에 테스트 준비에 더 많은 리소스 필요 높은 초기 비용: 올바르게 구축하기 위해서 상당한 시간과 노력이 필요 Microservice 구현 # 외부 서비스, 클라이언트에 대한 직접적인 노출 장애 격리 실패로 인한 시스템 전체의 붕괴 특정 메시지 큐를 직접 사용하여 불필요한 의존성 증대와 확장성 축소 Microservice와 Spring cloud # API Gateway를 이용해서 내부 시스템 은닉 및 안정성 제공 Circuit breaker를 이용해서 장애를 격리하고 시스템 안정성 유지. 장애 복구 여유 확보 추상화된 발행/구독, 생산/소비 메시지 패턴을 제공하여 확장성을 높이고 의존성 축소 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":83,"href":"/docs/redis/003_reactive_redis_intro/","title":"003 Reactive Redis Intro","section":"Redis","content":" 1. Redis 소개 # Redis instance # 여러 client가 하나의 redis 서버로 요청 전달 단일 redis 서버에 문제가 발생하면 장애 -\u0026gt; 모든 client에서 접속 불가 Redis replication # master와 replica로 구성 master에 데이터가 업데이트 -\u0026gt; replica 동기화 replica : 읽기만 가능 replica에 문제 발생 -\u0026gt; 여러 node에 data가 복제되었기 때문에 복구 가능 master에 문제 발생 -\u0026gt; 개발자가 직접 replica 중 하나를 master로 변경 Redis sentinel # master에 문제 발생 -\u0026gt; replica들이 master를 선출 이전 master가 복구 된 경우, replica로 전환되어 새로운 master를 바라보게된다. Automatic Failover 기능을 제공 자동 장애 조치 고가용성을 제공 Redis cluster # Redis sentinel 보다 조금 더 강력한 기능들을 제공 데이터를 자동으로 파티셔닝하고 client의 요청을 필요한 master 혹은 replica에게 전달 고가용성, 데이터 분산, 자동 파티셔닝 각각의 master에 문제가 생기면 replica가 마스터 역할을 수행 Redis connector # Spring에서는 redis connector로 jedis와 lettuce를 지원 현재는 기본으로 rettuce 사용 별도의 설정을 통해서 jedis를 설정 가능 Redis connector 차이 # Spring data redis reactive 스택 # Redis reposito # 동기 blocking 방식에서 redis repository를 지원 reactive 환경에서는 reactive redis repository 미지원 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":84,"href":"/docs/redis/004_Lettuce/","title":"004 Lettuce","section":"Redis","content":" 2. Lettuce # 구조 # Reactor 기반으로 Reactive API 지원 Reactive streams API 지원 동기 API, 비동기 API 모두 지원 Netty 기반으로 높은 성능과 확장성 제공 일반 TCP 통신 뿐만 아니라 epoll, kqueue 기반의 multiplexing I/O 지원 주요 컴포넌트 # RedisClient : Redis의 연결 정보를 포함하는 객체 Netty의 Channel, EventLoopGroup 등을 포함하기 때문에 가능한한 재사용 RedisConnecction 생성 StatefulRedisConnection : Redis 서버 Connection 여러 쓰레드가 동시에 접근해도 안전 동기, 비동기, Reactive command를 제공 RedisReactiveCommand : Redis API와 관련된 reactive command 제공 RedisReactiveCommand 획득 # RedisClient.create : Redis Client를 생성 RedisURI builder를 통해서 host, port 주입 연결 정보를 String으로 전달하여 생성 가능 RedisCleint의 connect를 통해서 StatefulRedisConnetion 획득 StatefulRedisConnection의 reactive를 통해서 reactiveCommand 획득 RedisReactiveCommand 실행 # StatefulRedisConnection과 ReactiveCommand는 key-value로 String 지원 set command를 실행한 후, then으로 get command 실행 StatefulRedisConnection # sync, async, reactive command 지원 addListener를 통해서 RedisPubSubListener나 RedisConnectionStateListener 등을 등록, 삭제 가능 Redis(Async)Commands # RedisCommands는 모든 commands를 동기 방식으로 수행 RedisAsyncCommands는 모든 commands를 RedisFuture 형태로 반환 RedisFuture # CompletableFuture와 비슷하게 CompletionStage와 Future를 구현 추가로 에러를 조회할 수 있는 getError와 특정 시간동안 응답을 기다리는 await 제공 RedisReactiveCommands # RedisReactiveCommands는 여러 타입의 Commands를 상속 각각의 Redis가 지원하는 데이터 타입과 맵핑 String List Set Hash SortedSet Stream HLL (HyperLogLog) String # set을 통해서 person:1:name, pserson:1:age를 설정 setnx를 통해서 값을 바꾸려고 하지만, 이미 key에 value가 존재하면 반영X get, mget을 통해서 value에 접근 mget은 KeyValue를 반환 incrby를 톨해서 key의 값을 증가 List - queue # lpush를 통해서 값들을 추가 llen을 이용해서 길이를 파악 rpop을 하면 가장 처음에 들어간 item이 pop되기 때문에 queue를 구현 가능 List - stack # lpush를 통해서 값들을 추가 llen을 이용해서 길이를 파악 lpop을 하면 가장 마지막에 들어간 item이 pop되기 때문에 stack을 구현 가능 Set # sadd를 통해서 추가, 결과로 추가된 갯수 반환 set은 값이 unique하기 때문에 이미 존재하는 item에 대해서는 무시 scard : set의 cardinality 반환 smembers : 모든 item 조회 sismember : redis에 item이 포함되는지 확인 srem으로 item 삭제 Hash # hest에 map을 제공하여 여러 field를 한번에 추가 hgetall : 모든 필드에 접근하여 flux로 반환 hlen : hahs의 크기 조회 hincrby : age 필드의 크기를 10만큼 증가 hmget : 여러 필드에 대한 값 조회 hdel : age 필드 제거 SortedSet # zadd : score와 value 추가 zrem : value를 삭제 zcard : sortedSet의 cardinality 조회 zrangeWithScores : 특정 범위의 값들을 score와 함께 조회 zrank : 특정 value의 rank를 조회 TTL # expire : key에 ttl을 부여 ttl command : key에 남은 ttl 확인 ttl이 지난 후 key에 조회시 empty key가 존재하지 않기 때문에 -2를 반환 TTL - persist # expire : key에 ttl을 부여 후 persist : key를 영구 보관 가능 key에 ttl이 없는 경우 -1을 반환 PubSub # StatefulRedisPubSubConnection과 RedisPubSubReactiveCommand를 이용 channel을 subscribe하고 observe 가능 다른 StatefulRedisConnection을 만들어서 channel에 publish Stream # 특정 stream의 최신 메세지를 읽기 위해 대기\nxread : count만큼의 stream을 받은 후 complete\n지속적으로 listen하기 위해서는 repeat 등을 활용 가능\ncomplete 이후 다시 subscribe xread : block 인자로 받은 시간만큼 connection을 blocking 두 번째 connection을 만들고 redis transaction을 이용해서 여러 stream을 xadd로 추가 HyperLogLog # pfadd : key에 value들을 추가 pfcount : key의 크기를 추정 hyperloglog는 적은 메모리로 집합의 원소 개수를 추정 가능 key를 아티클의 id, value를 유저의 id로 둔다면 매우 적은 메모리로 unique view count를 구할 수 있다. References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":85,"href":"/docs/redis/005_ReactiveRedisTemplate/","title":"005 Reactive Redis Template","section":"Redis","content":" 3. ReactiveRedisTemplate # ReactiveRedisTemplate # ReactiveRedisTemplate은 Spring data redis reactive의 추상화 클래스 ReactiveRedisConnectionFactory를 통해서 RedisConnection을 주입 ReactiveRedisConnectionFactory # ReactiveRedisConnectionFactory는 RedisConnection을 제공 LettuceConnectionFactory와 JedisConnectionFactory 구현체 RedisTemplate bean # RedisReactiveAutoConfiguration를 통해서 자동으로 ReactiveRedisTemplate bean 생성 JdkSerializationRedisSerializer는 ObjectOutputStream을 이용하여 key와 value로 주어지는 object를 binary로 변환 key, value에 대해서 String만 지원하는 ReactiveStringRedisTemplate bean도 등록 ReactiveRedisOperations # ReactiveRedisConnection에 직접 접근할 수 있는 execute, executeInSession 메소드 pub/sub 메소드 key와 관련된 메소드 스크립트 메소드 operations 접근 메소드 pub/sub # convertAndSend: destination 채널로 message를 전달하고 메시지를 받은 클라이언트의 숫자를 반환 listenToChannel: channels에 주어진 채널들을 listen하고 메시지를 Flux 형태로 전달 key 관련 # hasKey: EXISTS. key가 존재하는지 확인 scan: SCAN. key들을 non-blocking으로 분할해서 순회 delete: DELETE. key들을 삭제 expire: EXPIRE. key에 TTL 부여 expireAt: EXPIREAT. unix time을 기반으로 key에 TTL을 부여 persist: PERSIST. key에 TTL 제거 getExpire: PTTL. key의 TTL을 milliseconds로 반환 Operations # opsForValue: value opsForList: list opsForSet: set opsForHash: hash opsForZSet: sorted set opsForStream: stream opsForHyperLogLog: hyperLogLog References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":86,"href":"/docs/redis/006_ReactiveOperations/","title":"006 Reactive Operations","section":"Redis","content":" 4. ReactiveOperations # ReactiveValueOperations 실행 # set으로 특정 key에 value를 추가 setIfAbsent로 key에 값이 없을때만 설정 get으로 key의 value를 조회 multiGet으로 여러 key에 접근 increment로 특정 key의 value를 증가 ReactiveListOperations # size: LLEN. list의 크기를 반환 leftPush: LPUSH. list의 head에 값을 추가 rightPush: RPUSH. list의 tail에 값을 추가 set: LSET. 특정한 index에 값을 설정 remove: LREM. list에서 value를 count 숫자만큼 제거 leftPop: LPOP. list의 head에서 값을 제거하고 반환 rightPop: RPOP. list의 tail에서 값을 제거하고 반환 delete: DEL. key에 설정된 list를 제거 ReactiveListOperations - Queue # leftPush를 통해서 100, 200 값을 추가 size를 통해서 list의 크기를 출력 rightPop을 통해서 처음에 추가한 값들을 제거 이를 통해서 queue를 구현 ReactiveListOperations - Stack # leftPush를 통해서 100, 200 값을 추가 size를 통해서 list의 크기를 출력 leftPop을 통해서 처음에 추가한 값들을 제거 이를 통해서 stack을 구현 ReactiveSetOperations # add: SADD. value들을 set에 추가. 결과로 추가된 개수 반환 remove: SREM: set에서 item들 제거 size: SCARD. set의 cardinality 반환 isMember: SISMEMBER. set에 item이 포함되는지 확인 members: SMEMBERS. set의 모든 item 조회 delete: DEL. key에 설정된 set을 제거 ReactiveSetOperations 실행 # add를 통해서 set에 값을 추가 size를 통해서 set의 cardinality를 출력 members로 모든 item 조회 isMember로 item이 set에 존재하는지 확인 remove로 set에서 item 제거 ReactiveHashOperations # remove: HDEL. hash에서 주어진 field key를 갖는 field들을 제거 get: HGET. hash에서 주어진 field의 value 조회 multiGet: HMGET. 여러 field의 value 조회 increment: HINCRBY. 특정 field의 value를 주어진 값만큼 증가 size: HLEN. hash의 field 크기 반환 putAll: HSET. 여러 field들을 한번에 추가 values: HGETALL. 모든 field들을 조회 delete: DEL. key에 설정된 hash를 제거 ReactiveHashOperations 실행 # putAll로 여러 field를 한번에 추가 values로 모든 필드에 접근하여 flux로 반환 size으로 hash의 크기 조회 increment로 age 필드의 크기를 10만큼 증가 multiGet으로 여러 필드에 대한 값 조회 remove로 age 필드 제거 ReactiveZSetOperations # addAll: ZADD. sorted set에 value와 score들을 추가 remove: ZREM. set에서 value들을 제거 rank: ZRANK. 주어진 value의 순위를 반환 rangeWithScores: ZRANGE. 특정 범위 안의 value와 score를 조회 size: ZCARD. sorted set의 cardinality 반환 delete: DEL. key에 설정된 sorted set을 제거 ReactiveZSetOperations 실행 # addAll로 여러 value를 한번에 추가 remove로 특정 value 제거 size으로 set의 크기 조회 rangeWithScores로 모든 value와 score 조회 rank로 특정 value의 순위 출력 ReactiveStreamOperations # add: XADD. streams에 record를 추가 createGroup: XGROUP. streams에 consumer group을 생성. group 이름 반환 range: XRANGE. 주어진 범위의 record를 반환 read: XREAD. 특정 offset 이후 혹은 최신 record를 count만큼 읽음. 최대 count개만큼 가져온 후 complete 이벤트 발생 consumer 제공 가능 ReactiveStreamOperations 실행 # 10초동안 block되고 최대 2개를 받을 수 있는 option 생성 10초가 지나면 next 이벤트 없이 complete 이벤트 latest를 통해서 최신 record 반환 ReactiveHyperLogLogOperations # add: PFADD. hyperloglog에 item을 추가 size: PFCOUNT. hyperloglog의 item 개수를 추정 delete: DEL. hyperloglog를 제거 ReactiveHyperLogLogOperations 실행 # add로 여러 value를 한번에 추가 size로 hyperloglog의 크기를 추정 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":87,"href":"/docs/java/001_facade_pattern/","title":"001 Facade Pattern","section":"Java","content":" 퍼사드 (Facade) 패턴 # 복잡한 서브 시스템 의존성을 최소화하는 방법. 클라이언트가 사용해야 하는 복잡한 서브 시스템 의존성을 간단한 인터페이스로 추상화 할 수 있다. 복잡한 디테일을 퍼사드 뒤로 숨긴다. 복잡한 서브 클래스들의 공통적인 기능을 정의하는 상위 수준의 인터페이스를 제공하는 패턴이다. 서브 클래스의 코드에 의존하는 일을 감소시켜 주고, 복잡한 소프트웨어를 간단히 사용 할 수 있게 간단한 인터페이스를 제공해준다. 서브 시스템(SubSystem)들 간의 종속성을 줄여줄 수 있으며, 퍼사드 객체를 사용하는 곳(Client)에서는 여러 서브 클래스들을 호출할 필요 없이 편리하게 사용할 수 있다. Facade Object(WashingMachine)만을 호출하여 \u0026lsquo;어떤 동작\u0026rsquo;을 수행할 수 있으며, 메서드의 의미 또한 명확하게 알 수 있다. 장점 # 서브 시스템에 대한 의존성을 한곳으로 모을 수 있다. (결합도 감소, 응집도 증가) 단점 # 퍼사드 클래스가 서브 시스템에 대한 모든 의존성을 가지게된다. (EmailSender) 적용 전 # package com.designpattern.report._10_facade.step1_before; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; import java.util.Properties; public class Client { public static void main(String[] args) { String to = \u0026#34;keesun@whiteship.me\u0026#34;; String from = \u0026#34;whiteship@whiteship.me\u0026#34;; String host = \u0026#34;127.0.0.1\u0026#34;; // Properties 로 서버 정보 셋팅 Properties properties = System.getProperties(); properties.setProperty(\u0026#34;mail.smtp.host\u0026#34;, host); Session session = Session.getDefaultInstance(properties); /** * SOLID 객체 지향 원칙 */ try { MimeMessage message = new MimeMessage(session); // 메시지 정보 설정 message.setFrom(new InternetAddress(from)); message.addRecipient(Message.RecipientType.TO, new InternetAddress(to)); message.setSubject(\u0026#34;Test Mail from Java Program\u0026#34;); message.setText(\u0026#34;message\u0026#34;); // send Transport.send(message); } catch (MessagingException e) { e.printStackTrace(); } } } 적용 후 # EmailMessage\npackage com.designpattern.report._10_facade.step2_after; import lombok.Getter; import lombok.Setter; @Getter @Setter public class EmailMessage { private String from; private String to; private String cc; private String bcc; private String subject; private String text; } EmailSettings\npackage com.designpattern.report._10_facade.step2_after; import lombok.Getter; import lombok.Setter; @Getter @Setter public class EmailSettings { private String host; } EmailSender\npackage com.designpattern.report._10_facade.step2_after; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; import java.util.Properties; /** * 인터페이스로 만들고 * 각 구현클래스로 확장해갈 수도 있다. */ public class EmailSender { private EmailSettings emailSettings; public EmailSender(EmailSettings emailSettings) { this.emailSettings = emailSettings; } /** * 이메일 보내는 메소드 * @param emailMessage */ public void sendEmail(EmailMessage emailMessage) { Properties properties = System.getProperties(); properties.setProperty(\u0026#34;mail.smtp.host\u0026#34;, emailSettings.getHost()); Session session = Session.getDefaultInstance(properties); try { MimeMessage message = new MimeMessage(session); message.setFrom(new InternetAddress(emailMessage.getFrom())); message.addRecipient(Message.RecipientType.TO, new InternetAddress(emailMessage.getTo())); message.addRecipient(Message.RecipientType.CC, new InternetAddress(emailMessage.getCc())); message.setSubject(emailMessage.getSubject()); message.setText(emailMessage.getText()); Transport.send(message); } catch (MessagingException e) { e.printStackTrace(); } } } Client\npublic class Client { public static void main(String[] args) { /* Email Setting */ EmailSettings emailSettings = new EmailSettings(); emailSettings.setHost(\u0026#34;127.0.0.1\u0026#34;); /* Email Sender */ EmailSender emailSender = new EmailSender(emailSettings); /* Email 내용 */ EmailMessage emailMessage = new EmailMessage(); emailMessage.setFrom(\u0026#34;keesun\u0026#34;); emailMessage.setTo(\u0026#34;whiteship\u0026#34;); emailMessage.setCc(\u0026#34;일남\u0026#34;); emailMessage.setSubject(\u0026#34;오징어게임\u0026#34;); emailMessage.setText(\u0026#34;밖은 더 지옥이더라고..\u0026#34;); emailSender.sendEmail(emailMessage); } } 적용 후 모습 # References 강의 : https://www.inflearn.com/course/%EB%94%94%EC%9E%90%EC%9D%B8-%ED%8C%A8%ED%84%B4 블로그 : https://velog.io/@bagt/Design-Pattern-Facade-Pattern-%ED%8D%BC%EC%82%AC%EB%93%9C-%ED%8C%A8%ED%84%B4 "},{"id":88,"href":"/docs/kafka/001_kafka_cdc/","title":"001 Kafka Cdc","section":"Kafka","content":" tech blog 글 읽고 정리하기 # CDC 너두 할 수 있어(feat. B2B 알림 서비스에 Kafka CDC 적용하기) # B2B 알림 서비스에 CDC를 도입을 하게 되었다.\nB2B 알림 서비스 # B2B 알림 서비스 프로젝트가 무엇일까? 배민 B2C 고객 서비스에서는 알림센터라는 시스템을 통해 고객에 알림을 발송한다. 하지만, 사장님에게 발송되는 알림은 플랫폼이 부재한 관계로 카카오 알림톡으로 발송하고 있었다. 개선을 위해 알림센터를 활용해 사장님에게 전달되는 메시지를 내부 서비스를 통해 전달함으로써, 내부 서비스 활용도와 사용자 편의성을 향상시키고자 진행한 프로젝트가 \u0026lsquo;B2B 알림서비스\u0026rsquo;다. 여기서, 세일즈 매니저에 대한 알림톡을 알림센터 내 웹 푸시 알림으로 전환 하는 과정에서 CDC를 도입하게되었다. B2B 알림서비스에 CDC를 도입하게 된 이유 # 세일즈 매니저에게 알림이 발송되는 경우\n세일즈 매니저 본인이 만든 업무 요청 건의 상태가 변경되는 경우 세일즈 매니저 본인이 해당 가게의 세일즈 매니저로 설정되어있는 업무 요청 건의 상태가 변경될 경우 기존에 알림 발송은 프론트 코드에 있던 상태였지만, 아래와 같은 문제 존재\n네트워크 문제로 알림 발송 누락 알림 발송 이후 요청 건의 상태 변경에 실패하면 실제 데이터와 맞지 않음 위와 같은 사유로 백엔드에서 처리를 해야했다. 그래서 요청 건의 상태가 변경되면 DB에 반영되는 것에 착안하여 변경된 데이터를 감지하는 CDC를 선택하게되었다. CDC (Change Data Capture) # 소스 시스템에서 데이터가 변경된 것을 감지하여, 타깃 시스템이 변경 작업에 대응하는 작업을 수행하도록 하는 프로세스\n소스 시스템 : DB 타깃 시스템 : B2B 알림 서비스 CDC를 활용하면 데이터를 사용하는 모든 시스템에서 일관성을 유지할 수 있다는 장점이 있다. pull 방식 타깃 시스템의 주기적인 풀링으로 변경 사항이 있는지 확인 실시간성이 떨어진다. 구현은 쉽다.\npush 방식 소스 시스템이 변경될때마다 타깃 시스템에 알려준다. pull 방식에 비해 소스 시스템이 많은 작업을 해야하고, 타깃 시스템에 문제가 발생하면 변경 이벤트에 누락이 발생할 수 있다. 실시간성이 뛰어나다.\nKafka CDC\n위 방식 중에 Push 방식에서 이벤트 누락의 단점을 메시지 큐인 Kafka를 통해 해결하여, CDC 시스템을 만드는 것 Debezium MySQL Connector\nDB로부터 데이터의 변경 이벤트를 감지해서 Kafka 이벤트를 발행해주는 것 Mysql의 binlog를 읽어 INSERT, UPDATE, DELETE 연산에 대한 변경 이벤트를 만들어 KAfka 토픽으로 이벤트를 전송 binlog를 기반으로 데이터를 수집하기 때문에, DB에서 수행된 모든 이벤트가 안정적으로 수집되고, 이벤트 발행시 정확한 순서 보장 Kafka CDC를 활용 방법 # 사전준비 Kafka Kafka Connect Debezium MySQL Connector Kafka CDC를 활용한 코드 작성하기 Kafka를 통해 넘어오는 이벤트 레코드를 변환해야 하는데, Debezium MySQL Connector는 Apache Avro를 지원한다. 이를 사용하려면 스키마 레지스트리를 사용해야한다. 스키마 레지스트리에 등록된 스키마를 받기 위해서, 프로젝트의 gradle에 설정을 추가해두면 편하다. val schemaRegistry = \u0026#34;http://localhost:8081\u0026#34; // 스키마 레지스트리 주소 val downloadInputs = listOf( \u0026#34;schema.data-key\u0026#34;, \u0026#34;schema.data-value\u0026#34; ) val avroDestination = \u0026#34;org/main/avro\u0026#34; //avro 스키마가 저장될 프로젝트상의 위치 schemaRegistry { url.set(schemaRegistry) download { // 패턴에 해당하는 서브젝트(스키마)를 다운로드 downloadInputs.forEach { subjectPattern( inputPattern = it, file = avroDestination ) } } } arvo 스키마 { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Envelope\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;schema.data\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;before\u0026#34;, \u0026#34;type\u0026#34;: [ \u0026#34;null\u0026#34;, { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Value\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; } // ... ], \u0026#34;connect.name\u0026#34;: \u0026#34;schema.data\u0026#34; } ], \u0026#34;default\u0026#34;: null }, { \u0026#34;name\u0026#34;: \u0026#34;after\u0026#34;, \u0026#34;type\u0026#34;: [ \u0026#34;null\u0026#34;, \u0026#34;Value\u0026#34; ], \u0026#34;default\u0026#34;: null }, { \u0026#34;name\u0026#34;: \u0026#34;source\u0026#34;, \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;io.debezium.connector.mysql\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;version\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } // ... ], \u0026#34;connect.name\u0026#34;: \u0026#34;io.debezium.connector.mysql.Source\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;op\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;ts_ms\u0026#34;, \u0026#34;type\u0026#34;: [ \u0026#34;null\u0026#34;, \u0026#34;long\u0026#34; ], \u0026#34;default\u0026#34;: null }, { \u0026#34;name\u0026#34;: \u0026#34;transaction\u0026#34;, \u0026#34;type\u0026#34;: [ \u0026#34;null\u0026#34;, { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ConnectDefault\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;io.confluent.connect.avro\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;total_order\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;data_collection_order\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; } ] } ], \u0026#34;default\u0026#34;: null } ], \u0026#34;connect.name\u0026#34;: \u0026#34;schema.Envelope\u0026#34; } 빌드 후 생성되는 클래스 fun Envelop.toBefore(): CdcRecord? { val before = this.getBefore() ?: return null return CdcRecord( //... ) } ConsumerConfig 설정 @Configuration class CdcConsumerConfig { @Bean(CDC_CONTAINER_FACTORY) fun cdcListenerContainerFactory( properties: CdcConsumerProperties, @Value(\u0026#34;\\${spring.kafka.bootstrap-servers}\u0026#34;) bootstrapServers: String ): KafkaListenerContainerFactory\u0026lt;ConcurrentMessageListenerContainer\u0026lt;String, Envelope\u0026gt;\u0026gt; { val factory = ConcurrentKafkaListenerContainerFactory\u0026lt;String, Envelope\u0026gt;() factory.consumerFactory = DefaultKafkaConsumerFactory( mapOf( ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG to bootstrapServers, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG to properties.keyDeserializerClass, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG to properties.valueDeserializerClass, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG to properties.enableAutoCommit, ConsumerConfig.MAX_POLL_RECORDS_CONFIG to properties.maxPollRecords, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG to properties.autoOffsetReset, // Schema Registry, Avro 관련 설정 필수 KafkaAvroDeserializerConfig.SCHEMA_REGISTRY_URL_CONFIG to properties.schemaRegistryUrl, KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG to properties.specificAvroReader, ) ) // ... } } 이벤트 리스너 생성 class CdcEventListener( private val cdcEventProcessor: List\u0026lt;CdcEventProcessor\u0026gt;, ) { private val sinkQueue = Queues.get\u0026lt;List\u0026lt;Envelope\u0026gt;\u0026gt;(4096).get() private val sinks = Sinks.many() .unicast() .onBackpressureBuffer(sinkQueue) private lateinit var disposable: Disposable @KafkaListener( topics = [\u0026#34;\\${kafka.cdc.topic}\u0026#34;], groupId = \u0026#34;\\${kafka.cdc.groupId}\u0026#34;, containerFactory = CdcConsumerConfig.CDC_CONTAINER_FACTORY, ) fun listen( @Payload payloads: List\u0026lt;Envelope?\u0026gt;, @Header(KafkaHeaders.RECEIVED_PARTITION_ID) partition: Int, @Header(KafkaHeaders.RECEIVED_TOPIC) topic: String, @Header(KafkaHeaders.RECEIVED_TIMESTAMP) ts: Long, acknowledgment: Acknowledgment, ) { // ... } } 조건 : CDC 이벤트의 종류에 따라서 다른 알림을 보내야 한다. 서로 다른 이벤트의 종류를 받을 이벤트 프로세서 + 각 이벤트 내에서 어떤 알림을 보낼지 결정하는 이벤트 핸들러\n요청 건에 대한 어떤 이벤트인가? : 가게 로고 수정, 가게 소개 수정 등\n어떤 이벤트인가? : 요청이 승인됨, 요청이 반려됨 등 각 이벤트 프로세서에 이벤트를 전달하는 코드\nclass CdcEventListener( private val cdcEventProcessor: List\u0026lt;CdcEventProcessor\u0026gt;, ) { // ... @PostConstruct protected fun init() { disposable = sinks.asFlux() // ... // 이벤트를 처리 하는 과정에서 doAlarm으로 알림 발송 .doOnNext(::doAlarm) // ... } private fun doAlarm(CdcRecords: List\u0026lt;Envelope\u0026gt; = emptyList()) { Flux.fromIterable(CdcRecords) .flatMap { Mono.fromCallable { // 각각의 이벤트 프로세서에게 이벤트를 처리 하도록 지시 cdcEventProcessor.forEach { processor -\u0026gt; try { processor.process( before = it.toBefore(), after = it.toAfter(), ) } catch (e: Exception) { log.warn(\u0026#34;[CdcEventProcessor] occured exception\u0026#34;, e) } } }.subscribeOn(Schedulers.boundedElastic()) }.subscribe() } } 이벤트를 처리하는 이벤트 프로세서 @Service class NotificationCenterAlarmFacade( private val notificationHandlers: List\u0026lt;NotificationHandler\u0026gt; ) : CdcEventProcessor { override fun process(before: CdcRecord?, after: CdcRecord?) { log.debug(\u0026#34;[알림서비스] process 진입 before = `{}`, after = `{}`\u0026#34;, before, after) if (after == null) { log.info(\u0026#34;[알림서비스] 데이터 삭제건에 대해서는 알림서비스 발송처리를 하지 않습니다. before: `{}`\u0026#34;, before) return } notificationHandlers.find { it.accept(before = before, after = after) } ?.send(record = after) } } 이벤트 프로세서에서는 자신이 전달받은 이벤트에 대해서 처리할 수 있는 이벤트인지 확인한 후, 처리할 수 있는 이벤트의 종류라면 자신이 가지고 있는 이벤트 핸들러들에게 처리를 위임한다.\n핸들러는 본인이 처리할 수 있는 이벤트인지 확인하고 알림을 보낸다. -\u0026gt; B2B 알림 서비스 동작 @Component class CompleteHandler : NotificationHandler { override fun send(record: CdcRecord) { // 알림 발송 로직 } override fun accept(before: CdcRecord?, after: CdcRecord?): Boolean { // 완료 이벤트에 대한 알림을 발송하는 핸들러이기 때문에, 완료 이벤트인지 확인하는 조건 if (before == null || after == null || before.status == Complete || after.status != Complete ) { return false } log.info(\u0026#34;[알림서비스] 완료 이벤트 감지 `{}`\u0026#34;, after.id) return true } } 이제 특정 요청 건들의 상태가 변경되면 자동으로 알림을 받을 수 있다.\n주의할점 # AWS Aurora 환경에서 쓰기 부하가 많은 경우 Debezium MySQL Connector를 연동하면 binlog dump thread가 Aurora MySQL 클러스터 스토리지의 binlog를 읽는데, 이때 락을 건다. Aurora MySQL 2.10.2 미만의 버전에서는 쓰기 부하가 많은 경우 부하가 심해질 수 있다. binlog dump thread의 부하가 심해지는 경우 INSERT, UPDATE, DELETE, COMMIT 등 DML 관련 레이턴시가 증가하게 되고, 이에 따라 장애가 발생할 수 있다.\n중복 메시지 발생의 가능성 여러 가지 경우로 Kakfa 메시지는 중복될 수 있다.\n해결 방법 : Redis Cache\nclass CdcEventListener( private val cdcEventProcessor: List\u0026lt;CdcEventProcessor\u0026gt;, ) { // ... @PostConstruct protected fun init() { disposable = sinks.asFlux() // ... // 이벤트를 처리 하는 과정에서 doCheckDuplicationPrevent 으로 중복 확인 .flatMap(::doCheckDuplicationPrevent) // ... } private fun doCheckDuplicationPrevent(cdcRecords: List\u0026lt;Envelope\u0026gt;): Mono\u0026lt;List\u0026lt;Envelope\u0026gt;\u0026gt; { return Mono.fromCallable { cdcRecords.filter { // HashCode를 이용한 RedisKey 생성 val key = RedisCacheType.DUPLICATION_PREVENT.addPostfix(name = \u0026#34;${it.getAfter().getId()}:${it.hashCode()}\u0026#34;) // 해당 Key가 이미 존재하는지 확인 val existKey = redisTemplate.opsForValue().existKey( key = key ) log.debug( \u0026#34;[CDC][EventEmitterSinks] Check Duplication Prevent. Key = `{}`, Value = `{}`\u0026#34;, key, existKey ) (!existKey) } }.subscribeOn(Schedulers.boundedElastic()) } } 정리 # CDC 키워드로 보게된 기술 블로그 포스팅이였는데, CDC에 대한 감을 잡는데에 도움이 된것같다. CDC, Kafka CDC, Debezium MySQL Connector 키워드에 대해서 좀더 공부해야할 필요를 느꼈다.\nReferences https://techblog.woowahan.com/10000/?ref=codenary "},{"id":89,"href":"/docs/mongodb/003_spring_data_mongodb/","title":"003 Spring Data Mongodb","section":"Mongodb","content":" Spring data mongodb reactive # Entity # 데이터베이스에서 하나의 Document와 매칭되는 클래스 ReactiveMongoEntityTemplate, ReactiveMongoRepository 등은 데이터베이스에 요청을 보내고 그 결과를 Entity 형태로 반환한다. Collection, Document에 필요한 데이터베이스 metadata를 어노테이션 등으로 제공 ReactiveMongoTemplate # ReactiveMongoTemplate은 Spring data mongodb reactive의 추상화 클래스 Mongo 쿼리들을 Bson 형태로 넘기거나 PojoCodec, Custom codec 등을 등록하지 않아도, 메소드 체이닝을 통해서 쿼리를 수행하고 결과를 entity 객체로 받을 수 있다 ReactiveMongoOperations를 구현 MongoTemplate 생성 # MongoClient와 databaseName을 전달하여 생성 가능 spring에서는 ReactiveMongoDatabaseFactory와 MongoConverter을 주입받아 생성 ReactiveMongoOperations # ReactiveMongoTemplate의 operations를 담당하는 interface ReactiveFluentMongoOperations를 상속하고 MongoConverter 제공 MongoConverter: 주어진 Document를 Entity로 만드는 converter ReactiveMongoDatabaseFactory # getMongoDatabase: MongoDatabase를 반환 getCodecRegistry: bson의 CodecRegistry를 반환 ReactiveMongoTemplate은 ReactiveMongoDatabaseFactory의 MongoDatabase를 통해서 MongoCollection에 접근 ReactiveMongoTemplate 구현 # ReactiveMongoTemplate은 createMono 혹은 createFlux를 이용하여 MongoCollection을 획득 ReactiveMongoTemplate 구현 # createFlux를 이용하여 collectionName과 callback을 전달 callback에서 Publisher를 반환 이런 방식으로 여러 operations를 구현 MongoConverter # MongoWriter(EntityWriter를 상속), EntityReader를 구현\n구현체로 MappingMongoConverter 다양한 전략을 통해서 Object \u0026lt;-\u0026gt; document 변환\ncustom converter로 mapping Spring data의 object mapping convention 기반의 mapping metadata 기반의 mapping Custom converter mapping # Configuration를 통해서 converter들을 등록 데이터베이스에 읽고 쓰기를 하기 위해 두개의 Converter가 필요 bson.Document -\u0026gt; Target 클래스로 변환하는 Converter Target -\u0026gt; bson.Document로 변환하는 Converter ReadConverter # Document -\u0026gt; source, Entity -\u0026gt; target으로 하는 Converter Document로부터 name으로 field에 접근할 수 있고, 변환하고나 type에 해당하는 메서드 호출 WriteConverter # Entity -\u0026gt; source, Document -\u0026gt; target으로 하는 Converter Document에 값을 추가 put을 통해서 field의 이름, value 순으로 전달 Document를 데이터베이스에 전달 위 CustomConverter 등록 # AbstractMongoClientConfiguration를 상속하는 Configuration 생성 AbstractR2dbcConfiguration의 configureConverters에 custom converter들을 register References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":90,"href":"/docs/mongodb/004_object_mapping/","title":"004 Object Mapping","section":"Mongodb","content":" Object mapping # Spring data의 object mapping # 만약 지원하는 converter가 없다면 MappingMongoConverter는 다음 과정을 거쳐서 Document를 entity로 변환 Object creation constructor, factory method 등을 이용해서 Document의 field들로 Object 생성 Property population setter, with.. 메소드 등을 이용해서 Document의 field를 Object에 주입 Object creation # 다음 순서로 체크하여 해당하는 알고리즘으로 Document를 Object로 변환 @PersistenceCreator 어노테이션을 갖는 constructor가 있다면 해당 constructor 사용 인자가 없는 constructor가 있다면 해당 constructor 사용 constructor가 정확히 하나 있다면 해당 constructor 사용 id mapping # mongodb에서 모든 document는 _id를 필요 MappingMongoConverter는 다음의 방법으로 _id를 감지 @Id가 붙어있는 필드 필드명이 id이고 @Field를 통해서 별도의 이름이 부여되지 않은 경우 id 필드가 제공되지 않는 경우, 자동으로 추가 Property population # r2dbc에서는 property가 mutable할때만 property population 적용이 가능했지만, mongodb에서는 with 메소드 지원 No-args constructor를 호출하여 텅 빈 객체를 만들고, gender를 제외한 나머지 필드는 reflection으로 진행 gender는 withGender 메소드 호출 Metadata Mapping # Entity 클래스에 annotation을 추가하여 데이터베이스와 관련된 설정들을 주입 @Id: _id에 해당하는 필드에 적용 @Document: entity class에 적용. Collection 이름을 변경 가능 @DBRef: mongodb의 DBRef 형태로 저장해야 하는 필드 @Indexed: 필드에 대해서 인덱스를 생성. 기본적으론 자동 생성이 비활성화이므로 별도로 설정 필요 @CompoundIndex: 클래스에 적용. 여러 필드로 구성된 복합 인덱스 제공 @TextIndexed: 필드에 text index를 적용 @HashIndexed: 필드에 hash index를 적용 @Transient: 기본적으로 모든 필드는 mapping 대상. @Transient가 붙은 필드는 mapping에서 제외. @Field: entity의 property 필드에 적용. @Field가 붙은 필드에 대해서는 convention 기반 대신 Field에 주어진 name으로 적용 @Version: 낙관적 잠금 (Optimistic Lock)에 이용. entity가 update 될때마다 자동으로 update @PersistenceConstructor: 특정 constructor에 대해서 Object creation할 때 사용하게끔 지정. constructor의 argument 이름에 따라서 mapping References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":91,"href":"/docs/mongodb/005_mongoOperations/","title":"005 Mongo Operations","section":"Mongodb","content":" ReactiveMongoOperations # ReactiveMongoOperations # ReactiveFluentMongoOperations를 상속 ReactiveFluentMongoOperations는 여러 Operations를 상속 ReactiveFindOperation: find query와 관련된 메서드 제공 ReactiveInsertOperation: insert query와 관련된 메서드 제공 ReactiveUpdateOperation: update query와 관련된 메서드 제공 ReactiveRemoveOperation: delete query와 관련된 메서드 제공 ReactiveAggregationOperation: aggregation query와 관련된 메서드 제공 ReactiveChangeStreamOperation: watch query와 관련된 메서드 제공 ReactiveFindOperation # ReactiveFindOperation의 query 부터 시작 TerminatingFind의 count, exists, first, one, all, tail 등으로 종료 query -\u0026gt; inCollection -\u0026gt; as -\u0026gt; matching -\u0026gt; 최종 query -\u0026gt; inCollection -\u0026gt; matching -\u0026gt; 최종 query -\u0026gt; as -\u0026gt; matching -\u0026gt; 최종 query -\u0026gt; matching -\u0026gt; 최종 query -\u0026gt; 최종 ReactiveFindOperation # inCollection query를 실행할 collection 이름을 전달 제공되지 않을 경우 domain Type의 class 이름 통해 collection 이름 획득 @Document 어노테이션 통해 collection 이름 획득 as Entity를 전부 mapping하지 않고 특정 필드만 mapping 하고 싶은 경우 Entity의 일부 property만 담고 있는 subclass 또는 interface를 넘겨서 projection projection이 제공되지 않는다면 Entity에 모든 필드를 mapping matching query의 filter에 해당 Query를 전달하여 filter에 들어갈 내용을 설정 matching을 생략하면 collection 전체에 대한 요청을 보내는 것과 동일 최종 마지막으로 count, exists, first, one, all, tail 등의 연산을 선택 count: 조건에 맞는 document의 개수 반환 exists: 조건에 맞는 document 존재 여부 반환 first: 조건에 맞는 첫 번째 document 반환 one: 조건에 맞는 하나의 document 반환. 하나가 넘으면 exception all: 조건에 맞는 모든 document 반환 tail: cursor를 이용하여 조건에 해당하는 document를 지속적으로 수신 ReactiveFindOperation 실행 # MongoClient를 이용하여 ReactiveMongoTemplate을 생성 Query와 Criteria를 이용해서 query 생성 PersonNameOnlyDocument class를 이용해서 id와 name만 projection ReactiveInsertOperation # ReactiveInsertOperation의 insert 부터 시작하여 TerminatingInsert의 one, all로 종료 insert -\u0026gt; into -\u0026gt; (one, all) insert -\u0026gt; (one, all) ReactiveInsertOperation # one insert query에 이용할 entity 하나를 전달 주어진 entity를 Document로 변환하고 insert 결과를 Mono로 반환 all bulk insert 지원 주어진 entity Collection -\u0026gt; Document Collection으로 변환하고 insert 결과를 flux로 반환 ReactiveInsertOperation 실행 # inCollection을 통해서 insert할 collection 명시 entity를 생성하여 all에 전달 ReactiveUpdateOperation # ReactiveUpdateOperation의 update 부터 시작 update, findAndModify, findAndReplace 지원 findAndReplace # Document를 찾고 다른 Document로 대체 쿼리를 실행하고 그 결과를 Mono로 반환 replaceWith : 대상을 찾게되었을때 대체할 객체를 제공 withOptions : findAndReplace에 대한 옵션 제공 returnNew : true -\u0026gt; 대체된 document를 반환, false(default) -\u0026gt; 기존 document를 반환 upsert : true -\u0026gt; 조건에 만족하는 document가 없는 경우 insert, false -\u0026gt; 존재하는 경우에만 as : 값을 대체한 후 그 결과를 전부 mapping하지 않고 특정 필드만 mapping 하고 싶은 경우 findAndModify # 값을 찾아서 update 하고 그 결과를 Mono로 변환 withOptions : findAndModify에 대한 옵션 제공 returnNew : true -\u0026gt; 대체된 document를 반환, false(default) -\u0026gt; 기존 document를 반환 upsert : true -\u0026gt; 조건에 만족하는 document가 없는 경우 insert, false -\u0026gt; 존재하는 경우에만 remove : true -\u0026gt; update 대신 delete 수행 update # apply : update를 수행 insert와 다르게 Entity가 아닌 Update 객체 전달 Update 객체의 update, fromDocument 등을 통해서 생성 set, unset, setOrInsert, inc, push, pop, pull, rename, currentDate, multiply 등의 연산 지원\\ all : 조건을 만족하는 모든 document에 대해 update first : 조건을 만족하는 첫 document에 대해서 update upsert: 조건을 만족하는 document가 있다면 update하고 없다면 새로 생성 findAndReplace 실행 # inCollection을 통해서 update할 collection 명시 matching으로 update 영향을 받는 document 제한 returnNew 옵션 제공 findAndModify 실행 # inCollection을 통해서 update할 collection 명시 matching으로 update 영향을 받는 document 제한 Update 객체로 여러 필드 수정 update 실행 # inCollection을 통해서 update할 collection 명시 조건을 만족하는 document가 없게 만듬 Update 객체로 여러 필드 수정 upsert로 새로 추가 upsert: 조건을 만족하는 document가 있다면 update하고 없다면 새로 생성 ReactiveRemoveOperation # ReactiveRemoveOperation의 remove 부터 시작하여 TerminatingRemove의 all, findAndRemove으로 종료 remove -\u0026gt; inCollection -\u0026gt; matching -\u0026gt; 실행 remove -\u0026gt; inCollection -\u0026gt; 실행 remove -\u0026gt; matching -\u0026gt; 실행 remove -\u0026gt; 실행 ReactiveAggregationOperation # ReactiveAggregationOperation의 aggregateAndReturn 부터 시작하여 TerminatingAggregationOperation의 all로 종료 aggregateAndReturn -\u0026gt; inCollection -\u0026gt; by -\u0026gt; all aggregateAndReturn -\u0026gt; by -\u0026gt; all ReactiveChangeStreamOperation # ReactiveChangeStreamOperation의 changeStream 부터 시작하여 TerminatingChangeStream의 listen로 종료 withOptions: changeStream과 관련된 옵션 제공 filter: stream을 listen하는 동안 filter할 대상 as: stream의 결과로 mapping할 Class 제공 resumeAt: 주어진 Token부터 listen 재개 resumeAfter: 주어진 Token 이후부터 listen 재개 startAfter: 주어진 Token부터 listen 새로 시작 mongoTemplate.changeStream(ChatDocument.class) .listen() .doOnNext(item -\u0026gt; { ChatDocument target = item.getBody(); OperationType operationType = item.getOperationType(); log.info(\u0026#34;target: {}\u0026#34;, target); log.info(\u0026#34;type: {}\u0026#34;, operationType); if (target != null \u0026amp;\u0026amp; operationType == OperationType.INSERT) { String from = target.getFrom(); String to = target.getTo(); String message = target.getMessage(); doSend(from, to, message); } }) .subscribe(); ReactiveMongoOperations # ReactiveFluentMongoOperations에서 제공하는 조합 방식 대신 다양한 쿼리를 수행하는 단축 메소드 제공 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":92,"href":"/docs/mongodb/006_reactiveMongoRepository/","title":"006 Reactive Mongo Repository","section":"Mongodb","content":" ReactiveMongoRepository # ReactiveMongoRepository # ReactiveSortingRepository, ReactiveQueryByExampleExecutor를 상속한 interface SimpleReactiveMongoRepository에서 구현 ReactiveMongoRepository 등록 # MongoReactiveRepositoriesAutoConfiguration가 활성화되어 있다면 SpringBootApplication 기준으로 자동으로 scan 혹은 EnableReactiveMongoRepositories를 통해서 repository scan SimpleReactiveMongoRepository # ReactiveMongoRepository를 구현 ReactiveMongoOperations를 기반으로 Mongo 쿼리를 실행하고 결과를 Entity로 mapping save # save mongoOperations의 insert 혹은 update를 이용 새로운 entity라면 insert, 아니라면 update, Id 필드가 null이라면 new saveAll concatMap을 이용하여 save를 순차적으로 실행 전부 new entity라면 bulkInsert, 아니라면 각각을 save @Transactional이 없는 점 find # findById, existsById, count 모두 ReactiveMongoOperations에서 제공하는 단축 메소드 (findById, exists, count) 사용 delete # ReactiveMongoOperations에서 제공하는 단축 메소드 (remove) 사용 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":93,"href":"/docs/mongodb/007_query_method/","title":"007 Query Method","section":"Mongodb","content":" Query method # 쿼리 메소드 (Query method) # ReactiveMongoRepository를 상속한 repository interface에 메소드를 추가 메소드의 이름을 기반으로 Query 생성 조회, 삭제 지원 @Query, @Update, @Aggregation 어노테이션을 사용해서 복잡한 쿼리 실행 가능 쿼리 메소드 - find # id 뿐만 아니라 다른 필드를 이용해서 조회 가능 first 등의 키워드를 사용해서 query에 limit 제공 가능 기존의 Entity 뿐만 아니라 Projection을 사용하여 일부 필드만 조회 가능 사용예제\nfindFirstByNameOrderByAgeDesc name이 “taewoo”인 row들을 찾고 age 내림차순으로 sort 하여 limit을 1로 모든 field를 조회하여 PersonDocument class로 mapping 쿼리 메소드 - delete # 다른 필드를 이용해서 삭제 가능 여러 반환 타입 지원 Long: 영향을 받은 row 수 반환 Flux: 삭제된 document 반환 사용예제\ndeleteByAgeGreaterThan age가 100 초과인 document를 찾고 삭제한 후 영향을 받은 document가 있다면 Flux 형태로 반환 쿼리 메소드 시작 키워드 # find, read, get, query, search, stream find 쿼리를 실행하고 결과를 Publisher으로 반환 exists find exists 쿼리를 실행하고 결과를 Publisher으로 반환 count find count 쿼리를 실행하고 결과를 Publisher으로 반환 delete, remove delete 쿼리를 실행하고 Publisher 혹은 publisher로 삭제된 개수 반환 First, Top 쿼리의 limit을 N으로 설정. find와 By 사이 어디에든 등장 가능 Distinct distinct 기능을 제공. find와 By 사이 어디에든 등장 가능 쿼리 메소드 지원 키워드 # And: $and Or: $or After: $gt Before: $lt Containing,:regex로 제공 (String), $in (Collection) Between, IsBetween: $gt, $lt EndingWith: regex로 제공 Exists: $exists False, IsFalse: false와 비교 GreaterThan: $gt GreaterThanEqual: $gte In: $in NotNull, IsNotNull: $ne:null Null, IsNull: null LessThan: $lt LessThanEqual: $lte Like: regex로 제공 Near: $near Not: $not NotIn: $nin NotLike, IsNotLike: $not Regex,: $regex StartingWith: regex로 제공 True, IsTrue: true와 비교 Within, IsWithin: $geoWithin OrderBy: 주어진 property path와 direction에 따라서 쿼리에 Sort 제공 쿼리 메소드 - @Query # query가 메소드 이름으로 전부 표현이 되지 않는 경우 쿼리 메소드 예약어에서 지원되지 않는 문법을 사용하는 경우 복잡한 query문을 사용하는 경우 @Update와 조합하여 update를 수행 쿼리 메소드 - @Aggregate # @Aggregate를 이용해서 mongo aggregate수행 pipeline들을 array 형태로 전달 각각의 pipeline이 순차적으로 수행되고 인자로 넘긴 값들이 사용 사용예제\naggregateGroupByName 2개의 aggregate pipeline 포함 name이 주어진 인자와 같은 document만 필터 name으로 group 하여 count @Transactional # @Transactional를 사용하여 여러 query를 묶어서 진행 새로운 Entity를 만들어서 save하고 update한 후 findAll을 통해서 모든 document 반환 TransactionalOperator # transactional 메소드를 통해서 주어진 Flux 혹은 Mono를 transaction 안에서 실행 flux를 바로 반환하지 않고 transactionalOperator의 transactional로 wrapping 하여 전달\n혹은 execute를 통해서 TransactionCallback 형태로 실행 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":94,"href":"/docs/mongodb/001_reactive_mongodb/","title":"001 Reactive Mongodb","section":"Mongodb","content":" Reactive MongoDB driver # MongoDB driver # MongoDB사에서 공식적인 2가지 java driver를 제공 Sync Driver Reactive Streams Driver Sync driver # 동기적으로 동작 클라이언트 요청을 보내면 응답이 돌아오기 전까지 쓰레드가 blocking 메서드가 응답 객체를 바로 반환 -\u0026gt; 직관적 쓰레드 동시성 문제 발생 가능성 Reactive Streams driver # 비동기적으로 동작 클라이언트가 요청을 보내면 쓰레드는 non-blocking 모든 응답이 publisher를 이용해서 전달되기 때문에 처리하기 어렵다. Spring reactive stack과 함께 사용되어 높은 성능, 안정성 제공 Spring Data MongoDB Reactive, REactive Streams MongoDB Driver # Mongo Reactive streams driver # MongoCollection 획득 # MongoDB의 MongoClient, MongoDatabase, MongoCollection MongoClient MongoDB 클러스터를 가리키는 객체 (MongoDatabase factory 역할) MongoDatabase Mongo의 Database를 가리킨다. Codec, WriteConcern, ReadPreference 등의 정보를 포함 collection 이름을 인자로 받고 MongoCollection 제공 MongoCollection MongoDB의 Collection mongodb commands를 실행 MongoCollection 획득 예제 # ConnectionString을 이용해서 MongoDB 연결 정보를 String 형태로 제공 MongoClientSettings builder에 Connection 정보를 전달 MongoClientSettings로 MongoClient 생성 MongoClient로 MongoDatabase 접근 MongoDatabase로 MongoCollection 접근 MongoCollection - count # ClientSession을 통해서 multi document transaction 제공 다양한 수행건들의 트랜잭션을 ClientSession을 사용해서 하나로 묶을 수 있음 Bson 구현체 (BsonDocument 등)로 filter 제공 CountOptions로 hint, limit, skip, maxTime, collation 등의 정보 제공 // CountOptions를 사용하여 옵션 설정 CountOptions countOptions = new CountOptions(); countOptions.skip(10); // 처음 10개 문서 제외하고 count countOptions.limit(20); // 최대 20개의 문서만 count countOptions.maxTime(5000); // 최대 수행 시간 (5초) // countDocuments 메서드에 CountOptions 적용 long documentCount = collection.countDocuments(countOptions); System.out.println(\u0026#34;Total documents in the collection: \u0026#34; + documentCount); MongoCollection - find # Filters helper 클래스를 통해서 filter 설정 가능 eq, ne, gt, gte, lt, lte, in, nin, and, or, not, nor, exists, type, mod, regex, text 등의 기본 연산자 제공 geoWithin, geoWithinBox 등의 geo 연산자도 제공 // Filters를 사용한 쿼리 조건 생성 // 예: age가 30 이상인 문서 검색 FindIterable\u0026lt;Document\u0026gt; result = collection.find(Filters.gte(\u0026#34;age\u0026#34;, 30)); // 결과 출력 for (Document document : result) { System.out.println(document.toJson()); } aggregate # pipeline을 생성하고 mongo shard 전체에 대해서 필터, 집계, 그룹 등의 연산을 수행 MongoCollection - aggregate # Aggregates helper 클래스를 통해서 aggregate pipeline 제공 addFields, set, bucket, bucketAuto count, match, project, sort, sortByCount skip, limit, lookup facet, graphLookup, group, unionWith, unwind, out, merge, replaceRoot, replaceWith, sample 등 AggregatePublisher를 반환 // MongoDB 연결 설정 AggregatePublisher\u0026lt;Document\u0026gt; publisher = MongoClients.create() .getDatabase(\u0026#34;your_database_name\u0026#34;) .getCollection(\u0026#34;your_collection_name\u0026#34;, Document.class) .aggregate(Arrays.asList( new Document(\u0026#34;$group\u0026#34;, new Document(\u0026#34;_id\u0026#34;, \u0026#34;$city\u0026#34;).append(\u0026#34;totalPopulation\u0026#34;, new Document(\u0026#34;$sum\u0026#34;, \u0026#34;$population\u0026#34;))), new Document(\u0026#34;$sort\u0026#34;, new Document(\u0026#34;totalPopulation\u0026#34;, -1)), new Document(\u0026#34;$limit\u0026#34;, 5) )); // 결과 처리 publisher.subscribe( document -\u0026gt; System.out.println(document.toJson()), throwable -\u0026gt; System.err.println(\u0026#34;Error: \u0026#34; + throwable.getMessage()), () -\u0026gt; System.out.println(\u0026#34;Aggregation completed\u0026#34;) ); MongoCollection - watch # Aggregates helper 클래스를 통해서 aggregate pipeline 제공 addFields, match, project, replaceRoot, replaceWith, redact, set, unset 지원 ChangeStreamPublisher를 반환하고 해당 Publisher를 subscribe ChangeStreamDocument를 onNext로 전달 resumeToken, 변경사항이 발생한 document 혹은 _id // ChangeStreamPublisher를 얻어옴 Publisher\u0026lt;ChangeStreamDocument\u0026lt;Document\u0026gt;\u0026gt; changeStreamPublisher = mongoCollection.watch(); // Publisher를 사용하여 Subscriber에게 변경 사항을 알림 changeStreamPublisher.subscribe(new ExampleSubscriber()); MongoCollection - bulkWrite # Delete, Insert, Replace, Update 등을 모아서 한번에 실행하는 operation WriteModel DeleteManyModel: 조건을 만족하는 document를 모두 삭제 DeleteOneModel: 조건을 만족하는 document를 최대 1개만 삭제 InsertOneModel: 하나의 document를 추가 ReplaceOneModel: 조건을 만족하는 document를 최대 1개만 대체 UpdateManyModel: 조건을 만족하는 document를 모두 수정 UpdateOneModel: 조건을 만족하는 document를 최대 1개만 수정 // BulkWrite 작업 생성 List\u0026lt;WriteModel\u0026lt;Document\u0026gt;\u0026gt; bulkOperations = new ArrayList\u0026lt;\u0026gt;(); // 삽입 작업 Document document1 = new Document(\u0026#34;_id\u0026#34;, 1).append(\u0026#34;name\u0026#34;, \u0026#34;John\u0026#34;); InsertOneModel\u0026lt;Document\u0026gt; insertOneModel1 = new InsertOneModel\u0026lt;\u0026gt;(document1); bulkOperations.add(insertOneModel1); // 업데이트 작업 Document filter = new Document(\u0026#34;_id\u0026#34;, 2); Document update = new Document(\u0026#34;$set\u0026#34;, new Document(\u0026#34;age\u0026#34;, 25)); UpdateOneModel\u0026lt;Document\u0026gt; updateOneModel = new UpdateOneModel\u0026lt;\u0026gt;(filter, update); bulkOperations.add(updateOneModel); // 삭제 작업 DeleteOneModel\u0026lt;Document\u0026gt; deleteOneModel = new DeleteOneModel\u0026lt;\u0026gt;(Filters.eq(\u0026#34;_id\u0026#34;, 3)); bulkOperations.add(deleteOneModel); // BulkWrite 수행 BulkWriteResult result = mongoCollection.bulkWrite(bulkOperations); MongoCollection - insert # 하나 혹은 여러 document를 추가하는 operation InsertOneOptions, InsertManyOptions validation 우회 여부를 결정 InsertManyOptions라면 insert의 순서를 보장할지 결정 InsertOneResult, InsertManyResult wasAcknowledged() : write 성공 여부 getInsertedIds() : write된 id들을 제공 // 삽입할 문서 생성 Document document = new Document(\u0026#34;_id\u0026#34;, 1) .append(\u0026#34;name\u0026#34;, \u0026#34;John\u0026#34;) .append(\u0026#34;age\u0026#34;, 30) .append(\u0026#34;city\u0026#34;, \u0026#34;New York\u0026#34;); // 검사 우회 옵션 추가 Document options = new Document(\u0026#34;bypassDocumentValidation\u0026#34;, true); // 컬렉션에 문서 삽입 (검사 우회) mongoCollection.insertOne(document, options); InsertOneOptions\n// InsertOneOptions를 사용한 단일 문서 삽입 옵션 설정 InsertOneOptions insertOneOptions = new InsertOneOptions(); insertOneOptions.bypassDocumentValidation(true); // 데이터 유효성 검사 우회 // 단일 문서 삽입 Document document = new Document(\u0026#34;_id\u0026#34;, 1) .append(\u0026#34;name\u0026#34;, \u0026#34;John\u0026#34;) .append(\u0026#34;age\u0026#34;, 30); mongoCollection.insertOne(document, insertOneOptions); InsertManyOptions\n// InsertManyOptions를 사용한 다수의 문서 삽입 옵션 설정 InsertManyOptions insertManyOptions = new InsertManyOptions(); insertManyOptions.ordered(false); // 순서 무시 // 다수의 문서 삽입 List\u0026lt;Document\u0026gt; documents = Arrays.asList( new Document(\u0026#34;_id\u0026#34;, 2).append(\u0026#34;name\u0026#34;, \u0026#34;Jane\u0026#34;).append(\u0026#34;age\u0026#34;, 25), new Document(\u0026#34;_id\u0026#34;, 3).append(\u0026#34;name\u0026#34;, \u0026#34;Bob\u0026#34;).append(\u0026#34;age\u0026#34;, 35) ); mongoCollection.insertMany(documents, insertManyOptions); MongoCollection - update # 하나 혹은 여러 document를 수정하는 operation Filters helper 클래스를 통해서 filter 설정 가능 Updates helper 클래스를 통해 update 설정 가능 UpdateOptions를 통해서 upsert, hint, collation, variables 등 제공 // 업데이트 옵션 설정 UpdateOptions options = new UpdateOptions(); options.upsert(true); // 일치하는 문서가 없을 경우 삽입 (upsert) // 업데이트 작업 수행 mongoCollection.updateOne(eq(\u0026#34;name\u0026#34;, \u0026#34;John\u0026#34;), set(\u0026#34;age\u0026#34;, 31), options); MongoCollection - atomic # findOneAndDelete, findOneAndReplace, findOneAndUpdate 등 find와 write를 묶어서 atomic한 operation 제공 트랜잭션 내에서 수행되는 각 작업은 clientSession을 통해 수행 MongoCollection - index # Collection에서 특정 필드들에 대한 index 생성, 조회, 삭제 가능\nIndexes helper 클래스를 통해서 다양한 index 제공 IndexModel과 IndexOptions를 통해서 어떤 필드들에 대해서 어떻게 Index를 적용할 것인지 설정 가능\nbackground: index의 생성을 background에서 진행할지 여부 unique: unique index를 생성할지 여부 name: index에 name 설정 partialFilterExpression: 특별한 조건을 충족한 경우에만 index를 걸고 싶은 경우 설정 // IndexModel을 사용하여 색인 정의 IndexModel indexModel = new IndexModel( new Document(\u0026#34;field1\u0026#34;, 1), // 1은 오름차순, -1은 내림차순 new Document(\u0026#34;field2\u0026#34;, -1) ); // IndexOptions를 사용하여 색인 옵션 설정 IndexOptions indexOptions = new IndexOptions(); indexOptions.name(\u0026#34;custom_index_name\u0026#34;); // 색인의 이름 설정 indexOptions.unique(true); // 고유 색인 설정 indexOptions.background(true); // 비동기적으로 색인 생성 // 색인 생성 mongoCollection.createIndex(indexModel, indexOptions); References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":95,"href":"/docs/mongodb/002_mongodb_document/","title":"002 Mongodb Document","section":"Mongodb","content":" Reactive MongoDB Document # Document # MongoCollection에 query를 실행하여 bson의 Document를 반환 bson의 Document : Map\u0026lt;String, Object\u0026gt;를 구현하고 내부에 LinkedHashMap을 저장하여 Map 메서드 override Document 예제 # collection에서 findAll query를 실행 결과를 subscribe하여 onNext로 출력 모든 결과를 찾은 후, onComplete 이벤트로 종료 MongoDB BSON 인코딩 # 첫 줄은 전체 document의 크기를 가리킨다 데이터타입, 필드명, 길이(데이터 타입에 따라 optional), 값으로 구성 BSON Codec # bson 라이브러리는 Codec을 제공 Codec을 통해서 특정 java type이 주어졌을때 어떻게 encode, docode 해야할지 지정 Default codec # MongoClientSettings에서 Default codec을 제공 Java 자체 클래스와 관련된 codec들 IterableCodecProvider: Iterable 클래스 지원 MapCodecProvider: Map 클래스 지원 ValueCodecProvider: Java에서 제공하는 클래스 지원 Jsr310CodecProvider: Instant, LocalDate, LocalDateTime 등 Date, Time 관련 클래스 지원 EnumCodecProvider: Enum 지원 Jep395RecordCodecProvider: Record 지원 Bson과 관련된 codec들 BsonValueCodecProvider: Bson 타입들을 java로 1대1 맵핑한 클래스 지원 DBRefCodecProvider: DBRef 지원 DBObjectCodecProvider: DBObject 지원 DocumentCodecProvider: Document 지원 GeoJsonCodecProvider: Geometry, LineString, MultiPoint, Point, Polygon 등의 geojson 지원 GridFSFileCodecProvider: GridFSFile 지원 JsonObjectCodecProvider: JsonObject 지원 BsonValueCodecProvider # Bson 타입들과 1 대 1 매칭 BsonNull, BsonUndefined BsonBinary BsonBoolean BsonDateTime, BsonTimestamp BsonDBPointer BsonDouble, BsonInt32, BsonInt64, BsonDecimal128 BsonMinKey, BsonMaxKey BsonJavaScript BsonObjectId BsonRegularExpression BsonString, BsonSymbol ValueCodecProvider # Java 타입들을 지원 Binary (byte[]), Byte, ByteArray (byte[]) Boolean, AtomicBoolean Date Short, Float, Double, Integer, Long, Decimal128, BigDecimal, AtomicInteger, AtomicLong MinKey, MaxKey Code (javascript code) ObjectId Pattern Character, String, Symbol Codec 예제 - StringCodec # String을 binary로 encode 만약 변환 대상이 Bson.STRING 라면 그대로 write 만약 변환 대상이 Bson.OBJECT_ID 라면 ObjectId로 변환해서 write binary를 String으로 decode 만약 Bson.STRING 에서 String 으로 변환하는 경우 readString 혹은 readSymbol 만약 Bson.OBJECT_ID 에서 String으로 변환하는 경우 readObjectId 후 hexString으로 변환 PojoCodec # 주어진 POJO (Plain old java object)를 bson으로 bson을 POJO로 자동 변환하는 Codec PojoCodec은 기본으로 추가되지 않으므로 별도로 추가 필요 PojoCodec 등록 # PojoCodecProvider builder를 이용하여 automatic을 true로 제공 automatic을 true로 제공해야만 pojo 변환을 지원 getCollection의 2번째 인자로 PersonDocument 제공 Document 대신 PersonDocument로 find Custom Codec # Codec 인터페이스를 직접 구현 reader의 readObjectId, readString 등을 사용하면 하나의 필드를 읽고 java 클래스로 mapping 만약 encode 되어있는 필드 순서와 read하는 순서가 다르면 exception 발생 가능 이를 위해 readName을 호출하여 필드 이름을 파악한 후 해당 필드 이름과 매칭되는 readXX 메소드 호출도 가능 Custom Codec 등록 # fromCodecs를 이용하여 custom codec을 CodecRegistry로 변형 해당 CodecRegistry를 fromRegistries로 settings에 등록 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":96,"href":"/docs/batch/001_performance_improved_batch/","title":"001 Performance Improved Batch","section":"Batch","content":" tech blog 글 읽고 정리하기 # 누구나 할 수 있는 10배 더 빠른 배치 만들기 # 우아한형제들 셀러 시스템 배치 개선 이야기 # 우아한형제들 기술 블로그의 글을 읽으면서 정리해본다.\n최근 셀러시스템팀에서 하루 한 번 주기로 실행되는 배치를 최적화하는 과제를 진행한 내용에 대한 포스팅이다.\n비운영 시간 데이터 # 셀러시스템에서는 가게와 업주에 대한 다양한 데이터를 관리 사장님들의 관리 사항 \u0026lsquo;가게가 운영하는지 안하는지\u0026rsquo;에 대한 정보를 유관 부서에 전달한다. \u0026lsquo;비운영시간 데이터\u0026rsquo; 실시간으로 수정되는 정보를 반영 매일 새벽에 전체 데이터를 계산하고 그 결과를 미리 갱신해둔 후, 유관부서에 전파 다양한 채널에서 입력되는 각족 운영과 휴무 데이터를 취합해서 비운영시간 데이터를 계산 위 계산된 데이터가 클라이언트까지 잘 전달될 수 있도록 각 지면에 적절한 형태로 가공하여 제공 문제상황 # 새벽에 배치 작업을 할때, 수많은 가게의 데이터를 매일 갱신하므로 배치 수행시간이 오래걸린다. 배포 예정 시간과 배치 실행 시간이 겹칠 경우, 배포가 여러번 복잡한 절차를 밟아 진행해야한다. (잠재적인 리스크) 배치 성능 개선이 필요 I/O 최적화 # I/O 병목을 먼저 살펴본다. I/O (Input / Output) 병목이란? 컴퓨팅에서 부하를 설명할 때에는 크게 CPU 부하와 I/O 부하로 나뉩니다. 데이터를 계산하고 처리하는 과정인 CPU 부하와 달리, I/O 부하는 디스크에 파일을 읽고 쓰거나 DB 및 외부 컴포넌트와 통신하는 과정에서 발생합니다. I/O 병목은 이러한 I/O 부하가 시스템의 전체적인 효율성을 떨어뜨리는 부분을 말합니다.\n배치에서 사용하는 I/O 부하 중 가장 핵심은 DB 쿼리였다. 원인 : JPA 지연 로딩으로 설정된 연관 관계 엔티티를 가져오는 과정에서 N+1 문제가 발생 데이터가 모두 1:N 구조의 연관관계로 설정되어 있어서 관련 데이터를 가져오는데에 오랜 시간이 걸린다. 해결\n연관관계로 설정된 엔티티의 종류가 많고 실제 연관관계 데이터의 수정은 불필요하다는 점 등을 고려하여, 각 엔티티 정보를 연관관계를 통해 가져오는 것이 아닌 별도 쿼리 호출을 통해 명시적으로 한 번에 읽어오게끔 수정 Before\npublic List\u0026lt;LiveShopClose\u0026gt; generateLiveShopClose(Shop shop, LocalDate startDate, LocalDate endDate) { final List\u0026lt;ShopCalendar\u0026gt; shopCalendars = shopCalendarRepository.findAllByCalendarDateBetween(startDate, endDate); final List\u0026lt;ShopTemporaryClosed\u0026gt; shopTemporaryCloses = shop.getActiveShopTemporaryClosed(); final List\u0026lt;ShopClosed\u0026gt; shopCloses = shop.getActiveShopClosed(); final List\u0026lt;ShopOperationHour\u0026gt; operationHours = shop.getShopOperationHourIsType(OperationHourType.OPERATION); return /* LiveShopClose 데이터 생성 */ } After\nList\u0026lt;LiveShopClose\u0026gt; generateLiveShopCloses(List\u0026lt;Long\u0026gt; shopNos, LocalDate startDate, LocalDate endDate) { List\u0026lt;ShopNo\u0026gt; shopNoEntities = shopNos.stream().map(ShopNo::new).collect(Collectors.toList()); List\u0026lt;ShopCalendar\u0026gt; shopCalendars = shopCalendarRepository.findAllByCalendarDateBetween(startDate, endDate); Map\u0026lt;Long, List\u0026lt;ShopTemporaryClosed\u0026gt;\u0026gt; activeShopTemporaryClosedMap = shopTemporaryClosedRepository.findActiveByShopNos(shopNoEntities).stream() .collect(groupingBy(ShopTemporaryClosed::getShopNo, Collectors.toList())); Map\u0026lt;Long, List\u0026lt;ShopClosed\u0026gt;\u0026gt; activeShopClosedMap = shopClosedRepository.findActiveByShopNos(shopNoEntities).stream() .collect(groupingBy(ShopClosed::getShopNo, Collectors.toList())); Map\u0026lt;Long, List\u0026lt;ShopOperationHour\u0026gt;\u0026gt; operationHoursMap = shopOperationHoursRepository.findOperationHoursByShopNos(shopNoEntities).stream() .collect(groupingBy(ShopOperationHour::getShopNo, Collectors.toList())); return shopNos.stream() .flatMap(shopNo -\u0026gt; generateLiveShopClose( shopNo, shopCalendars, ListUtils.emptyIfNull(activeShopTemporaryClosedMap.get(shopNo)), ListUtils.emptyIfNull(activeShopClosedMap.get(shopNo)), ListUtils.emptyIfNull(operationHoursMap.get(shopNo)) ).stream()) .collect(Collectors.toList()); } List\u0026lt;LiveShopClose\u0026gt; generateLiveShopClose(Long shopNo, List\u0026lt;ShopCalendar\u0026gt; shopCalendars, List\u0026lt;ShopTemporaryClosed\u0026gt; activeShopTemporaryCloses, List\u0026lt;ShopClosed\u0026gt; activeShopCloses, List\u0026lt;ShopOperationHour\u0026gt; operationHours) { return /* LiveShopClose 데이터 생성 */ } 도메인 로직 및 기타 최적화 # 현재 가게의 비운영시간 데이터가 업데이트될 경우, 변경된 가게에 대한 이벤트를 발행 기존 로직에서는 실제 데이터의 변경 여부와는 관계없이 D-1~D+2 데이터를 무조건 재생성하기 때문에, 실제로는 데이터가 변경되지 않을 테지만 다시 데이터가 생성되어 변경 이벤트가 전송되는 케이스 존재 대응 : 이러한 케이스에 대응하여 데이터가 바뀌었는지 여부를 확인한 후 실제로 바뀐 경우에만 변경 사항을 적용 효과 : 변경 이벤트로 인한 간접적인 부하 개선 최적화 검토 # 성능 효율을 높이기 위해 컴퓨팅 업계에서는 많은 죄악이 저질러지는데(심지어 효율적이지조차 않을 때도 있다) 그 수는 그냥 멍청해서 저지르는 죄악보다 많다. William A. Wulf (1972) 우리는 세세한 성능 효율에 대해서는 무시할 필요가 있다. 말하자면 97%가 이 경우에 해당한다. 섣부른 최적화는 만악의 근원이다. Donald E. Knuth (1974) 우리는 최적화에 대해서 다음 두가지 규칙을 따른다. 첫째. 하지 마라. 둘째. (전문가 한정) 아직은 하지 마라. 최적화되지 않은 상태로도 완벽하게 깔끔한 해결책을 찾는 것이 먼저다. M. A. Jackson (1975) 효율만 쫒다가 득보다 실이 큰 경우를 경계하라는 뜻\n최적화를 하기 전에 항상 아래 두가지를 검토\n최적화 이전에 먼저 좋은 코드를 작성하기 코드를 작성하는 데 있어서 성능을 염두에 두는 것은 물론 중요합니다. 하지만 많은 경우 대부분의 코드는 성능상 영향이 크지 않고 실제로 병목이 되는 부분은 극히 일부분입니다. 좋은 코드를 최적화하기는 쉽지만, 섣부르게 최적화된 코드를 좋은 코드로 만드는 건 어렵습니다. 빠른 코드보다는 좋은 코드를 짜는 데에 먼저 집중하고 최적화는 그 다음에 생각해야 합니다.\n정량적으로 성능을 측정하면서 병목을 파악하기 정량화된 지표를 통해 실제로 병목이 되는 부분을 파악해야 합니다. 지엽적인 부분을 일일히 개선하는 마이크로 최적화는 많은 경우 100ms 를 99ms로 줄이는 것에 그칩니다. 마이크로 최적화 보다는 거시적인 관점에서 중요한 병목을 찾고 이를 구조적으로 해결하는 것이 중요합니다. 그리고 실질적으로 얼마나 빨라졌는지 정량적인 성과로 나타낼 수 있어야 합니다.\n검토해야할 부분 # 위 개선을 통해 배치 수행시간이 너무 빨라졌다. MSA 구조에서는 애플리케이션과 직접적으로 연동되는 DB와 로드밸런서 뿐만 아니라, 많은 모듈 및 유관부서들이 유기적으로 연결되어있기 때문에 영향 범위를 면밀히 검토해야한다. 데이터 변경이 발생하면 변경 사항이 큐를 통해서 유관 부서에 전달된다. 확인 지표\n개발 환경에서 테스트 당시 애플리케이션이 실행되는 서버의 CPU 및 I/O 지표 개발 환경에서 테스트 당시 DB CPU, 쿼리 지연 시간 등 지표 예상 트래픽을 산출, 현재 운영 환경에서의 피크 트래픽과 비교하여 문제가 없을지 검토 변경 사항을 전달하는 큐에서 지연이 발생해도 문제가 없을지 검토 너무 빨라도 문제 # 빠르게 동작하기 때문에, 유관 부서 트래픽 또한 예상 이상으로 인입되어 DB CPU가 높아짐 해결방안 : 의도적으로 지연 시간 설정\n@Bean(STEP_NAME) @JobScope public Step liveShopCloseCreateStep() { return stepBuilderFactory.get(STEP_NAME) .\u0026lt;Long, Long\u0026gt;chunk(CHUNK_SIZE) .reader(shopCloseScheduleReader(null)) .writer(liveShopCloseWriter(null, null, null)) .transactionManager(storeTransactionManager) .listener(new AfterChunkSleepListener(200)) .build(); } @Slf4j public class AfterChunkSleepListener implements ChunkListener { private final long sleepMillis; public AfterChunkSleepListener(long sleepMillis) { this.sleepMillis = sleepMillis; } @Override public void afterChunk(ChunkContext context) { try { log.info(\u0026#34;Chunk 실행 후 sleep {} millis. 현재 read Count : {}\u0026#34;, sleepMillis, context.getStepContext().getStepExecution().getReadCount()); TimeUnit.MILLISECONDS.sleep(sleepMillis); } catch (InterruptedException e) { log.error(\u0026#34;Thread sleep interrupted.\u0026#34;, e); } } @Override public void afterChunkError(ChunkContext context) { // 사용안함. } @Override public void beforeChunk(ChunkContext context) { // 사용안함. } } 지연 시간을 설정해도 배치는 390분 -\u0026gt; 30분 소요되는 결과를 얻음 최적화 작업 # 리스크를 확인하고 과제에 대한 우선순위를 조정하기 문제 상황을 분석하고 병목을 확인하기 I/O의 경우 최대한 한번에 여러건을 읽고 쓰도록 하여 효율성 높이기 도메인 로직을 검토하여 개선할 수 있는 부분이 있는지 살피기 유의미한 최적화인가? 정량적인 지표로 다시 검토하기 빨라도 문제일 수 있으니 최적화에 의한 영향 범위를 검토하고 운영 환경에서도 문제가 없을지 확인하기 참고 # 위 배치의 chunk size : 100으로 설정 References https://techblog.woowahan.com/13569/?ref=codenary https://www.codenary.co.kr/search?keyword=Java "},{"id":97,"href":"/docs/r2dbc/005_r2dbc_Metadata_mapping/","title":"005 R2dbc Metadata Mapping","section":"R2dbc","content":" 05. Metadata mapping # Entity 클래스에 어노테이션을 추가 # @Id: primary key에 해당하는 필드에 적용 @Table: entity class에 적용. Table 이름을 변경 가능 @Transient: 기본적으로 모든 필드는 mapping 대상. @Transient가 붙은 필드는 mapping 에서 제외 @Column: entity의 property 필드에 적용. @Column이 붙은 필드에 대해서는 convention 기반 대신 Column에 주어진 name으로 적용 @Version: 낙관적 잠금 (Optimistic Lock)에 이용. entity가 update 될때마다 자동으로 update @PersistenceConstructor: 특정 constructor에 대해서 Object creation할 때 사용하게끔 지정. constructor의 argument 이름에 따라서 mapping Metadata mapping 예제 # @Id : id 필드 지정 @Column(\u0026ldquo;name\u0026rdquo;) : fullName 필드에 row의 name 필드와 mapping @Transient : score 필드를 mapping 대상에서 제외 @PersistenceCreator : id, fullName, age, gender를 인자로 받는 생성자 설정 @Version : version 필드 설정 Metadata mapping 실행 # update 수행 후, version 1 증가 update된 결과에서 score를 20으로 변경 데이터베이스에는 반영되지 않지만 변경된 score를 두번째 update된 결과에 포함 Metadata mapping 실패 # version을 강제로 1 증가시킨다. (마치, 다른 쓰레드가 증가시킨것처럼) OptimisticLockingFailureException 발생 어떻게 property mapping에서 MySQL 타입을 java 타입으로 바꿀 수 있나? # property mapping 타입 변환 # Row로부터 get을 통해서 특정 Java 클래스로 변환 MySqlRow # Row의 구현체인 MySqlRow는 Codecs를 포함 해당 codecs를 이용하여 column의 정보, 값을 전달하고 type을 갖는 객체를 반환 FieldValue: column의 실제 값에 해당하는 ByteBuf 혹은 List 포함 MySqlColumnDescriptor: MySqlType, column name, nullable, size 등의 column meta 정보 포함 MySQL DefaultCodecs # MySQL defaultCodecs에는 기본적으로 codec들을 포함 각각의 codec은 canDecode, decode를 구현 canDecode : columnMetadata를 기반으로 target(특정 java 타입)으로 변경 가능한지 여부 반환 decode : 주어진 column의 value를 특정 타입의 객체로 반환 Codec 예제 - IntegerCodec # canPrimitiveDecode 메소드에서 주어진 column이 numeric 타입인지 체크 아래의 MySQL 타입인 경우 true DECIMAL TINYINT, TINYINT_UNSIGNED SMALLINT, SMALLINT_UNSIGNED INT, INT_UNSIGNED FLOAT DOUBLE BIGINT, BIGINT_UNSIGNED MEDIUMINT, MEDIUMINT_UNSIGNED YEAR decodeInt에서 MySQL 타입에 따라서 int로 변환 MySQL DefaultCodecs 지원 # 기본적으로 26개의 Codec 지원 Byte, Short, Integer, Long, BigInteger BigDecimal, Float, Double Boolean, BitSet ZonedDatetTime, LocalDateTime, Instant, OffsetDateTime LocalDate LocalTime, Duration, OffsetTime Year String Enum Set Clob, Blob ByteBuffer, ByteArray References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":98,"href":"/docs/r2dbc/006_r2dbcEntityOperations/","title":"006 R2dbc Entity Operations","section":"R2dbc","content":" 06. R2dbcEntityOperations # 구조 # R2dbcEntityTemplate가 R2dbcEntityOperations를 상속한다. R2dbcEntityOperations가 FluentR2dbcOperations를 상속한다. FluentR2dbcOperations는 여러 Operations를 상속한다. ReactiveSelectOperation : select query와 관련된 메서드 제공 ReactiveInsertOperation : insert query와 관련된 메서드 제공 ReactiveUpdateOperation : update query와 관련된 메서드 제공 ReactiveDeleteOperation : delete query와 관련된 메서드 제공 ReactiveSelectOperation # ReactiveSelectOperation의 select부터 시작 TerminatingSelect의 count, exists, first, one, all 등으로 종료 구조 select -\u0026gt; from -\u0026gt; as -\u0026gt; matching -\u0026gt; 실행 select -\u0026gt; from -\u0026gt; matching -\u0026gt; 실행 select -\u0026gt; as -\u0026gt; matching -\u0026gt; 실행 select -\u0026gt; matching -\u0026gt; 실행 select -\u0026gt; -\u0026gt; 실행 ReactiveSelectOperation 사용 # from : query를 실행할 table 이름을 전달 as : Entity를 전부 mapping 하지 않고 특정 필드만 mapping 하고 싶은 경우 Entity의 일부 프로퍼티만 담고 있는 subclass(혹은 인터페이스)를 넘겨서 projection projection이 제공되지 않는다면 Entity에 모든 필드를 mapping matching : query의 where문에 해당 matching을 생략하면 table 전체에 대한 요청을 보내는 것과 동일 실행 : 마지막으로 count, exists, first, one, all 등의 연산을 선택 count: 조건에 맞는 row의 개수 반환 exists: 조건에 맞는 row 존재 여부 반환 first: 조건에 맞는 첫 번째 row 반환 one: 조건에 맞는 하나의 row 반환. 하나가 넘으면 exception all: 조건에 맞는 모든 row 반환 ReactiveSelectOperation 실행 # ConnectionFactory를 이용하여 R2dbcEntityTemplate을 생성 Query와 Criteria를 이용해서 query 생성 PersonNameOnly class를 이용해서 name만 projection ReactiveInsertOperation # ReactiveInsertOperation의 insert 부터 시작하여 TerminatingInsert의 using으로 종료 구조 insert -\u0026gt; into -\u0026gt; using insert -\u0026gt; using ReactiveInsertOperation 사용 # into : query를 실행할 table 이름을 전달 using : insert query에 이용할 entity를 전달 주어진 entity를 OutboundRow로 변환 변환된 OutboundRow로 쿼리 실행 ReactiveInsertOperation 실행 # into를 통해서 insert할 table 명시 entity를 생성하여 using에 전달 ReactiveUpdateOperation # ReactiveUpdateOperation의 update부터 시작하여 TerminatingUpdate의 apply로 종료 구조 update -\u0026gt; inTable -\u0026gt; matching -\u0026gt; apply update -\u0026gt; inTable -\u0026gt; apply update -\u0026gt; matching -\u0026gt; apply update -\u0026gt; apply ReactiveUpdateOperation 사용 # inTable : query를 실행할 table 이름을 전달 matching : query의 where문에 해당 Query를 전달하여 query의 where에 들어갈 내용을 설정 matching을 생략하면 table 전체에 대한 요청을 보내는 것과 동일 apply : update를 수행 Entity가 아닌 Update 객체 전달 Update는 내부에 SqlIdentifier를 Key로 변경하려는 값을 Value로 갖는 Map 포함 from과 update static method로 Update 객체 생성 결과로 영향을 받은 row의 숫자 반환 ReactiveUpdateOperation 실행 # inTable을 통해서 update할 table 명시 matching으로 update 영향을 받는 row 제한 update를 생성하여 apply에 전달 ReactiveDeleteOperation # ReactiveDeleteOperation의 delete 부터 시작하여, TerminatingDelete의 all로 종료 구조 delete -\u0026gt; from -\u0026gt; matching -\u0026gt; all delete -\u0026gt; from -\u0026gt; all delete -\u0026gt; matching -\u0026gt; all delete -\u0026gt; all ReactiveDeleteOperation 사용 # all : delete를 수행 결과로 영향을 받은 row의 숫자 반환 ReactiveDeleteOperation 실행 # from을 통해서 delete할 table 명시 maching으로 delete 영향을 받는 row 제한 all을 실행하여 결과 출력 R2dbcEntityOperations # FluentR2dbcOperations에서 제공하는 조합 방식 대신 다양한 쿼리를 수행하는 단축 메소드 제공 Query 객체를 인자로 받는 경우 Entity를 직접 인자로 받는 경우\ninsert, update의 경우 주어진 entity로 값을 추가하거나 변경 delete는 id를 추출하여 해당 id를 갖는 row를 제거 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":99,"href":"/docs/r2dbc/007_r2dbcRepository/","title":"007 R2dbc Repository","section":"R2dbc","content":" 07. R2dbcRepository # R2dbcRepository 구조 # ReactiveSortingRepository와 ReactiveQueryByExampleExecutor를 상속한 interface인 SimpleR2dbcRepository에서 구현 R2dbcRepository 등록 # R2dbcRepositoriesAutoConfiguration가 활성화되어 있다면 SpringBootApplication 기준으로 자동으로 scan 혹은 EnableR2dbcRepositories를 통해서 repository scan 만약 여러 r2dbcEntityTemplate이 존재하거나 여러 데이터베이스를 사용하는 경우, basePackages, entityOperationRef 등을 통해서 다른 경로, 다른 entityTemplate 설정 가능 Repository # Spring data에서는 Repository interface를 제공 데이터에 접근하는 계층을 추상화하고 CRUD 작업, Entity mapping, SQL 쿼리 생성 등을 자동으로 수행 ReactiveCrudRepository # Spring data reactive에서는 CrudRepository의 Reactive 버전인 ReactiveCrudRepository 지원 entity의 CRUD에 집중 모든 결과값 그리고 일부 인자들이 Publisher 지원 ReactiveCrudRepository - save # saveAll은 @Transactional을 사용해서 각각의 save를 하나의 tx로 묶고 concatMap을 통해서 save를 순차적으로 수행 ReactiveCrudRepository - find # id 기반으로 하나 혹은 여러 개의 항목을 탐색하거나 존재 여부를 확인 모든 항목을 탐색하거나 모든 항목의 개수를 확인 ReactiveCrudRepository - delete # id 기반으로 하나 혹은 여러 개의 항목을 제거하거나 하나 혹은 여러 개의 entity를 기반으로 id를 추출하여 제거하거나, 모두 제거 ReactiveSortingRepository # ReactiveCrudRepository를 상속 spring data의 Sort를 기반으로 여러 항목 탐색 Sort 객체는 여러 Order 객체를 포함 이를 기반으로 query에 sort 옵션을 제공 SimpleR2dbcRepository # R2dbcRepository를 구현 R2dbcEntityOperations를 기반으로 SQL 쿼리를 실행하고 결과를 Entity로 mapping 기본적으로 모든 메소드에 @Transactional(readOnly = true) 적용 SimpleR2dbcRepository - save # new entity 확인 전략 @Id에 해당하는 필드를 확인. 만약 @Id 필드가 null이거나 0이라면 새로운 entity로 간주 SimpleR2dbcRepository - find # findById, existsById, count 모두 R2dbcEntityOperations에서 제공하는 단축 메소드 (selectOne, exists, count) 사용 SimpleR2dbcRepository - delete # R2dbcEntityOperations에서 제공하는 단축 메소드 (delete) 사용 R2dbcRepository의 한계 # R2dbcRepository는 기본적으로 CRUD를 수행할 수 있는 메소드를 제공 모두 혹은 id 기반으로 CRUD를 제공 특정 필드로 탐색을 하거나 상위 n개만 조회 등의 기능은 제공되지 않는다 join이나 집계와 관련된 함수들은 제공되지 않는다. 한계 해결 : query method 사용\nReferences 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":100,"href":"/docs/r2dbc/008_r2dbc_query_method/","title":"008 R2dbc Query Method","section":"R2dbc","content":" 08. R2dbc Query Method # 쿼리 메소드 (Query method) # R2dbcRepository를 상속한 repository interface에 메소드를 추가 메소드의 이름을 기반으로 Query 생성 조회, 삭제 지원 @Query 어노테이션을 사용해서 복잡한 쿼리나 update 문도 실행 가능 쿼리 메소드 - find # id 뿐만 아니라 다른 필드를 이용해서 조회 가능 first 등의 키워드를 사용해서 query에 limit 제공 가능 기존의 Entity 뿐만 아니라 Projection을 사용하여 일부 필드만 조회 가능 findFirstByNameOrderByAgeDesc name이 “taewoo”인 row들을 찾고 age 내림차순으로 sort 하여 limit을 1로 모든 field를 조회하여 PersonEntity class로 mapping 쿼리 메소드 - delete # 다른 필드를 이용해서 삭제 가능 여러 반환 타입 지원 Integer: 영향을 받은 row 수 반환 Boolean: 삭제되었는지 여부 반환 Void: 반환값보다는 completion이 중요한 경우 deleteByAgeGreaterThan age가 100 초과인 row를 찾고 삭제한 후 영향을 받은 row가 있다면 true를, 없다면 false를 반환 쿼리 메서드 시작 키워드 # find, read, get, query, search, stream find 쿼리를 실행하고 결과를 Publisher으로 반환 exists find exists 쿼리를 실행하고 결과를 Publisher으로 반환 count find count 쿼리를 실행하고 결과를 Publisher으로 반환 delete, remove delete 쿼리를 실행하고 Publisher 혹은 publisher로 삭제된 개수 반환 쿼리 메서드 제한 키워드 # First, Top 쿼리의 limit을 N으로 설정. find와 By 사이 어디에든 등장 가능 Distinct distinct 기능을 제공. find와 By 사이 어디에든 등장 가능 쿼리 메소드 predicate 키워드 # And: AND Or: OR After, IsAfter: AFTER Before, IsBefore: BEFORE Containing, IsContaining, Contains: CONTAINING Between, IsBetween: BETWEEN EndingWith, IsEndingWith, EndsWith: ENDING_WITH Exists: EXISTS False, IsFalse: FALSE GreaterThan, IsGreaterThan: GREATER_THAN GreaterThanEqual, IsGreaterThanEqual: GREATER_THAN_EQUALS In, IsIn: IN Is, Equals: IS IsEmpty, Empty: IS_EMPTY IsNotEmpty, NotEmpty: IS_NOT_EMPTY NotNull, IsNotNull: IS_NOT_NULL Null, IsNull: IS_NULL LessThan, IsLessThan: LESS_THAN LessThanEqual, IsLessThanEqual: LESS_THAN_EQUAL Like, IsLike: LIKE Near, IsNear: NEAR Not, IsNot: NOT NotIn, IsNotIn: NOT_IN NotLike, IsNotLike: NOT_LIKE Regex, MatchesRegex, Matches: REGEX StartingWith, IsStartingWith, StartsWith: STARTING_WITH True, IsTrue: TRUE Within, IsWithin: WITHIN IgnoreCase, IgnoringCase: 특정 필드에 적용. 비교하려는 대상 모두 UPPER로 만들어서 비교 AllIgnoreCase, AllIgnoringCase OrderBy: 주어진 property path와 direction에 따라서 쿼리에 Sort 제공 쿼리 메소드 반환 타입 # Mono Reactor에서 제공 0개 혹은 하나의 값을 반환하는 Publisher 만약 결과가 2개 이상이라면 IncorrectResultSizeDataAccessException 발생 Flux Reactor에서 제공 0개 이상의 값을 반환하는 Publisher 끝이 없는 수의 결과를 반환 가능 Single RxJava에서 제공 무조건 1개의 값을 반환하는 Publisher 만약 결과가 2개 이상이라면 IncorrectResultSizeDataAccessException 발생 만약 결과가 0개라면 NoSuchElementException 발생 Maybe RxJava에서 제공 0개 혹은 하나의 값을 반환하는 Publisher 만약 결과가 2개 이상이라면 IncorrectResultSizeDataAccessException 발생 Flowable RxJava에서 제공 0개 이상의 값을 반환하는 Publisher 끝이 없는 수의 결과를 반환 가능 쿼리 메소드 - @Query # query가 메소드 이름으로 전부 표현이 되지 않는 경우 쿼리 메소드 예약어에서 지원되지 않는 문법을 사용하는 경우 복잡한 query문을 사용하는 경우 쿼리 메소드 - @Query # inner join을 이용하여 person_role과 join하여 role이 특정값인 경우에만 조회 결과를 PersonEntity 형태로 반환 @Transactional # @Transactional를 사용하여 여러 query를 묶어서 진행 새로운 Entity를 만들어서 save하고 update 한 후, findAll을 통해서 모든 row 반환 로그 TransactionalOperator # transactional 메소드를 통해서 주어진 Flux 혹은 Mono를 transaction 안에서 실행 TransactionalOperator 사용 # flux를 바로 반환하지 않고 transactionalOperator의 transactional로 wrapping 하여 전달 혹은 execute를 통해서 TransactionCallback 형태로 실행 로그 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":101,"href":"/docs/kotlin/005_Kotlin_extractList_Method/","title":"005 Kotlin Extract List Method","section":"Kotlin","content":" Kotlin에서 리스트 추출하기 : subList, slice, take, drop # 리스트의 부분 리스트 구하기 : subList(), slice(), take() # Kotlin에서는 리스트의 부분 리스트를 구하는 메서드로 여러 메서드를 제공한다. 부분 리스트를 추출하는 기능을 하는 메서드에 대해 알아보자. 원본 리스트를 변경하지 않고 추출한 새로운 리스트를 반환하는 특징이 있다. 이 메서드들은 immutable한 리스트와 mutable한 리스트 모두에서 사용할 수 있다.\nsubList() # 리스트의 인덱스를 기반으로 리스트의 일부분을 추출하여, 새로운 리스트를 생성한다. Java의 subList와 유사하게 시작 인덱스부터 끝 인덱스까지 요소를 추출한다. 시작 인덱스는 포함, 끝 인덱스는 포함X val list = listOf(1, 2, 3, 4, 5) val sub = list.subList(1, 4) // [2, 3, 4] slice() # 리스트의 특정 범위를 추출하여 새로운 리스트를 생성한다. subList()와 다르게 IntRange를 받는다. IntRange에 해당하는 범위의 리스트를 추출하여 새로운 리스트로 생성한다. -\u0026gt; (1..4) 시작 인덱스, 끝 인덱스 모두 포함 val list = listOf(1, 2, 3, 4, 5) val sliced = list.slice(1..4) // [2, 3, 4, 5] subList() vs slice() # subList() : 추출된 부분 리스트는 원본 리스트의 View에 해당한다. 따라서 원본 리스트의 변경의 영향을 받는다. slice() : 추출된 부분 리스트는 원본 리스트와 완전히 독립된 리스트다. 따라서 영향을 받지 않는다. val mutableList = mutableListOf(1, 2, 3, 4, 5) val sub = mutableList.subList(1, 4) // [2, 3, 4] val sliced = mutableList.slice(1..3) // [2, 3, 4] // 원본 리스트 변경 mutableList[2] = 7 // subList() : 원본 리스트 변경 적용 println(sub) // [2, 7, 4] // slice() : 원본 리스트 변경 적용 X println(sliced) // [2, 3, 4] subList(), slice() 의 동작 방식 # val mutableList = mutableListOf(1, 2, 3, 4, 5) val sub = mutableList.subList(1, 4) // [2, 3, 4] val sliced = mutableList.slice(1..3) // [2, 3, 4] // mutableList[2] = 7 println(sub) // [2, 7, 4] println(sliced) // [2, 3, 4] mutableList, sub, sliced 각각 다른 해시코드를 가지고 있어, 다른 객체임을 확인할 수 있다. mutableList와 sliced는 ArrayList 클래스 타입이고, sub는 ArrayList 클래스의 내부 클래스인 SubList 클래스 타입이다. 각각의 리스트를 구성하는 요소들의 해시코드는 동일하다. 내부적으로 모두 동일한 객체를 가리킨다.\nval mutableList = mutableListOf(1, 2, 3, 4, 5) val sub = mutableList.subList(1, 4) // [2, 3, 4] val sliced = mutableList.slice(1..3) // [2, 3, 4] mutableList[2] = 7 println(sub) // [2, 7, 4] println(sliced) // [2, 3, 4] mutableList[2] = 7 코드를 수행시켜보자. mutablieList, sub : 7을 가리키는 새로운 객체(Integer@831)로 대체된다. sliced : 기존의 객체(Integer@837)로 유지된다. subList로 추출한 리스트는 원본 리스트의 참조를 따라가고, slice로 추출한 리스트의 경우 기존 참조를 유지한다. take(), drop() # take() : 리스트의 앞부분부터 지정한 개수만큼의 요소를 추출하여 새로운 리스트를 생성한다. drop() : 리스트의 앞부분부터 지정한 개수만큼의 요소를 뺀 새로운 리스트를 생성한다. take(), drop() 둘다 원본 리스트와 독립적으로 생성되며, 원본 리스트가 변경되어도 영향을 받지않는다. val list = listOf(1, 2, 3, 4, 5) val taken = list.take(3) // [1, 2, 3] val dropped = list.drop(3) // [4, 5] subList(slice) vs. take(drop) # subList와 slice : 리스트의 범위를 넘어가는 인덱스를 인자로 받을 경우 IndexOutOfBoundsException을 발생 take와 drop : 리스트의 범위를 넘어가는 인덱스를 인자로 받더라도 별도의 Exception을 발생X 리스트의 범위를 넘어가는 인덱스를 인자로 받을 경우 take : 원본 리스트를 그대로 가져온다. drop : 리스트의 모든 요소를 제외하여 부분 리스트를 생성한다. val list = listOf(1, 2, 3) // 길이가 3이고, maxIndex가 2인 list val sub = list.subList(0, 4) // IndexOutOfBoundsException val sliced = list.slice(0..3) // IndexOutOfBoundsException val taken = list.take(4) // [1, 2, 3] val dropped = list.drop(4) // [] 정리 # subList 원본 리스트와 부분 리스트 간의 관계를 유지하고자 할 때 0번 인덱스부터 자르는 것이 아닌 중간부터 자를 때 (끝까지 자르는 것이 아닌 도중에 자를 때) ex: subList(1, 4) slice 원본 리스트와 관계없는 독립적인 리스트를 생성해야 할 때 0번 인덱스부터 자르는 것이 아닌 중간부터 자를 때 (끝까지 자르는 것이 아닌 도중에 자를 때) ex: slice(1..4) take, drop slice를 대체할 수 있으면 사용하는 것이 좋음 slice(0, 3) 대신 take(3) 사용 → IndexOutOfBoundsException으로부터 안전 subList(0, 4) 대신 take(3)을 사용할 경우, 원본 리스트 간의 동기화 문제가 발생할 수 있으므로 이 점 참고하여 사용 IndexOutOfBoundsException을 발생시켜야 하는 상황에서는 다른 메서드 사용 References https://medium.com/@limgyumin/%EC%BD%94%ED%8B%80%EB%A6%B0-%EC%9D%98-apply-with-let-also-run-%EC%9D%80-%EC%96%B8%EC%A0%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94%EA%B0%80-4a517292df29 "},{"id":102,"href":"/docs/r2dbc/003_r2dbcEntityTemplate/","title":"003 R2dbc Entity Template","section":"R2dbc","content":" 03. R2dbcEntityTemplate # Entity # 데이터베이스에서 하나의 Row와 매칭되는 클래스 R2dbcEntityTemplate, R2dbcRepository 등은 데이터베이스에 요청을 보내고 그 결과를 Entity 형태로 반환 Table, Row, Column에 필요한 데이터베이스 metadat를 어노테이션으로 제공 R2dbcEntityTemplate # Spring data r2dbc의 추상화 클래스 메서드 체이닝을 통해서 쿼리를 수행하고 결과를 entity 객체로 받을 수 있다. R2dbcEntityOperations를 구현 public class R2dbcEntityTemplate implements R2dbcEntityOperations, BeanFactoryAware, ApplicationContextAware { private final DatabaseClient databaseClient; ... } R2dbcEntityTemplate 생성 # ConnectionFactory를 제공하거나 R2dbcDialect, R2dbcConverter를 제공하여 constructor로 생성 가능 R2dbcDialect : R2dbc 버전의 Dialect 확장 R2dbcEntityTemplate 빈 등록 # R2dbcDataAutoConfiguration 위 클래스를 통해서 DatabaseClient, R2dbcDialect, MappingR2dbcConverter를 주입 @AutoConfiguration(after = R2dbcAutoConfiguration.class) @ConditionalOnClass({ DatabaseClient.class, R2dbcEntityTemplate.class }) @ConditionalOnSingleCandidate(DatabaseClient.class) public class R2dbcDataAutoConfiguration { private final DatabaseClient databaseClient; private final R2dbcDialect dialect; public R2dbcDataAutoConfiguration(DatabaseClient databaseClient) { this.databaseClient = databaseClient; this.dialect = DialectResolver.getDialect(this.databaseClient.getConnectionFactory()); } @Bean @ConditionalOnMissingBean public R2dbcEntityTemplate r2dbcEntityTemplate(R2dbcConverter r2dbcConverter) { return new R2dbcEntityTemplate(this.databaseClient, this.dialect, r2dbcConverter); } @Bean @ConditionalOnMissingBean public R2dbcMappingContext r2dbcMappingContext(ObjectProvider\u0026lt;NamingStrategy\u0026gt; namingStrategy, R2dbcCustomConversions r2dbcCustomConversions) { R2dbcMappingContext relationalMappingContext = new R2dbcMappingContext( namingStrategy.getIfAvailable(() -\u0026gt; NamingStrategy.INSTANCE)); relationalMappingContext.setSimpleTypeHolder(r2dbcCustomConversions.getSimpleTypeHolder()); return relationalMappingContext; } @Bean @ConditionalOnMissingBean public MappingR2dbcConverter r2dbcConverter(R2dbcMappingContext mappingContext, R2dbcCustomConversions r2dbcCustomConversions) { return new MappingR2dbcConverter(mappingContext, r2dbcCustomConversions); } ... } R2dbcEntityOperations # DatabaseClient와 R2dbcConverter를 제공 DatabaseClient ConnectionFactory를 wrapping하여 결과를 Map이나 Integer로 반환 R2dbcConverter 주어진 Row를 Entity로 만드는 converter R2dbcEntityTemplate에서는 이 DatabaseClient, R2dbcConverter로 쿼리를 실행하고 결과를 entity로 반환한다. public interface R2dbcEntityOperations extends FluentR2dbcOperations { DatabaseClient getDatabaseClient(); ... R2dbcConverter getConverter(); ... } DatabaseClient # 내부에 포함된 ConnectionFactory에 접근 가능 sql 메서드를 통해서 GenericExecuteSpec을 반환한다. bind를 통해서 parameter를 sql에 추가 fetch를 통해서 FetchSpec을 반환 public interface DatabaseClient extends ConnectionAccessor { ConnectionFactory getConnectionFactory(); GenericExecuteSpec sql(String sql); ... interface GenericExecuteSpec { GenericExecuteSpec bind(int index, Object value); GenericExecuteSpec bindNull(int index, Class\u0026lt;?\u0026gt; type); GenericExecuteSpec bind(String name, Object value); GenericExecuteSpec bindNull(String name, Class\u0026lt;?\u0026gt; type); default GenericExecuteSpec filter(Function\u0026lt;? super Statement, ? extends Statement\u0026gt; filterFunction) { Assert.notNull(filterFunction, \u0026#34;Filter function must not be null\u0026#34;); } GenericExecuteSpec filter(StatementFilterFunction filter); default \u0026lt;R\u0026gt; RowsFetchSpec\u0026lt;R\u0026gt; map(Function\u0026lt;Row, R\u0026gt; mappingFunction) { Assert.notNull(mappingFunction, \u0026#34;Mapping function must not be null\u0026#34;); return map((row, rowMetadata) -\u0026gt; mappingFunction.apply(row)); } \u0026lt;R\u0026gt; RowsFetchSpec\u0026lt;R\u0026gt; map(BiFunction\u0026lt;Row, RowMetadata, R\u0026gt; mappingFunction); FetchSpec\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; fetch(); Mono\u0026lt;Void\u0026gt; then(); } } FetchSpec # RowsFetchSpec, UpdatedRowFetchSpec을 상속 RowsFetchSpec : one, first, all 메서드 제공 one : 없거나 혹은 하나의 결과를 Mono로 제공 (그 이상일 경우 에러 반환) first : 첫번째 결과를 Mono로 제공 all : 모든 결과를 Flux로 제공 UpdatedRowFetchSpec : 쿼리의 영향을 받은 row 수를 Mono로 제공 DatabaseClient 실행 # sql을 실행하여 GenericExecuteSpec을 반환 fetch() : FetchSpec 반환 예제코드를 보니, 여전히 직접 mapping 을 해줘야한다는 단점이 존재한다. R2dbcConverter # EntityReader, EntityWriter를 상속 구현체로 MappingR2dbcConverter가 존재 public interface R2dbcConverter extends EntityReader\u0026lt;Object, Row\u0026gt;, EntityWriter\u0026lt;Object, OutboundRow\u0026gt;, RelationalConverter { ... } public class MappingR2dbcConverter extends BasicRelationalConverter implements R2dbcConverter { /** * Creates a new {@link MappingR2dbcConverter} given {@link MappingContext}. * * @param context must not be {@literal null}. */ public MappingR2dbcConverter( MappingContext\u0026lt;? extends RelationalPersistentEntity\u0026lt;?\u0026gt;, ? extends RelationalPersistentProperty\u0026gt; context) { super(context, new R2dbcCustomConversions(R2dbcCustomConversions.STORE_CONVERSIONS, Collections.emptyList())); } ... } 다양한 전략을 통해서 Object를 데이터베이스의 row로, 데이터베이스의 row를 Object로 변환 custom converter로 mapping Spring data의 object로 mapping convention 기반의 mapping metadata 기반의 mapping Custom Converter Mapping # Configuration을 통해서 converter들을 등록 read, write 각각의 Converter이 필요 (2개) ReadConverter # Row를 source로 Entity를 target으로 하는 converter WriteConverter # Entity를 source로 Row를 target으로 하는 converter key : column 이름 value : parameter.from을 이용해서 entity의 속성 전달 OutboudRow에 값을 추가하고, DefaultDatabaseClient에서 이를 이용해서 SQL 생성 CustomConverter 등록 # AbstractR2dbcConfiguration를 상속하는 Configuration 생성 AbstractR2dbcConfiguration의 getCustomCOnverters에 custom converter들을 List 형태로 제공 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":103,"href":"/docs/r2dbc/004_r2dbc_object_mapping/","title":"004 R2dbc Object Mapping","section":"R2dbc","content":" 04. Object mapping # Spring data의 object mapping # 만약 지원하는 converter이 없다면 MappingR2dbcConverter는 다음 과정을 거쳐서 Row를 Entity로 변환한다. Object cretion : Row의 column들로 Object 생성 Property population : direct set, setter, with..메서드 등을 이용해서 Row의 Column을 Objec에 주입 Object creation # Object creation 테스트 # R2dbcEntityTemplate의 select 호출시, R2dbcConverter를 사용하기 때문에 이를 이용해서 selet에 class를 넘기는 방식으로 테스트 PersistenceCreator constructor # @PersistenceCreator 을 갖는 constructor가 존재한다면 해당 constructor를 사용 여러개가 존재한다면 가장 마지막 @PersistenceCreator가 붙은 constructor를 사용 NoArgsConstructor, AllArgsConstructor 전부 패스 NoArgs constructor # @PersistenceCreator 을 갖는 constructor가 없는 경우 No-args constructor가 존재한다면 해당 constructor를 사용 다른 constructor 전부 패스 하나의 constructor # 오직 하나의 constructor이 존재한다면 해당 constructor 사용 2개 이상의 constructor가 있다면? # @PersistenceCreator를 갖는 constructor도 없고, No-args constructor도 없다면, Exception 발생 생성자에 전체 필드가 없었어도, 결과를 보면 필드에 값이 들어가있었다. -\u0026gt; Property popultaion 동작\nProperty population # r2dbc에서는 property가 mutable할때만 property population 적용 id, name 필드를 채움 property 순회하여 mutable한 경우에만 reflection을 사용해서 값 주입 (남은 필드들도 값이 생김) Object mapping 최적화 # 객체를 가능한한 Immutable하게 모든 property를 인자로 갖는 All-args 제공 property polulation이 발생하지 않게되어 성능 향상 코드 중복 방지를 위해 lombok 사용 Naming strategy # 별도의 @Table, @Column 어노테이션이 없다면, naming strategy에 맞춰서 클래스명, 변수명을 변경해서 table과 column에 mapping NamingStrategy interface를 구현하여 bean으로 등록하면 일괄 변경가능 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":104,"href":"/docs/r2dbc/002_r2dbc_mysql/","title":"002 R2dbc Mysql","section":"R2dbc","content":" 02. R2dbc MySQL # R2dbc MysqlConnection # Connection을 구현한 MysqlConnection ConnectionMetadata를 구현한 MysqlConnectionMetadata Statement를 구현한 MysqlStatement MysqlConnectionFactory # Mono 형태로 포함 MysqlConnectionConfiguration을 인자로 받아서 MysqlConnectionFactory를 생성 MysqlConnectionFactory로 MysqlConnection 생성 MysqlConnection으로 MysqlStatement를 생성 MysqlConnection으로 transaction을 start, rollback, commit MysqlConnectionConfiguration # MYSQL 연결의 설정을 포함하는 객체 Builder 패턴 host, port, database, username 등 기본 설정 제공 serverZoneId 설정 MysqlConnection 생성 # Sql 준비 # Sql 실행 # ConnectionFactory의 create()를 통해서 connection 접근 connection의 createStatement를 통해서 sql 준비 result의 map으로 row에 접근하고 Person으로 변환 thenMany() chaining : 순차적으로 실행 selectPeople 결과를 아래로 전달 result의 map으로 row에 접근하고 Person으로 변환 MysqlConnection의 한계 # SQL 쿼리를 명시적으로 전달 반환된 결과를 수동으로 파싱 별도의 mapper를 만들어야하고 확장성이 떨어짐 Transaction 실행 # connection의 beginTransaction과 commitTransaction으로 transaction 시작과 commit 수행 롤백 수행 : conn.rollbackTransaction() References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":105,"href":"/docs/kotlin/004_Kotlin_Scoping_Functions/","title":"004 Kotlin Scoping Functions","section":"Kotlin","content":" Kotlin Scoping Functions apply vs. with, let, also, and run # apply, with, let, also, run # Kotlin의 Receiver # 객체 외부의 람다 코드 블록을 마치 해당 객체 내부에서 사용하는 것 처럼 작성할 수 있게 해주는 장치\nblock : T.() -\u0026gt; R 위 람다 블록은 객체 T를 receiver로 이용하여 객체 R을 반환한다.\nreceiver : 객체 T receiver를 사용하는 람다 : lambda with receiver block : (T) -\u0026gt; R 위의 경우 객체 T를 리시버가 아니라 람다 파라미터로 받는다.\n범위 지정 함수란? # 수신객체 수신객체 지정 람다 (lambda with receiver) 람다 식 내에서 수신 객체의 멤버에 직접 접근할 수 있게 하는 기능 with # inline fun \u0026lt;T, R\u0026gt; with(receiver: T, block: T.() -\u0026gt; R): R { return receiver.block() } 수신객체 : receiver T 수신 객체 지정 람다 : block Before\nclass Person { var name: String? = null var age: Int? = null } val person: Person = getPerson() print(person.name) print(person.age) After\nval person: Person = getPerson() with(person) { print(name) print(age) } also # inline fun \u0026lt;T\u0026gt; T.also(block: (T) -\u0026gt; Unit): T { block(this) return this } T 의 확장함수로 수신 객체가 암시적으로 제공 수신 객체 지정 람다 : 매개변수 T 로 코드 블록 내에 명시적으로 전달 Before\nclass Person { var name: String? = null var age: Int? = null } val person: Person = getPerson() print(person.name) print(person.age) After\nval person: Person = getPerson().also { print(it.name) print(it.age) } with, also, apply, let, run 차이점 # 호출시에 수신 객체가 매개 변수로 명시적으로 전달되거나 수신 객체의 확장 함수로 암시적 수신 객체 로 전달 수신 객체 지정 람다 에 전달되는 수신 객체가 명시적 매개 변수 로 전달 되거나 수신 객체의 확장 함수로 암시적 수신 객체로 코드 블록 내부로 전달 범위 지정 함수의 결과로 수신 객체를 그대로 반환하거나 수신 객체 지정 람다 의 실행 결과를 반환 inline fun \u0026lt;T, R\u0026gt; with(receiver: T, block: T.() -\u0026gt; R): R { return receiver.block() } inline fun \u0026lt;T\u0026gt; T.also(block: (T) -\u0026gt; Unit): T { block(this) return this } inline fun \u0026lt;T\u0026gt; T.apply(block: T.() -\u0026gt; Unit): T { block() return this } inline fun \u0026lt;T, R\u0026gt; T.let(block: (T) -\u0026gt; R): R { return block(this) } inline fun \u0026lt;T, R\u0026gt; T.run(block: T.() -\u0026gt; R): R { return block() } apply 사용 규칙 # 수신 객체 람다 내부에서 수신 객체의 함수를 사용하지 않고 수신 객체 자신을 다시 반환 하려는 경우 Before\nval clark = Person() clark.name = \u0026#34;Clark\u0026#34; clark.age = 18 After\nval peter = Person().apply { // apply 의 블록 에서는 오직 프로퍼티 만 사용합니다! name = \u0026#34;Peter\u0026#34; age = 18 } also 사용 규칙 # 수신 객체 람다가 전달된 수신 객체를 전혀 사용 하지 않거나 수신 객체의 속성을 변경하지 않고 사용하는 경우 수신 객체를 반환 하므로 블록 함수가 다른 값을 반환 해야하는 경우 사용 불가능 inline fun \u0026lt;T\u0026gt; T.also(block: (T) -\u0026gt; Unit): T { block(this) return this } Before\nclass Book(val author: Person) { init { requireNotNull(author.age) print(author.name) } } After\nclass Book(author: Person) { val author = author.also { requireNotNull(it.age) print(it.name) } } apply 와 also : 리시버와 파라미터의 차이 # public inline fun \u0026lt;T\u0026gt; T.also(block: (T) -\u0026gt; Unit): T { block(this) return this } public inline fun \u0026lt;T\u0026gt; T.apply(block: T.() -\u0026gt; Unit): T { block() return this } also : 객체를 람다 파라미터로 받는다. apply : 객체를 리시버로 받는다. class person { var name = \u0026#34;kotlin\u0026#34; private val id = \u0026#34;1541\u0026#34; } person.also { println(\u0026#34;my name is ${it.name}\u0026#34;) } person.apply { println(\u0026#34;my name is $name\u0026#34;) } also : it을 사용한다. apply : this를 사용한다. it vs this # 내가 작성하고자 하는 코드의 의미(semantics) 에 따라 also를 쓸지 apply를 쓸지 결정하는 것이다.\nit also는 객체를 람다 아규먼트로 받기 때문에 객체에 접근할 때 it(혹은 내가 정의한 다른 이름)을 사용 이는 코드가 객체 외부에서 해당 객체에 접근한다는 인상을 강하게 준다. 객체를 외부에서 접근하는 느낌을 주기 때문에 해당 객체와 더불어(혹은 이용해서) 어떠한 행위를 수행하고자 할 때 쓰인다. person.also { println(\u0026#34;my name is ${it.steven}\u0026#34;) } this apply 는 객체를 람다 리시버로 받기 때문에 객체에 접근할 때 this(혹은 생략)을 사용 코드가 해당 객체의 외부가 아니라 객체 내부에 있는듯한 인상을 준다. apply코드 블록이 객체 내부에 있는 듯한 느낌을 주기 때문에 주로 객체를 초기화 하는 코드 혹은 객체의 상태를 변경하는 코드에 많이 사용된다. person.apply { name = \u0026#34;steven\u0026#34; age = 21 } let 사용 규칙 # 지정된 값이 null 이 아닌 경우에 코드를 실행해야 하는 경우 Nullable 객체를 다른 Nullable 객체로 변환하는 경우 단일 지역 변수의 범위를 제한 하는 경우 inline fun \u0026lt;T, R\u0026gt; T.let(block: (T) -\u0026gt; R): R { return block(this) } Before\nval person: Person? = getPromotablePerson() if (person != null) { promote(person) } val driver: Person? = getDriver() val driversLicence: Licence? = if (driver == null) null else licenceService.getDriversLicence(it) val person: Person = getPerson() val personDao: PersonDao = getPersonDao() personDao.insert(person) After\ngetNullablePerson()?.let { // null 이 아닐때만 실행됩니다. promote(it) } val driversLicence: Licence? = getNullablePerson()?.let { // nullable personal객체를 nullable driversLicence 객체로 변경합니다. licenceService.getDriversLicence(it) } val person: Person = getPerson() getPersonDao().let { dao -\u0026gt; // 변수 dao 의 범위는 이 블록 안 으로 제한 됩니다. dao.insert(person) } with 사용 규칙 # Non-nullable (Null 이 될수 없는) 수신 객체 이고 결과가 필요하지 않은 경우에만 with 를 사용 inline fun \u0026lt;T, R\u0026gt; with(receiver: T, block: T.() -\u0026gt; R): R { return receiver.block() } Before\nval person: Person = getPerson() print(person.name) print(person.age) After\nval person: Person = getPerson() with(person) { print(name) print(age) } run 사용 규칙 # 어떤 값을 계산할 필요가 있거나 여러개의 지역 변수의 범위를 제한하려면 run 을 사용 매개 변수로 전달된 명시적 수신객체 를 암시적 수신 객체로 변환 할때 run ()을 사용 inline fun \u0026lt;T, R\u0026gt; T.run(block: T.() -\u0026gt; R): R { return block() } Before\nval person: Person = getPerson() val personDao: PersonDao = getPersonDao() val inserted: Boolean = personDao.insert(person) fun printAge(person: Person) = { print(person.age) } After\nval inserted: Boolean = run { // person 과 personDao 의 범위를 제한 합니다. val person: Person = getPerson() val personDao: PersonDao = getPersonDao() // 수행 결과를 반환 합니다. personDao.insert(person) } fun printAge(person: Person) = person.run { // person 을 수신객체로 변환하여 age 값을 사용합니다. print(age) } 여러 범위 지정 함수 결합 # 하나의 코드 블록 내에서 여러 범위 지정 함수를 중첩하지 않는 것이 좋다. 수신객체 지정 람다 에 수신 객체가 암시적으로 전달되는 apply, run, with 는 중첩하지 말라. 이 함수들은 수신 객체를 this 또는 생략하여 사용하며, 수신객체의 이름을 다르게 지정할수 없기 때문에 중첩될 경우 혼동 하기 쉽다. also 와 let 을 중첩 해야만 할때는 암시적 수신 객체 를 가르키는 매개 변수 인 it 을 사용하지 말고, 명시적인 이름을 제공해서 코드상의 이름이 혼동되지 않도록 하자.\nprivate fun insert(user: User) = SqlBuilder().apply { append(\u0026#34;INSERT INTO user (email, name, age) VALUES \u0026#34;) append(\u0026#34;(?\u0026#34;, user.email) append(\u0026#34;,?\u0026#34;, user.name) append(\u0026#34;,?)\u0026#34;, user.age) }.also { print(\u0026#34;Executing SQL update: $it.\u0026#34;) }.run { jdbc.update(this) \u0026gt; 0 } References https://medium.com/@limgyumin/%EC%BD%94%ED%8B%80%EB%A6%B0-%EC%9D%98-apply-with-let-also-run-%EC%9D%80-%EC%96%B8%EC%A0%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94%EA%B0%80-4a517292df29 https://jaeyeong951.medium.com/kotlin-lambda-with-receiver-5c2cccd8265a "},{"id":106,"href":"/docs/r2dbc/001_r2dbc_intro/","title":"001 R2dbc Intro","section":"R2dbc","content":" R2dbc 소개 # 왜 JDBC, JPA는 non-blocking을 지원할 수 없을까? # JDBC : 동기 blocking I/O 기반으로 설계 Socket에 대한 연결과 쿼리 실행 모두 동기 blocking으로 동작 JPA 또한 JDBC 기반 -\u0026gt; 비동기 non-blocking 지원 불가 그래서 결국, 비동기 non-blocing 기반의 API, 드라이버를 새로 만든다. R2dbc # Reactive Relational Database Connectivity 비동기 non-blocking 관계형 데이터베이스 드라이버 Reactive streams 스펙을 제공하며 Project reactor 기반으로 구현 R2dbc 지원 데이터베이스 # 공식지원 r2dbc-h2 r2dbc-mssql r2dbc-pool : Reactor pool로 커넥션 풀 제공 벤더 지원 oracle-r2dbc r2dbc-mariadb r2dbc-postgresql 커뮤니티 지원 r2dbc-mysql mirromutth 에서 2020년 5월부터 업데이트 X asyncer-io에서 RELEASE 지원 R2dbc MySQL 구조 # r2dbc-spi와 Reactor Netty 기반 Reactor Netty를 이용하여 r2dbc-spi 스펙을 구현 Reactor Netty client로 성능과 확장성 모두 제공 r2dbc-spi 스펙을 구현하여 여러 데이터베이스 시스템과 호환 R2dbc SPI # r2dbc Service Provider Interface SPI에서 제공하는 인터페이스를 구현해야한다. db connection 스펙, Exception 등의 스펙, Result, Row 등 result 스펙, Statement(요청) 스펙 R2dbc SPI Connection # 데이터베이스에 대한 연결을 가리킨다.\nClosable을 구현하여 close 메서드로 connection을 닫을 수 있다.\nConnectionMetadata를 제공\ndatabase의 version과 productName을 제공 createStatement를 통해서 sql을 넘기고 Statement를 생성한다. transaction 관련된 기능을 제공\ntransaction을 시작\nTransactionDefinition로 고립 수준, 읽기 전용 여부, 이름, lockWaitTime 등을 설정 transaction savepoint를 생성\ntransaction 중간에 savepoint를 만들고 rollback 가능 transaction을 commit하거나 rollback R2dbc SPI ConnectionFactory # connection을 생성하는 factory ConnectionFactoryMetadat를 통해서 ConnectionFactory의 정보를 제공 ConnectionFactoryMetadata는 name을 제공 R2dbc SPI Statement # Statement는 Connection으로부터 createStatement을 통해서 생성 bind : sql에 parameter를 bind add : 이전까지 진행한 binding을 저장하고 새로운 binding을 생성 execute : 생성된 binding 수만큼 쿼리를 실행하고 publisher로 반환 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지_ "},{"id":107,"href":"/docs/kotlin/003_Kotlin_basic/","title":"003 Kotlin Basic","section":"Kotlin","content":" 코틀린 문법 한번에 정리하기 # 주석 정리 Variable # // top-level var x = 5 fun main() { x+= 1 println(x) val a : Int = 1 val b = 1 val c : Int c = 3 val d : Int d = 123 //val(value) : 불변(Immutable) //var(variable) : 가변(Mutable) var e : String = \u0026#34;Hello\u0026#34; e = \u0026#34;World\u0026#34; var f = 123 // f = \u0026#34;hi\u0026#34; // 컴파일 오류 타입은 변경이 불가 } Function # // 기본적인 함수 선언 스타일 fun sum(a: Int, b: Int) : Int { return a + b } // 표현식 스타일 fun sum2(a: Int, b: Int) : Int = a + b // 표현식 \u0026amp; 반환타입 생략 fun sum3(a: Int, b: Int) = a + b // 몸통이 있는 함수는 반환 타입을 제거하면 컴파일 오류 fun sum4(a: Int, b: Int) : Int { return a + b } // 반환타입이 없는 함수는 Unit을 반환한다 fun printSum(a: Int, b: Int) : Unit { println(\u0026#34;$a + $b = ${a + b}\u0026#34;) } // 디폴트 파라미터 fun greeting(message: String = \u0026#34;안녕하세요!!\u0026#34;) { println(message) } // //fun main( ) { // greeting() // greeting(\u0026#34;HI!!!\u0026#34;) //} fun log(level: String = \u0026#34;INFO\u0026#34;, message: String) { println(\u0026#34;[$level]$message\u0026#34;) } fun main( ) { log(message = \u0026#34;인포 로그\u0026#34;) log(level = \u0026#34;DEBUG\u0026#34;, \u0026#34;디버그 로그\u0026#34;) log(\u0026#34;WARN\u0026#34;, \u0026#34;워닝 로그\u0026#34;) log(level = \u0026#34;ERROR\u0026#34;, message = \u0026#34;에러 로그\u0026#34;) } For # fun main() { // 범위 연산자 .. 를 사용해 for loop 돌리기 for (i in 0..3) { println(i) } // until 을 사용해 반복한다 // 뒤에 온 숫자는 포함하지 않는다 for (i in 0 until 3) { println(i) } // step 에 들어온 값 만큼 증가시킨다 for ( i in 0..6 step 2) { println(i) } // downTo를 사용해 반복하면서 값을 감소시킨다 for (i in 3 downTo 1) { println(i) } // 전달받은 배열을 반복 val numbers = arrayOf(1,2,3) for (i in numbers) { println(i) } } If # fun main() { //if..else 사용 val job = \u0026#34;Software Developer\u0026#34; if (job == \u0026#34;Software Developer\u0026#34;) { println(\u0026#34;개발자\u0026#34;) } else { println(\u0026#34;개발자아님\u0026#34;) } //코틀린의 if...else는 표현식이다 val age : Int = 10 val str = if (age \u0026gt; 10) { \u0026#34;성인\u0026#34; } else { \u0026#34;아이\u0026#34; } //코틀린은 삼항 연산자가 없다. if..else가 표현식이므로 불필요하다 val a = 1 val b = 2 val c = if (b \u0026gt; a) b else a } When # fun main() { // 자바 코드를 코틀린의 when식으로 변환한 코드 val day = 2 val result = when (day) { 1 -\u0026gt; \u0026#34;월요일\u0026#34; 2 -\u0026gt; \u0026#34;화요일\u0026#34; 3 -\u0026gt; \u0026#34;수요일\u0026#34; 4 -\u0026gt; \u0026#34;목요일\u0026#34; else -\u0026gt; \u0026#34;기타\u0026#34; } println(result) // else를 생략할 수 있다 when(getColor()) { Color.RED -\u0026gt; print(\u0026#34;red\u0026#34;) Color.GREEN -\u0026gt; println(\u0026#34;green\u0026#34;) else -\u0026gt; println(\u0026#34;blue\u0026#34;) } // 여러개의 조건을 콤마로 구분해 한줄에서 정의할 수 있다 when (getNumber()) { 0, 1 -\u0026gt; print(\u0026#34;0 또는 1\u0026#34;) else -\u0026gt; print(\u0026#34;0 또는 1이 아님\u0026#34;) } } enum class Color { RED, GREEN, BLUE } fun getColor() = Color.RED fun getNumber() = 2 while # fun main() { // 자바의 while문과 동일 // 조건을 확인하고 참이면 코드 블록을 실행한 후 다시 조건을 확인 var x = 5 while (x \u0026gt; 0) { println(x) x-- } } Exception # fun main() { try { throw Exception() } catch (e: Exception) { println(\u0026#34;에러 발생!\u0026#34;) } finally { println(\u0026#34;finally 실행!\u0026#34;) } val a = try { \u0026#34;1234\u0026#34;.toInt() } catch (e: Exception) { println(\u0026#34;예외 발생 !\u0026#34;) } println(a) //throw Exception(\u0026#34;예외 발생!\u0026#34;) val b: String? = null val c: String = b ?: failFast(\u0026#34;a is null\u0026#34;) println(c.length) } fun failFast(message: String): Nothing { throw IllegalArgumentException(message) } Null Safety # fun getNullStr(): String? = null fun getLengthIfNotNull(str: String?) = str?.length ?: 0 fun main() { val nullableStr = getNullStr() val nullableStrLength = nullableStr?.length ?: \u0026#34;null인 경우 반환\u0026#34;.length println(nullableStrLength) val length = getLengthIfNotNull(null) println(length) //throw NullPointerException() // val c: String? = null // val d = c!!.length // println(Java_NullSafety.getNullStr()?.length ?: 0) } Class Property # class Coffee( var name: String = \u0026#34;\u0026#34;, var price: Int = 0, var iced: Boolean = false, ) { val brand: String get() { return \u0026#34;스타벅스\u0026#34; } var quantity : Int = 0 set(value) { if (value \u0026gt; 0) { // 수량이 0 이상인 경우에만 할당 field = value } } } class EmptyClass fun main() { val coffee = Coffee() coffee.name = \u0026#34;아이스 아메리카노\u0026#34; coffee.price = 2000 coffee.quantity = 1 coffee.iced = true if (coffee.iced) { println(\u0026#34;아이스 커피\u0026#34;) } println(\u0026#34;${coffee.brand} ${coffee.name} 가격은 ${coffee.price} 수량은 ${coffee.quantity}\u0026#34;) } Inheritance # open class Dog { open var age: Int = 0 open fun bark() { // 반드시 오버라이드해야하는건 아니다. open fun일 경우 반드시 body를 구현해야한다. println(\u0026#34;멍멍\u0026#34;) } } open class Bulldog(final override var age: Int = 0) : Dog() { final override fun bark() { super.bark() } } abstract class Developer { abstract var age: Int abstract fun code(language: String) } class BackendDeveloper(override var age : Int) : Developer() { override fun code(language: String) { println(\u0026#34;I code with $language\u0026#34;) } } fun main() { val backendDeveloper = BackendDeveloper(age = 20) println(backendDeveloper.age) backendDeveloper.code(\u0026#34;Kotlin\u0026#34;) val dog = Bulldog(age = 2) println(dog.age) dog.bark() } Interface # class Product(val name: String, val price: Int) interface Wheel { fun roll() } interface Cart : Wheel { var coin: Int val weight: String get() = \u0026#34;20KG\u0026#34; fun add(product: Product) fun rent() { if (coin \u0026gt; 0) { println(\u0026#34;카트를 대여합니다\u0026#34;) } } override fun roll() { println(\u0026#34;카트가 굴러갑니다\u0026#34;) } fun printId() = println(\u0026#34;1234\u0026#34;) } interface Order { fun add(product: Product) { println(\u0026#34;${product.name} 주문이 완료되었습니다\u0026#34;) } fun printId() = println(\u0026#34;5678\u0026#34;) } class MyCart(override var coin: Int) : Cart, Order { override fun add(product: Product) { if (coin \u0026lt;= 0) println(\u0026#34;코인을 넣어주세요\u0026#34;) else println(\u0026#34;${product.name}이(가) 카트에 추가됐습니다\u0026#34;) // 주문하기 super\u0026lt;Order\u0026gt;.add(product) } override fun printId() { super\u0026lt;Cart\u0026gt;.printId() super\u0026lt;Order\u0026gt;.printId() } } fun main() { val cart = MyCart(coin = 100) cart.rent() cart.roll() cart.add(Product(name = \u0026#34;장난감\u0026#34;, price = 1000)) cart.printId() } Collection # import java.util.* import java.util.stream.Collectors import kotlin.collections.ArrayList fun main() { // immutable val currencyList = listOf(\u0026#34;달러\u0026#34;, \u0026#34;유로\u0026#34;, \u0026#34;원\u0026#34;) // mutable val mutableCurrencyList: MutableList\u0026lt;String\u0026gt; = mutableListOf\u0026lt;String\u0026gt;().apply { add(\u0026#34;달러\u0026#34;) add(\u0026#34;유로\u0026#34;) add(\u0026#34;원\u0026#34;) } mutableCurrencyList.add(\u0026#34;파운드\u0026#34;) // immutable set val numberSet = setOf(1, 2, 3, 4) // mutable set val mutableSet = mutableSetOf\u0026lt;Int\u0026gt;().apply { add(1) add(2) add(3) add(4) } // immutable map val numberMap = mapOf(\u0026#34;one\u0026#34; to 1, \u0026#34;two\u0026#34; to 2) // mutable map val mutableNumberMap = mutableMapOf\u0026lt;String, Int\u0026gt;() mutableNumberMap[\u0026#34;one\u0026#34;] = 1 mutableNumberMap[\u0026#34;two\u0026#34;] = 2 mutableNumberMap[\u0026#34;three\u0026#34;] = 3 // 컬렉션 빌더는 내부에선 mutable 반환은 immutable val numberList: List\u0026lt;Int\u0026gt; = buildList{ add(1) add(2) add(3) add(4) } // linkedList val linkedList = LinkedList\u0026lt;Int\u0026gt;().apply { addFirst(3) add(2) addLast(1) } // arrayList val arrayList = ArrayList\u0026lt;Int\u0026gt;().apply { add(1) add(2) add(3) } // val iterator = currencyList.iterator() // while (iterator.hasNext()) { // println(iterator.next()) // } // // println(\u0026#34;===============\u0026#34;) // // for (currency in currencyList) { // println(currency) // } // // println(\u0026#34;===============\u0026#34;) // // currencyList.forEach { // println(it) // } // for loop -\u0026gt; map val lowerList = listOf(\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;) //val upperList = mutableListOf\u0026lt;String\u0026gt;() // for (lowerCase in lowerList) { // upperList.add(lowerCase.uppercase()) // } val upperList = lowerList.map { it.uppercase() } //val filteredList = mutableListOf\u0026lt;String\u0026gt;() // for (upperCase in upperList) { // if (upperCase == \u0026#34;A\u0026#34; || upperCase == \u0026#34;C\u0026#34; ) { // filteredList.add(upperCase) // } // } val filteredList = upperList .asSequence() // 대량데이터의 경우 이걸 사용하는게 좋다. .filter { it == \u0026#34;A\u0026#34; || it == \u0026#34;C\u0026#34; } .toList() println(filteredList) } Data Calss # data class Person(val name: String, val age: Int) { } fun main() { val person1 = Person(name = \u0026#34;tony\u0026#34;, age = 12) val (name, age) = person1 println(\u0026#34;이름=${name}, 나이=${age}\u0026#34;) // val set = hashSetOf(person1) // println(set.contains(person1)) } Singleton # import java.time.LocalDateTime //object Singleton { // // val a = 1234 // // fun printA() = println(a) //} // //fun main() { // println(Singleton.a) // Singleton.printA() //} //object DatetimeUtils { // // val now : LocalDateTime // get() = LocalDateTime.now() // // const val DEFAULT_FORMAT = \u0026#34;YYYY-MM-DD\u0026#34; // // fun same(a: LocalDateTime, b: LocalDateTime) : Boolean { // return a == b // } // //} // //fun main() { // println(DatetimeUtils.now) // println(DatetimeUtils.now) // println(DatetimeUtils.now) // // println(DatetimeUtils.DEFAULT_FORMAT) // // val now = LocalDateTime.now() // println(DatetimeUtils.same(now, now)) //} class MyClass { private constructor() companion object MyCompanion { val a = 1234 fun newInstance() = MyClass() } } fun main() { println(MyClass.a) println(MyClass.newInstance()) println(MyClass.a) println(MyClass.newInstance()) } Sealed Class # sealed class Developer { abstract val name: String abstract fun code(language: String) } data class BackendDeveloper(override val name: String) : Developer() { override fun code(language: String) { println(\u0026#34;저는 백엔드 개발자입니다 ${language}를 사용합니다\u0026#34;) } } data class FrontendDeveloper(override val name: String) : Developer() { override fun code(language: String) { println(\u0026#34;저는 프론트엔드 개발자입니다 ${language}를 사용합니다\u0026#34;) } } object OtherDeveloper : Developer() { override val name: String = \u0026#34;익명\u0026#34; override fun code(language: String) { TODO(\u0026#34;Not yet implemented\u0026#34;) } } data class AndroidDeveloper(override val name: String) : Developer() { override fun code(language: String) { println(\u0026#34;저는 안드로이드 개발자입니다 ${language}를 사용합니다\u0026#34;) } } data class IosDeveloper(override val name: String) : Developer() { override fun code(language: String) { println(\u0026#34;저는 Ios 개발자입니다 ${language}를 사용합니다\u0026#34;) } } object DeveloperPool { val pool = mutableMapOf\u0026lt;String, Developer\u0026gt;() // 컴파일러는 Developer 구현 클래스가 무엇인지를 모름 // else가 없으면 when절에 컴파일 오류남 // Developer 를 sealed Class로 정의하면 else문 생략가능 // 같은 패키지/하위 모듈에 있는 경우에만 sealed class의 하위클래스가 될 수 있다 // 컴파일러가 Developer 의 자식클래스를 알고있끼 때문이다. fun add(developer: Developer) = when(developer) { is BackendDeveloper -\u0026gt; pool[developer.name] = developer is FrontendDeveloper -\u0026gt; pool[developer.name] = developer is AndroidDeveloper -\u0026gt; pool[developer.name] = developer is IosDeveloper -\u0026gt; pool[developer.name] = developer is OtherDeveloper -\u0026gt; println(\u0026#34;지원하지않는 개발자종류입니다\u0026#34;) } fun get(name: String) = pool[name] } fun main() { val backendDeveloper = BackendDeveloper(name=\u0026#34;토니\u0026#34;) DeveloperPool.add(backendDeveloper) val frontendDeveloper = FrontendDeveloper(name=\u0026#34;카즈야\u0026#34;) DeveloperPool.add(frontendDeveloper) val androidDeveloper = AndroidDeveloper(name=\u0026#34;안드로\u0026#34;) DeveloperPool.add(androidDeveloper) println(DeveloperPool.get(\u0026#34;토니\u0026#34;)) println(DeveloperPool.get(\u0026#34;카즈야\u0026#34;)) println(DeveloperPool.get(\u0026#34;안드로\u0026#34;)) } Extension # /** * 문자열 첫번째 원소 리턴 */ fun String.first() : Char { return this[0] } fun String.addFirst(char: Char) : String { // this : 수신자 객체 return char + this.substring(0) } class MyExample { fun printMessage() = println(\u0026#34;클래스 출력\u0026#34;) } // MyExample의 확장함수 생성 // printMessage 멤버함수와 이름을 동일하게 했을때 멤버함수가 우선적으로 수행된다. // 확장함수의 멤버함수와 동일한 시그니처는 멤버함수가 실행됨 fun MyExample.printMessage() = println(\u0026#34;확장 출력\u0026#34;) // 시그니처가 다르면 확장함수 실행이 잘 됨 fun MyExample.printMessage(message:String) = println(message) // MyExample 이 null일 가능성이 존재 // null인 경우와 아닌 경우 분기처리 fun MyExample?.printNullOrNotNull() { if (this == null) println(\u0026#34;널인 경우에만 출력\u0026#34;) else println(\u0026#34;널이 아닌 경우에만 출력\u0026#34;) } fun main() { var myExample: MyExample? = null // 함수에서 null 체크를 하고있다는걸 컴파일러가 알고있어서 오류가 안난다. myExample.printNullOrNotNull() myExample = MyExample() myExample.printNullOrNotNull() //MyExample().printMessage(\u0026#34;확장 출력\u0026#34;) // println(\u0026#34;ABCD\u0026#34;.first()) // // println(\u0026#34;ABCD\u0026#34;.addFirst(\u0026#39;Z\u0026#39;)) } Generics # class MyGenerics\u0026lt;out T\u0026gt;(val t: T) { // 공변성은 자바 제네릭의 extends 코틀린에선 out } class Bag\u0026lt;T\u0026gt; { fun saveAll( to: MutableList\u0026lt;in T\u0026gt;, // 반공변성은 자바 제네릭의 super 코틀린에선 in from: MutableList\u0026lt;T\u0026gt;, ) { to.addAll(from) } } fun main() { val bag = Bag\u0026lt;String\u0026gt;() // String이 CharSequence의 하위타입인데, // 반공변성에서는 mutableListOf\u0026lt;CharSequence\u0026gt;가 mutableListOf\u0026lt;String\u0026gt;의 하위타입이 된다. bag.saveAll(mutableListOf\u0026lt;CharSequence\u0026gt;(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;), mutableListOf\u0026lt;String\u0026gt;(\u0026#34;3\u0026#34;, \u0026#34;4\u0026#34;)) // MyGenerics\u0026lt;CharSequence\u0026gt; 가 MyGenerics\u0026lt;String\u0026gt; 보다 상위타입 val generics = MyGenerics\u0026lt;String\u0026gt;(\u0026#34;테스트\u0026#34;) val charGenerics : MyGenerics\u0026lt;CharSequence\u0026gt; = generics // 제네릭을 사용한 클래스의 인스턴스를 만드려면 타입아규먼트를 제공 (컴파일러 타입 추론 가능) val generics2 = MyGenerics\u0026lt;String\u0026gt;(\u0026#34;테스트\u0026#34;) // 생략가능 val generics3 = MyGenerics(\u0026#34;테스트\u0026#34;) // 변수의 타입에 제네릭을 사용한 경우 val list1: MutableList\u0026lt;String\u0026gt; = mutableListOf() // 타입아규먼트를 생성자에서 추가 val list2 = mutableListOf\u0026lt;String\u0026gt;() val list3 : List\u0026lt;*\u0026gt; = listOf\u0026lt;String\u0026gt;(\u0026#34;테스트\u0026#34;) val list4: List\u0026lt;*\u0026gt; = listOf\u0026lt;Int\u0026gt;(1, 2, 3, 4) // PECS는 Producer-Extends, Consumer-Super // 공변성은 자바 제네릭의 extends 코틀린에선 out // -\u0026gt; 공변성 : T’가 T의 서브타입이면, C\u0026lt;T’\u0026gt;는 C\u0026lt;out T\u0026gt;의 서브타입이다. // 반공변성은 자바 제네릭의 super 코틀린에선 in // -\u0026gt; 반공변성 : T’가 T의 서브타입이면, C\u0026lt;T\u0026gt;는 C\u0026lt;in T’\u0026gt;의 서브타입이다. } Late init # class `7_LateInit` { // 가변 프로퍼티에 대한 지연 초기화 // nullable이 아님에도 초기호 안했어도 컴파일 오류가 발생하지 않는다. lateinit var text: String // var : 가변 val textInitialized: Boolean // isInitialized 는 클래스 내부에서만 사용 가능하다. (Main 등에서 사용 불가능) get() = this::text.isInitialized // 초기화 여부 fun printText() { println(text) } } fun a (str:String, block: (String) -\u0026gt; Unit) { block(str) } fun main() { \u0026#34;\u0026#34;.let { } a(\u0026#34;\u0026#34;) { it.length } val test = `7_LateInit`() if (!test.textInitialized) { test.text = \u0026#34;하이요\u0026#34; } test.printText() // 초기화 전에 출력 요청하면 오류 발생 } Lazy init # class HelloBot { // val 불변 // by lazy 사용 (멀티쓰레드 안전) // 기본 : LazyThreadSafetyMode.SYNCHRONIZED // LazyThreadSafetyMode.NONE 등 상태값을 설정하여 쓰레드 안전성 무시 가능 // 불변을 유지하면서 변수에 대한 초기화를 뒤로 미룰수 있다. // 변수가 처음으로 사용될 때까지 해당 변수의 초기화를 늦춘다. // -\u0026gt; 즉, 변수에 처음으로 접근하는 시점에서 초기화 코드가 실행 val greeting: String by lazy(LazyThreadSafetyMode.PUBLICATION) { // 멀티쓰레드 환경에서도 동기화가 필요하지 않을때 : LazyThreadSafetyMode.PUBLICATION // 한 번 초기화된 이후에는 모든 스레드가 같은 값을 공유 // 따라서 여러 스레드에서 동시에 초기화를 시도하더라도 최초 한 번만 초기화가 이루어지고 나면 이후에는 초기화 코드가 다시 실행되지않음 getHello() } fun sayHello() = println(greeting) } fun getHello() = \u0026#34;안녕하세요\u0026#34; fun main() { val helloBot = HelloBot() // 초기화 이후에는 더이상 by lazy {}을 수행하지 않음 // ... // ... for (i in 1..5) { Thread { // 쓰레드 생성하여 병렬로 수행해보자 helloBot.sayHello() // 변수 초기화 첫 실행 }.start() } } Pair Destructuring # // f((1, 3)) = 1 + 3 = 4 // f(1, 3) = 1 + 3 = 4 //data class Tuple(val a : Int, val b: Int) fun plus(pair: Pair\u0026lt;Int, Int\u0026gt;) = pair.first + pair.second fun main() { //println(plus(1,3)) val plus = plus(Pair(1, 3)) println(plus) val pair = Pair(\u0026#34;A\u0026#34;, 1) // 불변 val newPair = pair.copy(first = \u0026#34;B\u0026#34;) // 새로운 Pair을 생성 println(newPair) val second = newPair.component2() // second 값 가져오기 println(second) val list = newPair.toList() println(list) /* 3개 요소 (4개 이상부터는 지원하지 않음, Collection 사용하면 됨) */ val triple = Triple(\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;) println(triple) // 출력 triple.first triple.second val newTriple = triple.copy(third = \u0026#34;D\u0026#34;) // third 값 변경한 새로운 Triple 생성 println(newTriple) println(newTriple.component3()) /* 구조분해 할당 */ val (a: String, b: String, c: String) = newTriple println(\u0026#34;$a, $b, $c\u0026#34;) val list3: List\u0026lt;String\u0026gt; = newTriple.toList() val (a1, a2, a3) = list3 println(\u0026#34;$a1, $a2, $a3\u0026#34;) list3.component1() list3.component2() list3.component3() // list3.component4() // list3.component5() val map = mutableMapOf(Pair(\u0026#34;이상훈\u0026#34;, \u0026#34;개발자\u0026#34;)) for ( (key, value) in map ) { println(\u0026#34;${key}의 직업은 $value\u0026#34;) } } 범위 지정 함수 # (이미지 출처 : https://medium.com/@limgyumin/%EC%BD%94%ED%8B%80%EB%A6%B0-%EC%9D%98-apply-with-let-also-run-%EC%9D%80-%EC%96%B8%EC%A0%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94%EA%B0%80-4a517292df29)\nRun # run 정의 inline fun \u0026lt;T, R\u0026gt; T.run(block: T.() -\u0026gt; R): R { return block() } 예제코드 class DatabaseClient { var url: String? = null var username: String? = null var password: String? = null // DB에 접속하고 Boolean 결과를 반환 fun connect(): Boolean { println(\u0026#34;DB 접속 중 ...\u0026#34;) Thread.sleep(1000) println(\u0026#34;DB 접속 완료\u0026#34;) return true } } fun main() { // val config = DatabaseClient() // config.url = \u0026#34;localhost:3306\u0026#34; // config.username = \u0026#34;mysql\u0026#34; // config.password = \u0026#34;1234\u0026#34; // val connected = config.connect() // run 안에서 수신자 객체 참조는 this로 함 (생략도 가능) // 변수 중복 참조를 생략할 수 있다는 장점 (위에서 config.xx가 반복됨) val connected: Boolean = DatabaseClient().run { url = \u0026#34;localhost:3306\u0026#34; username = \u0026#34;mysql\u0026#34; this.password = \u0026#34;1234\u0026#34; connect() // 자동 return } println(connected) val result: Boolean = with(DatabaseClient()) { url = \u0026#34;localhost:3306\u0026#34; username = \u0026#34;mysql\u0026#34; password = \u0026#34;1234\u0026#34; connect() } println(result) } Also # also 정의 inline fun \u0026lt;T\u0026gt; T.also(block: (T) -\u0026gt; Unit): T { block(this) return this } 예제 class User(val name: String, val password: String) { fun validate() { if (name.isNotEmpty() \u0026amp;\u0026amp; password.isNotEmpty()) { println(\u0026#34;검증 성공!\u0026#34;) } else { println(\u0026#34;검증 실패!\u0026#34;) } } fun printName() = println(name) } fun main() { User(name = \u0026#34;tony\u0026#34;, password = \u0026#34;1234\u0026#34;).also { // it을 사용해서 간결하게 사용 가능 it.validate() it.printName() } } Apply # apply 정의 inline fun \u0026lt;T\u0026gt; T.apply(block: T.() -\u0026gt; Unit): T { block() return this } 예제코드 fun main() { // return 타입이 Context 객체에 대한 타입 그대로 (DatabaseClient) DatabaseClient().apply { url = \u0026#34;localhost:3306\u0026#34; username = \u0026#34;mysql\u0026#34; this.password = \u0026#34;1234\u0026#34; }.connect() .run { println(this) } // this : connect() 함수의 반환결과 } Let # let 정의 inline fun \u0026lt;T, R\u0026gt; T.let(block: (T) -\u0026gt; R): R { return block(this) } 예제코드 fun main() { val str: String? = \u0026#34;안녕\u0026#34; // str 이 null이 아닌 경우에 동작한다. val result: Int? = str?.let { println(it) // it = str val abc: String? = \u0026#34;abc\u0026#34; val def: String? = \u0026#34;def\u0026#34; if (!abc.isNullOrEmpty() \u0026amp;\u0026amp; !def.isNullOrEmpty()) { println(\u0026#34;abcdef가 null 아님\u0026#34;) } // return 키워드 없이도 return 값으로 셋팅된다. 1234 } println(result) // val this: String? = null // val it : String? = null val hello = \u0026#34;hello\u0026#34; val hi = \u0026#34;hi\u0026#34; hello.let { a : String -\u0026gt; //println(a.length) hi.let{ b -\u0026gt; println(a.length) println(b.length) } } } With # with 정의 inline fun \u0026lt;T, R\u0026gt; with(receiver: T, block: T.() -\u0026gt; R): R { return receiver.block() } 예제 fun main() { val str = \u0026#34;안녕하세요\u0026#34; // val length: Int = with(str) { length // return 생략 가능 } println(length) } "},{"id":108,"href":"/docs/reactive_streams/002_impl1_reactor/","title":"002 Impl1 Reactor","section":"Reactive Streams","content":" Reactive Streams 구현 라이브러리 (1) Reactor # Project reactor # Pivotal 사에서 개발 Spring reactor에서 사용 Mono와 Flux publisher 제공 Project reactor - Flux # 0..n개의 item을 전달 에러가 발생하면 error signal 전달하고 종료 모든 item을 전달했다면 complete signal 전달 하고 종료 backPressure 지원 Flux 예제 # SimpleSubscriber\nFluxIterable publisher Subscription : StrictSubscriber @Slf4j @RequiredArgsConstructor public class p181_SimpleSubscriber\u0026lt;T\u0026gt; implements Subscriber\u0026lt;T\u0026gt; { private final Integer count; /** * 지속적으로 요청을 하는게 아니라, 딱 한번 N개의 요청을 받고 그 이후로 값을 계속 받음 * @param s the {@link Subscription} that allows requesting data via {@link Subscription#request(long)} */ @Override public void onSubscribe(Subscription s) { log.info(\u0026#34;subscribe\u0026#34;); s.request(count); // count만큼 request log.info(\u0026#34;request: {}\u0026#34;, count); } @SneakyThrows @Override public void onNext(T t) { log.info(\u0026#34;item: {}\u0026#34;, t); Thread.sleep(100); } @Override public void onError(Throwable t) { log.error(\u0026#34;error: {}\u0026#34;, t.getMessage()); } @Override public void onComplete() { log.info(\u0026#34;complete\u0026#34;); } } FluxSimpleExample\n@Slf4j public class p181_FluxSimpleExample { @SneakyThrows public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); // main 쓰레드에서 수행 getItems() // 고정된 개수를 subscribe .subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); Thread.sleep(1000); } private static Flux\u0026lt;Integer\u0026gt; getItems() { return Flux.fromIterable(List.of(1, 2, 3, 4, 5)); } } 실행결과\n13:18:58.672 [main] INFO com.example06.reactor.p181_FluxSimpleExample - start main 13:18:58.733 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 13:18:58.736 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 13:18:58.736 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 13:18:58.737 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 1 13:18:58.904 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 2 13:18:59.049 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 3 13:18:59.233 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 4 13:18:59.399 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 5 13:18:59.578 [main] INFO com.example06.reactor.p181_SimpleSubscriber - complete 13:18:59.578 [main] INFO com.example06.reactor.p181_FluxSimpleExample - end main Flux - subscribeOn 예제 # FluxSimpleSubscribeOnExample\n@Slf4j public class p182_FluxSimpleSubscribeOnExample { @SneakyThrows public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); getItems() .map(i -\u0026gt; { log.info(\u0026#34;map {}\u0026#34;, i); return i; }) // main 쓰레드가 아닌 다른 쓰레드에서 수행 .subscribeOn(Schedulers.single()) .subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); // 바로 호출 Thread.sleep(1000); } private static Flux\u0026lt;Integer\u0026gt; getItems() { return Flux.fromIterable(List.of(1, 2, 3, 4, 5)); } } 실행결과\nsingle-1 쓰레드에서 수행 13:22:13.042 [main] INFO com.example06.reactor.p182_FluxSimpleSubscribeOnExample - start main 13:22:13.094 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 13:22:13.120 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 13:22:13.120 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 13:22:13.122 [main] INFO com.example06.reactor.p182_FluxSimpleSubscribeOnExample - end main 13:22:13.124 [single-1] INFO com.example06.reactor.p182_FluxSimpleSubscribeOnExample - map 1 13:22:13.124 [single-1] INFO com.example06.reactor.p181_SimpleSubscriber - item: 1 13:22:13.264 [single-1] INFO com.example06.reactor.p182_FluxSimpleSubscribeOnExample - map 2 13:22:13.264 [single-1] INFO com.example06.reactor.p181_SimpleSubscriber - item: 2 13:22:13.440 [single-1] INFO com.example06.reactor.p182_FluxSimpleSubscribeOnExample - map 3 13:22:13.441 [single-1] INFO com.example06.reactor.p181_SimpleSubscriber - item: 3 13:22:13.613 [single-1] INFO com.example06.reactor.p182_FluxSimpleSubscribeOnExample - map 4 13:22:13.614 [single-1] INFO com.example06.reactor.p181_SimpleSubscriber - item: 4 13:22:13.789 [single-1] INFO com.example06.reactor.p182_FluxSimpleSubscribeOnExample - map 5 13:22:13.789 [single-1] INFO com.example06.reactor.p181_SimpleSubscriber - item: 5 13:22:13.969 [single-1] INFO com.example06.reactor.p181_SimpleSubscriber - complete Flux - subscribe # FluxNoSubscribeExample\nsubscribe하지 않으면, 아무 일도 일어나지 않는다. @Slf4j public class p183_FluxNoSubscribeExample { public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); getItems(); // subscribe 하지않으면 아무일도 일어나지 않는다. log.info(\u0026#34;end main\u0026#34;); } private static Flux\u0026lt;Integer\u0026gt; getItems() { return Flux.create(fluxSink -\u0026gt; { log.info(\u0026#34;start getItems\u0026#34;); for (int i = 0; i \u0026lt; 5; i++) { fluxSink.next(i); } fluxSink.complete(); log.info(\u0026#34;end getItems\u0026#34;); }); } } Flux - backPressure # 1번째 예제 @Slf4j public class p184_FluxSimpleRequestThreeExample { public static void main(String[] args) { // 3개 요청 (1, 2, 3 이후 종료) , 추가적인 요청 없음 getItems().subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(3)); } private static Flux\u0026lt;Integer\u0026gt; getItems() { return Flux.fromIterable(List.of(1, 2, 3, 4, 5)); } } SimpleSubscriber\n@Slf4j @RequiredArgsConstructor public class p181_SimpleSubscriber\u0026lt;T\u0026gt; implements Subscriber\u0026lt;T\u0026gt; { private final Integer count; /** * 지속적으로 요청을 하는게 아니라, 딱 한번 N개의 요청을 받고 그 이후로 값을 계속 받음 * @param s the {@link Subscription} that allows requesting data via {@link Subscription#request(long)} */ @Override public void onSubscribe(Subscription s) { log.info(\u0026#34;subscribe\u0026#34;); s.request(count); // count만큼 request log.info(\u0026#34;request: {}\u0026#34;, count); } ... } 실행결과\n09:24:32.953 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 09:24:32.957 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 09:24:32.957 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 3 09:24:32.958 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 1 09:24:33.091 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 2 09:24:33.232 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 3 2번째 예제 @Slf4j public class p185_FluxContinuousRequestSubscriberExample { public static void main(String[] args) { getItems().subscribe(new p185_ContinuousRequestSubscriber\u0026lt;\u0026gt;()); } private static Flux\u0026lt;Integer\u0026gt; getItems() { return Flux.fromIterable(List.of(1, 2, 3, 4, 5)); } } ContinuousRequestSubscriber\n@Slf4j public class p185_ContinuousRequestSubscriber\u0026lt;T\u0026gt; implements Subscriber\u0026lt;T\u0026gt; { private final Integer count = 1; private Subscription subscription; @Override public void onSubscribe(Subscription s) { this.subscription = s; log.info(\u0026#34;subscribe\u0026#34;); s.request(count); // 개수만큼 요청 log.info(\u0026#34;request: {}\u0026#34;, count); } @SneakyThrows @Override public void onNext(T t) { log.info(\u0026#34;item: {}\u0026#34;, t); Thread.sleep(1000); // 1개를 또 호출 subscription.request(1); log.info(\u0026#34;request: {}\u0026#34;, count); } @Override public void onError(Throwable t) { log.error(\u0026#34;error: {}\u0026#34;, t.getMessage()); } @Override public void onComplete() { log.info(\u0026#34;complete\u0026#34;); } } 아래 로직으로 계속 반복 수행한다. // 1개를 또 호출 subscription.request(1); 실행결과\n09:33:34.419 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 09:33:34.424 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - subscribe 09:33:34.424 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:33:34.425 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 1 09:33:35.445 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:33:35.445 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 2 09:33:36.518 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:33:36.518 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 3 09:33:37.589 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:33:37.589 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 4 09:33:38.655 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:33:38.655 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 5 09:33:39.718 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:33:39.727 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - complete Flux - error # @Slf4j public class p186_FluxErrorExample { public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); getItems().subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); } private static Flux\u0026lt;Integer\u0026gt; getItems() { return Flux.create(fluxSink -\u0026gt; { fluxSink.next(0); fluxSink.next(1); var error = new RuntimeException(\u0026#34;error in flux\u0026#34;); fluxSink.error(error); // 에러 전달 }); } } 실행결과\n09:36:46.692 [main] INFO com.example06.reactor.p186_FluxErrorExample - start main 09:36:46.756 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 09:36:46.762 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 09:36:46.762 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 09:36:46.764 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 0 09:36:46.885 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 1 09:36:47.028 [main] ERROR com.example06.reactor.p181_SimpleSubscriber - error: error in flux 09:36:47.028 [main] INFO com.example06.reactor.p186_FluxErrorExample - end main Flux - complete # @Slf4j public class p187_FluxCompleteExample { public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); getItems().subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); } private static Flux\u0026lt;Integer\u0026gt; getItems() { return Flux.create(fluxSink -\u0026gt; { fluxSink.complete(); // complete 전달 }); } } 실행결과\n09:37:31.038 [main] INFO com.example06.reactor.p187_FluxCompleteExample - start main 09:37:31.100 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 09:37:31.106 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 09:37:31.106 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 09:37:31.109 [main] INFO com.example06.reactor.p181_SimpleSubscriber - complete 09:37:31.109 [main] INFO com.example06.reactor.p187_FluxCompleteExample - end main Project reactor - Mono # 0..1개의 item을 전달 에러가 발생하면 error signal 전달하고 종료 모든 item을 전달했다면 complete signal 전달 하고 종료 Mono 예제 # 1개의 item만 전달하기 때문에 next 하나만 실행하면 complete가 보장 혹은 전달하지 않고 complete를 하면 값이 없다는 것을 의미 - 하나의 값이 있거나 없다 @Slf4j public class p190_MonoSimpleExample { @SneakyThrows public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); getItems() .subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); Thread.sleep(1000); } private static Mono\u0026lt;Integer\u0026gt; getItems() { return Mono.create(monoSink -\u0026gt; { monoSink.success(1); }); } } 실행결과\n09:39:02.445 [main] INFO com.example06.reactor.p190_MonoSimpleExample - start main 09:39:02.481 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 09:39:02.484 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 09:39:02.484 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 09:39:02.484 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 1 09:39:02.658 [main] INFO com.example06.reactor.p181_SimpleSubscriber - complete 09:39:02.658 [main] INFO com.example06.reactor.p190_MonoSimpleExample - end main Mono와 Flux # Mono : Optional\n없거나 혹은 하나의 값 Mono로 특정 사건이 완료되는 시점을 가리킬 수도 있다 Flux: List\n무한하거나 유한한 여러 개의 값 Flux를 Mono로 변환 # Mono.from으로 Flux를 Mono로 변환 첫 번째 값만 전달 @Slf4j public class p192_FluxToMonoExample { public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); // 1,2,3,4,5 중 첫번째값 1이 onNext로 전달되고 complete // 뒤에 있는 값들은 모두 무시 Mono.from(getItems()) .subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); } private static Flux\u0026lt;Integer\u0026gt; getItems() { return Flux.fromIterable(List.of(1, 2, 3, 4, 5)); } } 실행결과\n09:40:37.275 [main] INFO com.example06.reactor.p192_FluxToMonoExample - start main 09:40:37.340 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 09:40:37.363 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 09:40:37.363 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 09:40:37.365 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 1 09:40:37.473 [main] INFO com.example06.reactor.p181_SimpleSubscriber - complete 09:40:37.473 [main] INFO com.example06.reactor.p192_FluxToMonoExample - end main Flux를 Mono로 변환 (Mono\u0026lt;List\u0026gt;) # @Slf4j public class p193_FluxToListMonoExample { public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); getItems() // collect 하고 complete 이벤트 발생 시점에 모은 값들을 모두 전달 // 1, 2, 3, 4, 5를 내부 배열에 저장하고, 가지고있던 값들을 모두 onNext() 한다. // 하나로 합쳐져서 Mono로 한번 요청됨 ([1,2,3,4,5]) .collectList() .subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); } private static Flux\u0026lt;Integer\u0026gt; getItems() { return Flux.fromIterable(List.of(1, 2, 3, 4, 5)); } } 실행결과\n09:41:24.680 [main] INFO com.example06.reactor.p193_FluxToListMonoExample - start main 09:41:24.743 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 09:41:24.766 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 09:41:24.766 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 09:41:24.767 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: [1, 2, 3, 4, 5] 09:41:24.940 [main] INFO com.example06.reactor.p181_SimpleSubscriber - complete 09:41:24.940 [main] INFO com.example06.reactor.p193_FluxToListMonoExample - end main Mono를 Flux로 변환 # flux() Mono를 next 한 번 호출하고 onComplete를 호출하는 Flux로 변환 @Slf4j public class p194_MonoToFluxExample { public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); // flux() - Mono를 next 한번 호출하고 onComplete를 호출하는 Flux로 변환 // [1,2,3,4,5] getItems().flux() .subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); } private static Mono\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; getItems() { return Mono.just(List.of(1, 2, 3, 4, 5)); } } 실행결과\n09:42:16.606 [main] INFO com.example06.reactor.p194_MonoToFluxExample - start main 09:42:16.650 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 09:42:16.694 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 09:42:16.695 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 09:42:16.695 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: [1, 2, 3, 4, 5] 09:42:16.802 [main] INFO com.example06.reactor.p181_SimpleSubscriber - complete 09:42:16.802 [main] INFO com.example06.reactor.p194_MonoToFluxExample - end main Mono를 Flux로 변환 (Mono\u0026lt;List\u0026gt; -\u0026gt; Flux) # @Slf4j public class p195_ListMonoToFluxExample { public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); getItems() // Mono의 결과를 Flux 형태로 바꾸고, flux를 받아서 처리 // 1, 2, 3, 4, 5 하나씩 처리 .flatMapMany(value -\u0026gt; Flux.fromIterable(value)) .subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); } private static Mono\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; getItems() { return Mono.just(List.of(1, 2, 3, 4, 5)); } } 실행결과\n09:43:07.895 [main] INFO com.example06.reactor.p195_ListMonoToFluxExample - start main 09:43:07.931 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework 09:43:07.972 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 09:43:07.972 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 09:43:07.972 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 1 09:43:08.082 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 2 09:43:08.239 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 3 09:43:08.414 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 4 09:43:08.588 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 5 09:43:08.724 [main] INFO com.example06.reactor.p181_SimpleSubscriber - complete 09:43:08.724 [main] INFO com.example06.reactor.p195_ListMonoToFluxExample - end main References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지 github 예제코드 : https://github.com/seohaebada/webflux "},{"id":109,"href":"/docs/reactive_streams/003_impl2_rxjava/","title":"003 Impl2 Rxjava","section":"Reactive Streams","content":" Reactive Streams 구현 라이브러리 (2) RxJava # RxJava # Netflix 사에서 개발 닷넷 프레임워크를 지원하는 Reactive Extensions를 포팅 Flowable, Observable, Single, Maybe, Completable, publisher 제공 RxJava - Flowable # 0..n개의 item을 전달 에러가 발생하면 error signal 전달 하고 종료 모든 item을 전달했다면 complete signal 전달하고 종료 backPressure 지원 Reactor의 Flux와 유사 Flowable 예제 # @Slf4j public class p199_FlowableExample { public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); getItems() .subscribe(new p181_SimpleSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); log.info(\u0026#34;end main\u0026#34;); } private static Flowable\u0026lt;Integer\u0026gt; getItems() { return Flowable.fromIterable(List.of(1, 2, 3, 4, 5)); } } 실행결과\n09:53:02.296 [main] INFO com.example06.rxjava.p199_FlowableExample - start main 09:53:02.339 [main] INFO com.example06.reactor.p181_SimpleSubscriber - subscribe 09:53:02.339 [main] INFO com.example06.reactor.p181_SimpleSubscriber - request: 2147483647 09:53:02.340 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 1 09:53:02.450 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 2 09:53:02.622 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 3 09:53:02.762 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 4 09:53:02.933 [main] INFO com.example06.reactor.p181_SimpleSubscriber - item: 5 09:53:03.112 [main] INFO com.example06.reactor.p181_SimpleSubscriber - complete 09:53:03.114 [main] INFO com.example06.rxjava.p199_FlowableExample - end main Flowable - backPressure 예제 # @Slf4j public class p200_FlowableContinuousRequestSubscriberExample { public static void main(String[] args) { log.info(\u0026#34;start main\u0026#34;); getItems() // 1개씩 처리 (backPressure) .subscribe(new p185_ContinuousRequestSubscriber\u0026lt;\u0026gt;()); log.info(\u0026#34;end main\u0026#34;); } private static Flowable\u0026lt;Integer\u0026gt; getItems() { return Flowable.fromIterable(List.of(1, 2, 3, 4, 5)); } } ContinuousRequestSubscriber\n@Slf4j public class p185_ContinuousRequestSubscriber\u0026lt;T\u0026gt; implements Subscriber\u0026lt;T\u0026gt; { private final Integer count = 1; private Subscription subscription; @Override public void onSubscribe(Subscription s) { this.subscription = s; log.info(\u0026#34;subscribe\u0026#34;); s.request(count); // 개수만큼 요청 log.info(\u0026#34;request: {}\u0026#34;, count); } @SneakyThrows @Override public void onNext(T t) { log.info(\u0026#34;item: {}\u0026#34;, t); Thread.sleep(1000); // 1개를 또 호출 subscription.request(1); log.info(\u0026#34;request: {}\u0026#34;, count); } @Override public void onError(Throwable t) { log.error(\u0026#34;error: {}\u0026#34;, t.getMessage()); } @Override public void onComplete() { log.info(\u0026#34;complete\u0026#34;); } } 실행결과\n09:54:17.223 [main] INFO com.example06.rxjava.p200_FlowableContinuousRequestSubscriberExample - start main 09:54:17.258 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - subscribe 09:54:17.259 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:54:17.260 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 1 09:54:18.335 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:54:18.335 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 2 09:54:19.408 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:54:19.409 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 3 09:54:20.483 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:54:20.484 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 4 09:54:21.544 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:54:21.545 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - item: 5 09:54:22.618 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - request: 1 09:54:22.623 [main] INFO com.example06.reactor.p185_ContinuousRequestSubscriber - complete 09:54:22.623 [main] INFO com.example06.rxjava.p200_FlowableContinuousRequestSubscriberExample - end main RxJava - Observable # 0..n개의 item을 전달 에러가 발생하면 error signal 전달 하고 종료 모든 item을 전달했다면 complete signal 전달하고 종료 backPressure 지원 X Observable vs Flowable # Observable Flowable Push 기반 Pull 기반 Subscriber가 처리할 수 없더라도 item을 전달 Subscriber가 request의 수를 조절 Reactive manifesto의 message driven을 일부만 준수 Reactive manifesto의 message driven을 모두 준수 onSubscribe로 Disposable 전달 onSubscribe시 Subscription 전 달\nObservable 예제 # @Slf4j public class p203_ObservableExample { public static void main(String[] args) { // 배압 조절 불가능 getItems() .subscribe(new p203_SimpleObserver()); } private static Observable\u0026lt;Integer\u0026gt; getItems() { return Observable.fromIterable(List.of(1, 2, 3, 4, 5)); } } SimpleObserver\n@Slf4j public class p203_SimpleObserver implements Observer { private Disposable disposable; @Override public void onSubscribe(@NonNull Disposable d) { log.info(\u0026#34;subscribe\u0026#34;); this.disposable = d; } @Override public void onNext(@NonNull Object o) { log.info(\u0026#34;item: {}\u0026#34;, o); } @Override public void onError(@NonNull Throwable e) { log.error(\u0026#34;error: {}\u0026#34;, e.getMessage()); } @Override public void onComplete() { log.info(\u0026#34;complete\u0026#34;); } } 실행결과\n09:57:21.403 [main] INFO com.example06.rxjava.p203_SimpleObserver - subscribe 09:57:21.404 [main] INFO com.example06.rxjava.p203_SimpleObserver - item: 1 09:57:21.404 [main] INFO com.example06.rxjava.p203_SimpleObserver - item: 2 09:57:21.404 [main] INFO com.example06.rxjava.p203_SimpleObserver - item: 3 09:57:21.404 [main] INFO com.example06.rxjava.p203_SimpleObserver - item: 4 09:57:21.405 [main] INFO com.example06.rxjava.p203_SimpleObserver - item: 5 09:57:21.405 [main] INFO com.example06.rxjava.p203_SimpleObserver - complete RxJava - Single # 1개의 item을 전달 후 바로 onComplete signal 전달 1개의 item이 없다면 onError signal 전달 에러가 발생했다면 onError signal 전달 Single - success 예제 # public class p205_SingleExample { public static void main(String[] args) { getItem() .subscribe(new p205_SimpleSingleObserver\u0026lt;\u0026gt;()); } private static Single\u0026lt;Integer\u0026gt; getItem() { return Single.create(singleEmitter -\u0026gt; { singleEmitter.onSuccess(1); }); } } SimpleSingleObserver\n@Slf4j public class p205_SimpleSingleObserver\u0026lt;T\u0026gt; implements SingleObserver\u0026lt;T\u0026gt; { private Disposable disposable; @Override public void onSubscribe(@NonNull Disposable d) { this.disposable = d; log.info(\u0026#34;subscribe\u0026#34;); } @Override public void onSuccess(@NonNull Object o) { log.info(\u0026#34;item: {}\u0026#34;, o); } @Override public void onError(@NonNull Throwable e) { log.error(\u0026#34;error: {}\u0026#34;, e.getMessage()); } } 실행결과\n09:58:59.778 [main] INFO com.example06.rxjava.p205_SimpleSingleObserver - subscribe 09:58:59.780 [main] INFO com.example06.rxjava.p205_SimpleSingleObserver - item: 1 Single - error (빈 값) # @Slf4j public class p206_SingleNullExample { public static void main(String[] args) { getItem() .subscribe(new p205_SimpleSingleObserver\u0026lt;\u0026gt;()); } private static Single\u0026lt;Integer\u0026gt; getItem() { return Single.create(singleEmitter -\u0026gt; { singleEmitter.onSuccess(null); // 에러 발생시킴 }); } } 실행결과\n09:59:53.191 [main] INFO com.example06.rxjava.p205_SimpleSingleObserver - subscribe 09:59:53.193 [main] ERROR com.example06.rxjava.p205_SimpleSingleObserver - error: onSuccess called with a null value. Null values are generally not allowed in 3.x operators and sources. RxJava - Maybe # 1개의 item을 전달 후 바로 onComplete signal 전달 1개의 item이 없어도 onComplete signal 전달 가능 에러가 발생했다면 onError signal 전달 Reactor의 Mono와 유사 예제 # SimpleMaybeObserver\n@Slf4j public class SimpleMaybeObserver\u0026lt;T\u0026gt; implements MaybeObserver\u0026lt;T\u0026gt; { private Disposable disposable; @Override public void onSubscribe(@NonNull Disposable d) { this.disposable = d; log.info(\u0026#34;subscribe\u0026#34;); } @Override public void onSuccess(@NonNull T t) { log.info(\u0026#34;item: {}\u0026#34;, t); } @Override public void onError(@NonNull Throwable e) { log.error(\u0026#34;error: {}\u0026#34;, e.getMessage()); } @Override public void onComplete() { log.info(\u0026#34;complete\u0026#34;); } } Maybe - success 예제 # @Slf4j public class p208_MaybeExample { public static void main(String[] args) { maybeGetItem() .subscribe(new SimpleMaybeObserver\u0026lt;\u0026gt;()); } private static Maybe\u0026lt;Integer\u0026gt; maybeGetItem() { return Maybe.create(maybeEmitter -\u0026gt; { maybeEmitter.onSuccess(1); }); } } 실행결과\n10:42:03.579 [main] INFO com.example06.rxjava.SimpleMaybeObserver - subscribe 10:42:03.581 [main] INFO com.example06.rxjava.SimpleMaybeObserver - item: 1 Maybe - success (빈 값) 예제 # @Slf4j public class p209_MaybeEmptyValueExample { public static void main(String[] args) { maybeGetItem() .subscribe(new SimpleMaybeObserver\u0026lt;\u0026gt;()); } private static Maybe\u0026lt;Integer\u0026gt; maybeGetItem() { return Maybe.create(maybeEmitter -\u0026gt; { maybeEmitter.onComplete(); // complete()만 호출 }); } } 실행결과\n10:42:24.592 [main] INFO com.example06.rxjava.SimpleMaybeObserver - subscribe 10:42:24.593 [main] INFO com.example06.rxjava.SimpleMaybeObserver - complete Maybe - error 예제 # @Slf4j public class p209_MaybeNullValueExample { public static void main(String[] args) { maybeGetItem() .subscribe(new SimpleMaybeObserver\u0026lt;\u0026gt;()); } private static Maybe\u0026lt;Integer\u0026gt; maybeGetItem() { return Maybe.create(maybeEmitter -\u0026gt; { maybeEmitter.onSuccess(null); }); } } 실행결과\n10:42:48.454 [main] INFO com.example06.rxjava.SimpleMaybeObserver - subscribe 10:42:48.456 [main] ERROR com.example06.rxjava.SimpleMaybeObserver - error: onSuccess called with a null value. Null values are generally not allowed in 3.x operators and sources. RxJava - Completable # onComplete 혹은 onError signal만 전달 값이 아닌 사건을 전달 예제 # SimpleCompletableObserver\n@Slf4j public class SimpleCompletableObserver implements CompletableObserver { private Disposable disposable; @Override public void onSubscribe(@NonNull Disposable d) { log.info(\u0026#34;subscribe\u0026#34;); this.disposable = d; } @Override public void onComplete() { log.info(\u0026#34;complete\u0026#34;); } @Override public void onError(@NonNull Throwable e) { log.error(\u0026#34;error: {}\u0026#34;, e.getMessage()); } } Completable - success 예제 # @Slf4j public class p212_CompletableExample { public static void main(String[] args) { getCompletion() .subscribe(new SimpleCompletableObserver()); } private static Completable getCompletion() { return Completable.create(completableEmitter -\u0026gt; { Thread.sleep(1000); completableEmitter.onComplete(); // 값이 아닌 사건을 전달 }); } } 실행결과\n10:43:45.900 [main] INFO com.example06.rxjava.SimpleCompletableObserver - subscribe 10:43:46.924 [main] INFO com.example06.rxjava.SimpleCompletableObserver - complete Completable - error 예제 # @Slf4j public class p213_CompletableErrorExample { public static void main(String[] args) { getCompletion() .subscribe(new SimpleCompletableObserver()); } private static Completable getCompletion() { return Completable.create(completableEmitter -\u0026gt; { Thread.sleep(1000); completableEmitter.onError( new RuntimeException(\u0026#34;error in completable\u0026#34;) ); }); } } 실행결과\n10:44:07.096 [main] INFO com.example06.rxjava.SimpleCompletableObserver - subscribe 10:44:08.124 [main] ERROR com.example06.rxjava.SimpleCompletableObserver - error: error in completable References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지 github 예제코드 : https://github.com/seohaebada/webflux "},{"id":110,"href":"/docs/reactive_streams/004_impl3_munity/","title":"004 Impl3 Munity","section":"Reactive Streams","content":" Reactive Streams 구현 라이브러리 (3) Munity # Mutiny # Hibernate reactive에서 비동기 라이브러리로 제공 Multi, Uni publisher 제공 Mutiny - Multi # 0..n개의 item을 전달 에러가 발생하면 error signal 전달 하고 종료 모든 item을 전달했다면 complete signal 전달하고 종료 backPressure 지원 Reactor의 flux와 유사 Multi 예제 # @Slf4j public class p218_MultiExample { public static void main(String[] args) { getItems() .subscribe() // subscribe 동시에 넘길 수 없음, subscribe() 호출 후 아래 호출 필요 .withSubscriber( new p218_SimpleMultiSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE) ); } private static Multi\u0026lt;Integer\u0026gt; getItems() { return Multi.createFrom().items(1, 2, 3, 4, 5); } } SimpleMultiSubscriber\n@Slf4j @RequiredArgsConstructor public class p218_SimpleMultiSubscriber\u0026lt;T\u0026gt; implements MultiSubscriber\u0026lt;T\u0026gt; { private final Integer count; @Override public void onSubscribe(Flow.Subscription s) { s.request(count); log.info(\u0026#34;subscribe\u0026#34;); } @Override public void onItem(T item) { log.info(\u0026#34;item: {}\u0026#34;, item); } @Override public void onFailure(Throwable failure) { log.error(\u0026#34;fail: {}\u0026#34;, failure.getMessage()); } @Override public void onCompletion() { log.info(\u0026#34;completion\u0026#34;); } } 실행결과\n10:46:36.701 [main] INFO com.example06.mutiny.p218_SimpleMultiSubscriber - item: 1 10:46:36.702 [main] INFO com.example06.mutiny.p218_SimpleMultiSubscriber - item: 2 10:46:36.702 [main] INFO com.example06.mutiny.p218_SimpleMultiSubscriber - item: 3 10:46:36.702 [main] INFO com.example06.mutiny.p218_SimpleMultiSubscriber - item: 4 10:46:36.702 [main] INFO com.example06.mutiny.p218_SimpleMultiSubscriber - item: 5 10:46:36.702 [main] INFO com.example06.mutiny.p218_SimpleMultiSubscriber - completion 10:46:36.702 [main] INFO com.example06.mutiny.p218_SimpleMultiSubscriber - subscribe Mutiny - Uni # 0..1개의 item을 전달 에러가 발생하면 error signal 전달 하고 종료 모든 item을 전달했다면 complete signal 전달하고 종료 Reactor의 Mono와 유사 Uni 예제 # @Slf4j public class p220_UniExample { public static void main(String[] args) { getItem() .subscribe() .withSubscriber(new p220_SimpleUniSubscriber\u0026lt;\u0026gt;(Integer.MAX_VALUE)); } private static Uni\u0026lt;Integer\u0026gt; getItem() { return Uni.createFrom().item(1); } } SimpleUniSubscriber\n@Slf4j @RequiredArgsConstructor public class p220_SimpleUniSubscriber\u0026lt;T\u0026gt; implements UniSubscriber\u0026lt;T\u0026gt; { private final Integer count; private UniSubscription subscription; @Override public void onSubscribe(UniSubscription s) { this.subscription = s; s.request(1); log.info(\u0026#34;subscribe\u0026#34;); } @Override public void onItem(T item) { log.info(\u0026#34;item: {}\u0026#34;, item); } @Override public void onFailure(Throwable failure) { log.error(\u0026#34;error: {}\u0026#34;, failure.getMessage()); } } 실행결과\n10:47:27.202 [main] INFO com.example06.mutiny.p220_SimpleUniSubscriber - subscribe 10:47:27.208 [main] INFO com.example06.mutiny.p220_SimpleUniSubscriber - item: 1 References 강의 : Spring Webflux 완전 정복 : 코루틴부터 리액티브 MSA 프로젝트까지 github 예제코드 : https://github.com/seohaebada/webflux "},{"id":111,"href":"/docs/redis/001_redis_datastructure/","title":"001 Redis Datastructure","section":"Redis","content":" 레디스 자료구조 활용사례 # 리더보드 # 경쟁자들의 순위와 현재 점수를 보여주는 순위표를 의미한다. 스코어로 정렬되어 상위 경쟁자의 순위를 보여준다.\n절대적 리더보드 서비스의 모든 유저를 정렬시켜 상위권의 목록만을 표시\n상대적 리더보드 사용자의 스코어를 기반으로 그들을 다른 사용자와 비교해 순위를 결정 ex) 유저와 인접해있는 경쟁자들의 스코어를 보여주는 리더보드, 특정 그룹 내에서의 순위를 보여주는 리더보드, 주간 리더보드\n리더보드는 기본적으로 사용자의 스코어를 기반으로 데이터를 정렬하는 서비스이기 때문에 사용자의 증가에 따라 가공해야할 데이터가 몇 배로 증가한다. 또한 리더보드는 실시간으로 반영돼야하는 데이터다. 데이터가 실시간으로 계산되어, 자신의 순위 변동이 바로 확인되어야한다.\nsorted set을 이용한 리더보드 레디스의 sorted set 은 데이터가 저장될 때부터 정렬된다. 유저의 스코어를 가중치로 설정하여 스코어 순으로 유저를 정렬할 수 있다.\n▶ daily-score:\u0026lt;날짜\u0026gt; 를 이용해 sorted set 키를 만들고, 사용자의 스코어를 가중치로 사용해서 데이터를 입력해보자.\nZADD daily-score:228017 28 player:286 ZADD daily-score:228017 400 player:234 ZADD daily-score:228017 45 player:101 ZADD daily-score:228017 357 player:24 ZADD daily-score:228017 199 player:143 위와 같이 데이터를 저장했다. 데이터는 스코어 순으로 정렬되어 있을 것이다.\nZRANGE daily-score:228017 0 -1 withscores ▶ 상위 유저 3명만 출력 ZREVRANGE는 sorted set에 저장된 데이터를 내림차순으로 반환한다. 0번 인덱스 ~ 2번 인덱스(세번째 데이터)까지 출력한다.\nZREVRANGE daily-score:228017 0 2 withscores ▶ 데이터 업데이트 sorted set은 데이터가 중복으로 저장되지 않으므로, 같은 아이템을 저장하고자 할때 스코어가 다르면 기존 데이터의 스코어만 업데이트한다. 이와 동시에 재정렬된다.\nZADD daily-score:228017 200 player:286 ▶ ZINCRBY 커맨드 스코어를 증감시킬 수 있다.\nZINCRBY daily-score:228017 100 player:24 랭킹 합산 # 주간 리더보드가 매주 월요일마다 초기화된다고 가정하자. 레디스에서 주간 누적 랭킹은 ZUNIONSTORE 커맨드로 간단하게 구현할 수 있다.\n▶ ZUNIONSTORE 커맨드 지정한 키에 연결된 각 아이템의 스코어를 합산하는 커맨드다. 해당하는 일자의 키를 지정하기만 한다면 손쉽게 주간 리더보드 데이터를 얻을 수 있다.\n22년 8월 15일 ~ 17일까지의 데이터 합산 // \u0026lt;생성할 키 이름\u0026gt;\u0026lt;합산할 키 개수\u0026gt;\u0026lt;합산할 키\u0026gt;... ZUNIONSTORE weekly-score:2208-3 3 daily-score:228015 daily-score:228016 daily-score:228017 합산한 결과 조회 ZRANGE weekly-score:2208-3 0 -1 withscores ZREVRANGE weekly-score:2208-3 0 -1 withscores // 역순정렬 스코어에 가중치 주기 8월 16일에 스코어 두배 이벤트가 있었다면, 두배로 늘려 계산해야한다. // 순서대로 15일, 16일, 17일에 x1, x1, x2의 결과값으로 합산된 랭킹을 구할 수 있다. ZUNIONSTORE weekly-score:2208-03 3 daily-score:228015 daily-score:228016 daily-score:228017 weights 1 1 2 sorted set을 이용한 최근 검색 내역\n유저 별로 다른 키워드 노출 검색 내역은 중복 제거 가장 최근 검색한 5개의 키워드만 사용자에게 노출 sorted set은 중복을 허용하지 않으며, 스코어를 시간으로 사용한다면 최근 검색 기록을 정렬할 수 있다. 데이터를 저장할때 유저가 검색한 시간을 스코어로 저장한다면 검색 시간 순으로 정렬된 데이터가 저장된다.\n▶ user id가 123인 유저의 검색 기록 저장\nZADD search-keyword:123 20221106143501 코듀로이 ZADD search-keyword:123 20221105220913 실버 ZADD search-keyword:123 20221105221002 반지갑 ZADD search-keyword:123 20221105220954 에나멜 ZADD search-keyword:123 20221106152734 기모후드 ▶ 최근 데이터 조회\nZREVRANGE search-keyword:123 0 4 withscores ▶ 반지갑이라는 키워드 재검색\nZADD search-keyword:123 20221106160104 반지갑 ▶ 오래된 데이터 삭제 sorted set의 음수 인덱스를 사용해서 데이터를 삭제한다면, 아이템의 개수(6개가 되었는지)를 확인해야하는 번거로운 작업을 줄일 수 있다. 음수 인덱스는 아이템의 제일 마지막 값을 -1로 시작해서 역순으로 증가하는 값이다. ex) 데이터가 6개라면, 인덱스 0 또는 음수인덱스 -6이 제일 오래된 아이템이다.\n6번째 데이터를 삽입한다. ZADD search-keyword:123 20221106165302 버킷햇 전체 데이터를 조회한다. ZREVRANGE search-keyword:123 0 -1 withscores 0번째 인덱스 또는 -6 인덱스인 \u0026lsquo;실버\u0026rsquo;를 삭제한다. ZREMRANGEBYRANK search-keyword:123 -6 -6 // -6부터 -6까지 만약 아이템의 개수가 5개보다 많지 않을때에는 -6번째 인덱스는 존재하지 않기 때문에 삭제된 데이터가 없으므로 영향을 주지 않는다. 이로써 굳이 아이템의 개수를 체크할 필요없이 최근 데이터 5개만 유지할 수 있다.\nsorted set을 이용한 태그 기능 ▶ 각 포스트가 사용하는 태그를 레디스의 set을 이용해 저장해보자. 태그는 IT, REDIS, DataStore 이라고 하자.\nSADD post:47:tags IT REDIS DataStore SADD post:22:tags IT python 태그 기능을 사용하는 이유는 특정 게시물이 어떤 태그와 연관돼 있는지 확인하기 위함과, 특정한 태그를 포함한 게시물들만 확인하기 위해서다.\n▶ 태그를 기준으로 하는 set에 각각 데이터를 넣어보자.\nSADD tag:DataStore:posts 53 SADD tag:IT:posts 53 SADD tag:MySQL:posts 53 ▶ SMEMBERS 커맨드 특정 태그를 갖고있는 포스트를 쉽게 확인할 수 있다.\nSMEMBERS tag:IT:posts ▶ SINTER 커맨드 IT, DataStore 태그를 모두 포함하는 게시물을 확인하고 싶은 경우 set의 교집합을 구하면된다.\nSINTER tag:IT:posts tag:DataStore:posts 좋아요 처리하기 # 댓글 id를 기준으로 set을 생성한뒤, 좋아요를 누른 유저의 id를 저장하면 중복 없이 데이터를 저장할 수 있다.\n좋아요를 누른 유저 967 저장 SADD comment-like:12554 967 건수 조회 SCARD comment-like:12554 hash를 이용한 읽지 않은 메시지 수 카운팅하기 # 채널에 새로 추가된 메시지의 개수를 확인하면 된다. 사용자의 ID를 키로 사용하고, 채널의 ID를 아이템의 키로 활용해 숫자 형태의 메시지 카운트를 관리한다.\nID가 234인 사용자 -\u0026gt; 4234 채널에서 새로운 메시지를 수신 HINCRBY user:234 channel:4234 1 전송한 메시지를 삭제했다면 데이터 감소 HINCRBY user:234 channel:4234 -1 DAU 구하기 # DAU(Daily Active User)는 하루 동안 서비스에 방문한 사용자의 수를 의미한다. 하루에 여러번 방문했다 하더라도 한번으로 카운팅되는 값으로, 실제 서비스를 이용한 사용자의 유니크한 수를 파악할 수 있는 지표다.\n레디스의 비트맵을 이용하면 메모리를 효율적으로 줄이면서도 실시간으로 서비스의 DAU를 확인할 수 있다. 사용자 ID는 string 자료구조에서 하나의 비트로 표현될 수 있다.\n2022년 11월 6일에 방문한 유저 id를 구한다. uv:20221106 데이터를 만든 뒤, 접속한 유저 id의 bit를 1로 설정한다. id가 14인 유저가 접근했을때 오프셋 14를 1로 설정해준다. SETBIT uv:20221106 14 1 유저 수 확인 BITCOUNT uv:20221106 ▶ BITOP AND 커맨드 출석 이벤트를 진행하기 위해 특정 기간 11월 1일부터 11월 3일까지 매일 방문한 사용자를 구해보자.\n// 11월 1일 ~ 11월 3일 BITOP AND event:202211 uv:20221101 uv:20221102 uv:20221103 ▶ 위 이벤트 결과 확인\nGET event:202211 "},{"id":112,"href":"/docs/redis/002_redis_cache/","title":"002 Redis Cache","section":"Redis","content":" 레디스를 캐시로 사용하기 # [캐시란?] # 데이터의 원본보다 더 빠르고 효율적으로 액세스할 수 있는 임시 데이터 저장소를 의미한다.\n[캐시로서의 레디스] # 레디스는 자체적으로 고가용성 기능을 가지고있다. 일부 캐싱 전략에서는 캐시에 접근할 수 없게 되면 이는 곧바로 서비스 장애로 이어질 수 있따. 캐시 저장소도 일반적인 데이터 저장소와 같이 안정적으로 운영될 수 있는 조건을 갖추는 것이 좋다. 레디스의 센티널, 클러스터 기능을 사용하면 마스터 노드의 장애를 자동으로 감지해 페일오버(Failover; 장애대비)를 발생시키기 때문에, 운영자의 개입 없이 캐시는 정상으로 유지될 수 있어 가용성이 높아진다. 레디스의 클러스터를 사용하면 캐시의 스케일 아웃 또한 쉽게 처리 가능하다. 서비스 규모에 따라 캐시 자체의 규모도 늘어나야할 상황이 발생할 수 있는데, 자체 샤딩 솔루션인 클러스터를 사용하면 수평 확장이 간단해진다. 레디스는 캐시 저장소 용도로 이상적이다.\n[캐싱 전략] # 캐싱 전략은 캐싱되는 데이터의 유형과 데이터에 대한 엑세스 패턴에 따라 다르기 때문에 적절한 캐싱 전략을 선택해야한다.\n[읽기 전략 - look aside] # 애플리케이션에서 데이터를 읽어갈때 주로 사용한다. 레디스를 사용할때 가장 일반적으로 배치하는 방법이다.\n데이터가 먼저 캐시에 있는지 확인한다. 캐시에 있으면 캐시에서 데이터를 읽어온다. (=캐시 히트) 찾고자하는 데이터가 없을때에는 캐시 미스가 발생하며, 직접 데이터베이스에 접근해 찾고자하는 데이터를 가져온다. 찾고하자는 데이터가 레디스에 없을때에만 레디스에 저장하므로 lazy loading 이라고도 부른다. 장점\n레디스에 문제가 생겨 접근을 할 수 없는 상황이 발생하더라도 장애 발생이 아닌, 직접 데이터베이스에 다시 데이터를 가지고온다. 모든 커넥션이 한꺼번에 원본 데이터베이스로 몰려 많으 부하를 발생시킬 수 있다. 미리 데이터베이스에서 캐시로 데이터를 밀어넣어주는 작업을 하기도하는데, 이를 캐시 워밍(cache warming)이라고도 한다.\n[쓰기 전략과 데이터의 일관성] # 캐시 불일치 : 데이터가 변경될때 원본 데이터베이스에만 업데이트돼 캐시에는 변경된 값이 반영되지 않을때 발생하는 데이터 불일치\n[쓰기 전략 - writh throwgh] # 데이터베이스에 업데이트할때마다 매번 캐시에도 데이터를 함께 업데이트시키는 방식이다. 캐시는 항상 최신 데이터를 가지고있을 수 있다. 데이터는 매번 2개의 저장소에 저장돼야 하기 때문에 데이터를 쓸 때마다 시간이 많이 소요될 수 있다.\n다시 사용될만한 데이터가 아닌 경우에는? 무조건 캐시에도 저장되는건 리소스 낭비일 수도 있다. 따라서 위 방시을 사용할 경우 데이터를 저장할때 만료 시간을 사용할 것을 권장한다.\n[cache invalidation] # 데이터베이스에 값을 업데이트 할때마다 캐시에는 데이터를 삭제한다. 신규 데이터 저장보다, 데이터 삭제가 리소스를 훨씬 적게 사용하기 때문이다.\n[write behind(write back)] # 쓰기가 빈번하게 발생하는 시스템이라면 이 방식을 고려하자. 데이터베이스에 대량의 쓰기 작업이 발생하면 이는 많은 디스크I/O를 유말해, 성능 저하가 발생할 수 있다. 먼저 데이터를 빠르게 접근할 수 있는 캐시에 업데이트 한뒤, 이후에는 건수나 특정 시간 간격 등에 따라 비동기적으로 데이터베이스에 업데이트하는 것이다. 저장되는 데이터가 실시간으로 정확한 데이터가 아니어도 되는 경우 유용하다.\n[캐시에서의 데이터 흐름] # 캐시는 메모리이기 때문에 기본적인 스토리지 보다 데이터를 적게 저장할 수 밖에 없다. 캐시는 가득차지 않게 일정 양의 데이터를 유지해야하며 관리되야한다. 캐시로 레디스를 사용할때에는 데이터를 저장함과 동시에 적절한 시간의 TTL 값을 저장하는 것이 좋다.\n[만료시간] # TTL (Time To Live)은 데이터가 얼마나 오래 저장될 것인지를 나타내는 시간 설정이다. 만료시간이 설정되면 해당 키와 관련된 데이터는 지정된 시간이 지난 후에 레디스에서 자동으로 삭제된다.\n커맨드 : TTL, EXPIRE(초 단위), PTTL, PEXPIRE (밀리세컨드 단위)\n// 키에 만료시간 지정 SET a 100 EXPIRE a 60 TTL a INCR 커맨드로 데이터를 조작하거나, RENAME을 이용해 키의 이름을 바꾸더라도 만료시간은 그대로 유지된다. 그러나, 기존 키에 새로운 값을 저장해 키를 덮어쓸 때에는 이전에 설정한 마료시간은 유지되지 않고 사라진다.\nSET b 100 EXPIRE b 60 TTL b // 57 SET b banana TTL b // -1 (만료시간이 지정되지 않음) [메모리 관리와 maxmemory-policy 설정] # 레디스의 메모리는 제한적이기 때문에 모든 키에 만료시간을 설정하더라도 너무 많은 키가 저장되면 메모리가 가득 차는 상황이 발생한다. 메모리의 용량을 초과하는 양의 데이터가 저장되면 레디스는 내부 정책을 사용해 어떤 키를 삭제할지 결정한다.\nmaxmemory 설정: 데이터의 최대 저장 용량 설정 maxmemory-policy 설정값 : 이 용량을 초과할 때의 처리 방식을 결정하는 설정값\n[Noeviction] # 기본값이다. 레디스에 데이터가 가득 차더라도 임의로 데이터를 삭제하지 않고 더이상 레디스에 데이터를 저장할 수 없다는 에러를 반환한다.\n[LRU eviction] # LRU(Least-Recently-Used) evicition이란, 레디스에 데이터가 가득 찼을때 가장 최근에 사용되지 않은 데이터부터 삭제하는 정책이다.\nvolatile-lru : 만료 시간이 설정돼있는 키에 한해서 LRU 방식으로 키를 삭제한다. 만약 모든 키에 만료시간이 지정돼있지 않다면, noeviction 상황과 동일하다. allkeys-LRU : 모든 키에 대해 LRU 알고리즘을 사용해서 데이터를 삭제한다. [LFU eviction] # LFU(Least-Frequently-Used) eviction이란, 레디스에 데이터가 가득 찼을때 가장 자주 사용되지 않은 데이터부터 삭제하는 정책이다. 사용 우선순위는 유동적으로 바뀌므로 특정 케이스에서는 LRU보다 더 효울적일 수 있다.\nvolatile-lru : 만료 시간이 설정돼있는 키에 한해서 LFU 방식으로 키를 삭제한다. 만약 모든 키에 만료시간이 지정돼있지 않다면, noeviction 상황과 동일하다. allkeys-LRU : 모든 키에 대해 LFU 알고리즘을 이용해 데이터를 삭제한다. [LANDOM eviction] # 레디스에 저장된 키 중 하나를 임의로 골라내 삭제한다. 랜덤으로 데이터를 삭제하기 때문에 나중에 사용할 수도 있는 데이터를 삭제할 가능성이 높아진다.\nvolatile-random : 만료 시간이 설정돼있는 키에 한해 랜덤하게 키를 삭제한다. allkeys-random : 모든 키에 대해 랜덤하게 키를 삭제한다. [voldatile-ttl] # 만료시간이 가장 작은 키를 삭제한다. 삭제 예정 시간이 얼마 남지 않은 키를 추출해 해당 키를 미리 삭제하는 옵션이다.\n[캐시 스탬피드 현상] # 대규모 트래픽 환경에서 만료 시간을 어떻게 설정하느냐에 따라 캐시 스탬피드(cache-stampede)와 같은 예상치 못한 문제 상황이 발생할 수 있다. look aside 방식으로 레디스를 사용하고 있을때, 특정 키가 만료되는 시점에 키가 삭제된다면? 여러개의 어플리케이션에서 바라보던 키가 레디스에서 만료돼 삭제된다면 이 서버들은 한꺼번에 데이터베이스에 가서 데이터를 읽어오는 과정을 거친다. 이를 중복 읽기(duplicate read)라고 한다. 이후 각 애플리케이션에서는 읽어온 데이터를 레디스에 쓰게 되는데, 이 또한 여러번 반복되기 때문에 중복 쓰기(duplicate write)가 발생한다.\n[적절한 만료시간 설정] # 캐시 스탬피드를 줄이기 위한 가장 간단한 방법은 만료 시간을 너무 짧지않게 설정하는 것이다. 여러 애플리케이션에서 한꺼번에 접근해야하는 데이터이며, 반복적으로 사용돼야하는 데이터라면 만료시간을 충분히 길게 설정한다.\n[선 계산] # look aside 방식으로 캐시를 사용할때 애플리케이션은 다음 코드와 비슷하게 동작할 것이다. 키가 실제로 만료되기 전에 이 값을 미리 갱신해준다면 여러 애플리케이션에서 한꺼번에 데이터베이스에 접근해 데이터를 읽어오는 과정을 줄여 불필요한 프로세스를 줄일 수 있다.\n[PER 알고리즘] # PER(Probabilistic Early Recomputation) 알고리즘 캐시 값이 만료되기 전에 언제 데이터베이스에 접근해서 값을 읽어오면 되는지 최적으로 계산할 수 있다.\ncurrentTime - ( timeToCompute * beta * log(rand()) ) \u0026gt; expiry currentTime : 현재 남은 만료시간 timeToCompute : 캐시된 값을 다시 계산하는데 걸리는 시간 beta : 기본적으로 1.0 보다 큰 값으로 설정 가능 rand() : 0과 1 사이의 랜덤 값을 반환하는 함수 expiry : 키를 재설정할때 새로 넣어줄 만료 시간 위 알고리즘은 만료시간에 가까워질수록 true를 반환할 확률이 증가하므로, 이는 불필요한 재계산을 효과적으로 방지하는 가장 효율적인 방법일 수 있다.\n[세션 스토어로서의 레디스] # 세션이란? 서비스를 사용하는 클라이언트의 상태 정보를 의미한다. 애플리케이션은 현재 서비스에 로그인돼 있는 클라이언트가 누구인지, 그 클라이언트가 어떤 활동을 하고 있는지 저장하고 있으며, 유저가 서비스를 떠나면 세션스토어에서 유저의 정보를 삭제한다. 많은 서비스에서 레디스를 세션 스토어로 사용하고 있다.\n웹 서버가 여러대로 늘어나는 상황에서, 각 웹 서버별로 세션 스토어를 따로 관리한다면 유저는 유저의 세션 정보를 갖고있는 웹 서버에 종속되야한다. 따라서 레디스를 세션 스토어로 사용해 서버, 데이터베이스와 분리 해놓은 뒤 여러 서버에서 세션 스토어 (1개)를 바라보도록 구성해야한다. 유저는 세션 스토어에 구애받지 않고 어떤 웹 서버에 연결되더라도 동일한 세션 데이터를 조회할 수 있어 트래픽을 효율적으로 분산시킬 수 있으며, 데이터의 일관성도 고려할 필요가 없다. 또한 관계형 데이터베이스보다 훨씬 빠르고 접근하기도 간편하므로 데이터를 가볍게 저장할 수 있다.\n레디스의 hash 자료구조는 세션 데이터를 저장하기에 알맞은 형태다.\nHMSET usersession:1 Name Garimoo IP 10:20:104:30 Hits 1 HINCRBY userssession:2 Hits 1 [캐시와 세션의 차이] # 레디스를 캐시로 사용할때에의 가장 일반적인 look aside 전략을 이용할때 데이터는 데이터베이스의 서브셋으로 동작한다. 세션 스토어에 저장된 데이터는 여러 사용자간 공유되지 않으며, 특정 사용자 ID에 한해 유효하다. 일반적인 세션 스토어에서는 유저가 로그인하면 세션 데이터는 세션 스토어에 저장된다. 유저가 로그인해 있는 동안, 즉 세션이 활성화돼 있는 동안에는 애플리케이션은 유저의 데이터를 데이터베이스가 아닌 세션 스토어에만 저장한다.\n"},{"id":113,"href":"/docs/kotlin/002_Functional_Programming_Example/","title":"002 Functional Programming Example","section":"Kotlin","content":" 코틀린으로 함수형 프로그래밍 시작하기 # [고차함수 : 함수를 함수에 넘기기] # 함수형 프로그램을 작성할때 기본이 되는 몇가지 주제\n함수도 값이다. 함수를 변수에 대입하거나 데이터 구조에 저장하거나 함수의 인자로 넘길 수 있다. 고차함수란? 다른 함수를 인자로 받는 함수\n고차함수 예제 어떤 수의 절댓값과 다른 수의 계승(팩토리얼; factorial)을 출력하는 프로그램\n루프를 함수적으로 작성하는 방법 n의 계승을 계산하는 함수를 추가한다. 재귀(recursion)를 통해 순수 함수로 루프를 작성할 수 있다. fun factorial(i: Int): Int { fun go(n: Int, acc: Int): Int = // \u0026lt;1\u0026gt; if (n \u0026lt;= 0) acc // 루프를 종료시키려면 재귀 호출을 하지 않고, 값을 반환한다. else go(n - 1, n * acc) // 재귀 호출 return go(i, 1) // \u0026lt;2\u0026gt; } i=1 go(0, 1x1) i=2 go(1, 2x1) i=3 go(2, 3x2) i=4 go(3, 4x6) i=5 go(4, 5x24)\n코틀린은 이런 식의(루프로 표현할 수 있는) 재귀를 수동으로 감지하지는 않지만 함수 앞에 tailrec 변경자를 붙이도록 요구한다. tailrec이 붙은 경우 컴파일러는 재귀 호출이 꼬리 위치(tail position)인 경우에 한해 while 루프로 작성했을 때와 같은 종류의 바이트 코드를 토해낸다. =\u0026gt; \u0026ldquo;꼬리 위치에 있는 재귀 호출을 최적화해 while 루프 형태로 변환한다.\u0026rdquo;\n코틀린의 꼬리 호출 fun factorial(i: Int): Int { tailrec fun go(n: Int, acc: Int): Int = // \u0026lt;1\u0026gt; if (n \u0026lt;= 0) acc else go(n - 1, n * acc) // \u0026lt;2\u0026gt; return go(i, 1) } 재귀적인 함수 호출을 하는 호출자가 재귀 함수 호출이 반환값을 즉시 호출하기만 하고 다른 아무일도 하지 않을때, 재귀 호출이 꼬리 위치에 있다고 말한다.\n꼬리 위치에 있다 : go(n-1, n*acc) 꼬리 위치에 있지 않다 : 1 + go(n-1, n*acc) 어떤 함수의 모든 재귀 호출이 꼬리 위치에 있고 함수 앞에 tailrec 변경자가 붙은 경우 코틀린 컴파일러는 재귀를 이터레이션시 호출 스택을 소비하지 않는 반복적인 루프로 컴파일한다. =\u0026gt; 이는 스택 오버플로우를 방지하고 메모리를 효율적으로 사용하는 데 도움이 된다.\n첫번째 고차함수 작성하기\nobject Example { private fun abs(n: Int): Int = if (n \u0026lt; 0) -n else n private fun factorial(i: Int): Int { //\u0026lt;1\u0026gt; 계층함수(formatFactorial())를 추가했으므로 private로 변경 fun go(n: Int, acc: Int): Int = if (n \u0026lt;= 0) acc else go(n - 1, n * acc) return go(i, 1) } fun formatAbs(x: Int): String { val msg = \u0026#34;The absolute value of %d is %d\u0026#34; return msg.format(x, abs(x)) } fun formatFactorial(x: Int): String { //\u0026lt;2\u0026gt; 새로 추가한 함수 (결과 출력) val msg = \u0026#34;The factorial of %d is %d\u0026#34; return msg.format(x, factorial(x)) } } fun main() { println(Example.formatAbs(-42)) println(Example.formatFactorial(7)) //\u0026lt;3\u0026gt; } 두 함수 formatAbs, formatFactorial은 거의 같다. 이 두 함수를 일반화해서 formatResult() 함수를 만들자.\nfun formatResult(name: String, n: Int, f: (Int) -\u0026gt; Int): String { val msg = \u0026#34;The %s of %d is %d.\u0026#34; return msg.format(name, n, f(n)) } fun main() { println(formatResult(\u0026#34;factorial\u0026#34;, 7, ::factorial)) println(formatResult(\u0026#34;absolute value\u0026#34;, -42, ::abs)) } 고차함수\nfun formatResult(name: String, n: Int, f: (Int) -\u0026gt; Int): String { ... } f에 대해서도 타입을 지정한다. f가 정수를 인자로 받고 정수를 반환하는 함수라는 뜻이다.\nabs()나 factorial()을 formatResult의 f 인자로 넘길 수 있다.\nprintln(formatResult(\u0026#34;factorial\u0026#34;, 7, ::factorial)) println(formatResult(\u0026#34;absolute value\u0026#34;, -42, ::abs)) [다형적 함수 : 타입에 대해 추상화하기] # 다형적(polymorphic) 함수 : 고차 함수를 작성할때 어떤 타입이 주어지든 관계없이 동작하는 코드\n예제\n배열에서 어떤 문자열을 찾는 단형적 함수 fun findFirst(ss: Array\u0026lt;String\u0026gt;, key: String): Int { tailrec fun loop(n: Int): Int = when { n \u0026gt;= ss.size -\u0026gt; -1 // \u0026lt;1\u0026gt; ss[n] == key -\u0026gt; n // \u0026lt;2\u0026gt; else -\u0026gt; loop(n + 1) // \u0026lt;3\u0026gt; } return loop(0) // \u0026lt;4\u0026gt; } 위 1)번의 단형적 함수를 다형적 함수로 변환 술어 함수(predicate function)는 어떤 조건을 평가하여 참(true) 또는 거짓(false) 중 하나를 반환하는 함수 fun \u0026lt;A\u0026gt; findFirst(xs: Array\u0026lt;A\u0026gt;, p: (A) -\u0026gt; Boolean): Int { // \u0026lt;1\u0026gt; 원소 타입이 A인 배열에 작용, 배열의 각 원소에 작용하는 술어 함수를 파라미터로 받음 tailrec fun loop(n: Int): Int = when { n \u0026gt;= xs.size -\u0026gt; -1 p(xs[n]) -\u0026gt; n // \u0026lt;2\u0026gt; 술어 함수를 배열 원소에 적용하기 else -\u0026gt; loop(n + 1) } return loop(0) } 타입 변수 A를 사용하는 위치\n배열의 원소는 모두 A 타입이어야 한다. xs: Array\u0026lt;A\u0026gt; p 함수는 A 타입의 값을 인자로 받는다. p: (A) -\u0026gt; Boolean 두 위치에서 같은 타입 변수를 참조한다는 사실은 findFirst()의 두 인자에서 해당 타입이 같은 타입이어야만 한다는 사실을 암시한다. 컴파일러는 findFirst()를 호출하는 코드가 이 두 부분에서 같은 타입을 사용하도록 강제한다.\n[익명 함수를 사용해 고차함수 호출하기] # \u0026gt;\u0026gt;\u0026gt; findFirst(arrayOf(7, 9, 13), {i: Int -\u0026gt; i == 9} {i: Int -\u0026gt; i == 9} 라는 구문을 함수 리터럴이나 익명 함수라고 한다.\n[타입에 맞춰 구현하기] # 함수 시그니처에 의해 구현이 하나로 정해지는 예제를 살펴보자.\nfun \u0026lt;A, B, C\u0026gt; partial1(a: A, f: (A, B) -\u0026gt; C): (B) -\u0026gt; C = TODO() 부분 적용(partial application)을 수행하는 고차함수다. partial1() 함수는 어떤 값과 함수(인자를 둘 받아서, 다른 결과를 내놓음)를 인자로 받는다. 인자를 하나만 받아서 결과를 내놓는 함수를 반환한다. 고차함수 f: (A, B) -\u0026gt; C 반환타입 우리가 반환해야하는 대상의 타입이다. (B) -\u0026gt; C 인자 타입이 B인 함수 리터럴을 작성한다. partial1()의 본문 안에서 a값을 자유롭게 사용할 수 있다. 마찬가지로 b도 익명함수의 인자이므로 함수 본문에서 자유롭게 사용할 수 있다. fun \u0026lt;A, B, C\u0026gt; partial1(a: A, f: (A, B) -\u0026gt; C): (B) -\u0026gt; C = { b: B -\u0026gt; TODO() } 내부 함수가 C 타입의 값을 반환해야한다. C타입의 값을 어떻게 얻을까? C 타입 값은 f 고차함수의 결괏값이다. 따라서 C 타입 값을 얻는 유일한 방법은 f 함수에 A, B 타입 값을 넘기는 것뿐이다. fun \u0026lt;A, B, C\u0026gt; partial1(a: A, f: (A, B) -\u0026gt; C): (B) -\u0026gt; C = { b: B -\u0026gt; f(a, b) } 결과적으로 인자 2개를 받아서 부분적으로 적용해 돌려주는 고차함수가 생긴 것이다. =\u0026gt; \u0026ldquo;A 그리고 A, B를 인자로 받아서 C를 생성해주는 함수가 있다면, A는 이미 인자에 있기 때문에 B만 제공한다면 C를 돌려주는 새로운 함수를 만들 수 있다.\u0026rdquo; 코틀린의 타입 추론 특성으로, 타입 애너테이션 생략이 가능하다. fun \u0026lt;A, B, C\u0026gt; partial1(a: A, f: (A, B) -\u0026gt; C): (B) -\u0026gt; C = { b -\u0026gt; f(a, b) } // 여기서 타입 생략 "},{"id":114,"href":"/docs/kotlin/001_Functional_Programming/","title":"001 Functional Programming","section":"Kotlin","content":" 함수형 프로그래밍이란? # 명령어 스타일 (imperative style) # 컴퓨터에게 정해진 명령 또는 지시를 하나하나 내림으로써 각 명령 단계마다 시스템의 상태를 바꾼다. 처음에는 단순화하려는 의도나, 시스템이 커질수록 복잡해지며, 그 결과 코드를 더이상 유지보수할 수 없게 되고, 테스트 하기 어려워지며 코드를 추론하는데에 어려워진다. 함수형 프로그래밍 (FP, Functional Programming) # 위 명령어 스타일의 대안으로, \u0026lsquo;부수 효과\u0026rsquo;를 완전히 없애는 개념이다. 함수형 프로그래밍의 전제는, 순수 함수를 통해 프로그램을 구성한다는 것이다. 순수 함수 : 아무 부수 효과가 없는 함수 부수 효과란? 결과를 반환하는 것 외에 무언가 다른 일을 하는 함수는 부수 효과가 있는 함수다. 변경이 일어나는 블록 외부 영역에 있는 변수를 변경한다. 데이터 구조를 인플레이스로 변경한다. (즉, 메모리의 내용을 직접 변경한다.) 객체의 필드를 설정한다. 예외를 던지거나 예외를 발생시키면서 프로그램을 중단시킨다. 콘솔에 출력을 하거나 사용자 입력을 읽는다. 파일을 읽거나 쓴다. 화면에 무언가를 그린다. 함수형 프로그래밍(FP)의 장점 : 예제로 알아보기 # 부수 효과가 있는 프로그램 순수하지 않은 프로그램 val listing1 = { class CreditCard { fun charge(price: Float): Unit = TODO() } data class Coffee(val price: Float = 2.50F) //tag::init1[] class Cafe { fun buyCoffee(cc: CreditCard): Coffee { val cup = Coffee() // \u0026lt;1\u0026gt; cc.charge(cup.price) // \u0026lt;2\u0026gt; return cup // \u0026lt;3\u0026gt; } } CreditCard 객체의 charge() 메서드를 호출한다. 이로써 부수 효과가 생긴다. 신용카드를 청구하려면, 신용 카드사에 요청해야하므로 외부에서 부수적으로 벌어지는 일이다. 반환하는 객체는 단지 Coffee 객체다. 이 부수효과로 인해서, 테스트가 어려워진다. 실제 신용 카드사에 접속해서 요청하는 것은 원하지 않는다. CreditCard는 신용카드사에 접속해 비용을 청구하는 방법을 알아서는 안된다. CreditCard가 이런 관심사를 알지 못하게 하고, buyCoffee에 Payments 객체를 넘김으로써 이 코드를 좀더 모듈화하고 테스트성을 향상시킬 수 있다. val listing2 = { data class Coffee(val price: Float = 2.95F) class CreditCard class Payments { fun charge(cc: CreditCard, price: Float): Unit = TODO() } //tag::init2[] class Cafe { fun buyCoffee(cc: CreditCard, p: Payments): Coffee { val cup = Coffee() p.charge(cc, cup.price) return cup } } //end::init2[] } Payments를 인터페이스로 선언할 수 있고, 이 인터페이스에에 대해 테스트에 적합한 mock 객체를 구현할 수 있다. 불필요하게 Payments를 인터페이스로 선언해야한다. buyCoffee()를 재사용하기가 어렵다. 한 고객이 커피를 12잔 주문할 경우, for문으로 buyCoffee()를 호출할 것이다. 이런 식으로 호출하면 charge() 메서드가 12번 수행되어 신용카드사에 12번 연결해서 청구라는 행위를 수행하게 된다. 위 문제의 처리 방안으로, buyCoffess()라는 새로운 함수를 작성해서 한꺼번에 청구하는 로직을 넣을 수 있다. 함수형 해법 # 부수 효과를 제거하고 buyCoffee가 Coffee와 함께 청구할 금액을 반환하게하자. val listing3 = { class CreditCard data class Coffee(val price: Float = 2.50F) data class Charge(val cc: CreditCard, val amount: Float) //tag::init3[] class Cafe { fun buyCoffee(cc: CreditCard): Pair\u0026lt;Coffee, Charge\u0026gt; { val cup = Coffee() return Pair(cup, Charge(cc, cup.price)) // 어떤 금액 청구를 만드는 관심사(Coffee), 청구를 처리하거나 해석하는 관심사(Charge) } } } 두 관심사로 분리했다. 어떤 금액 청구를 만드는 관심사 = Coffee 청구를 처리하거나 해석하는 관심사 = Charge Charge CreditCard와 amount를 포함한다. 같은 CreditCard에 대한 청구를 하나로 묶어줄때 편리하게 쓸 수 있는 combine 함수를 제공한다. val listing4 = { class CreditCard //tag::init4[] data class Charge(val cc: CreditCard, val amount: Float) { // \u0026lt;1\u0026gt; 생성자와 불변 필드가 있는 데이터 클래스 선언 fun combine(other: Charge): Charge = // \u0026lt;2\u0026gt;같은 신용카드에 대한 청구를 하나로 묶음 if (cc == other.cc) // \u0026lt;3\u0026gt; 같은 카드인지 검사. 그 외의 경우 에러 발생 Charge(cc, amount + other.amount) // \u0026lt;4\u0026gt; 이 Charge와 다른 Charge의 금액을 합산한 새로운 Charge를 반환 else throw Exception( \u0026#34;Cannot combine charges to different cards\u0026#34; ) } } buyCoffees 생성 # 이제는 우리 바람대로 buyCoffee를 바탕으로 이 함수를 구현할 수 있다. val listing5 = { class CreditCard data class Coffee(val price: Float = 2.50F) data class Charge(val cc: CreditCard, val amount: Float) { fun combine(other: Charge): Charge = TODO() } //tag::init5[] class Cafe { fun buyCoffee(cc: CreditCard): Pair\u0026lt;Coffee, Charge\u0026gt; = TODO() fun buyCoffees( cc: CreditCard, n: Int // 구매할 커피잔 수 ): Pair\u0026lt;List\u0026lt;Coffee\u0026gt;, Charge\u0026gt; { val purchases: List\u0026lt;Pair\u0026lt;Coffee, Charge\u0026gt;\u0026gt; = List(n) { buyCoffee(cc) } // \u0026lt;1\u0026gt; 자체적으로 초기화되는 리스트를 생성한다. val (coffees, charges) = purchases.unzip() // \u0026lt;2\u0026gt; Pair의 리스트를 두 리스트로 분리한다. List\u0026lt;Coffee\u0026gt;, List\u0026lt;Charge\u0026gt; return Pair( coffees, charges.reduce { c1, c2 -\u0026gt; c1.combine(c2) } ) // \u0026lt;3\u0026gt; coffees를 한 Charge로 합친 출력을 생성한다. } } } 이제는 buyCoffees 함수를 정의할때 직접 buyCoffee를 재사용할 수 있다. Payments 인터페이스에 대한 복잡한 mock 구현을 정의하지 않아도 이 두 함수를 아주 쉽게 테스트할 수 있다. Cafe 객체는 이제 Charge 값이 어떻게 처리되는지와는 무관하다. 같은 카드에 청구하는 금액을 모두 합치기 # val listing6 = { class CreditCard data class Charge(val cc: CreditCard, val amount: Float) { fun combine(other: Charge): Charge = TODO() } //tag::init6[] fun List\u0026lt;Charge\u0026gt;.coalesce(): List\u0026lt;Charge\u0026gt; = this.groupBy { it.cc }.values .map { it.reduce { a, b -\u0026gt; a.combine(b) } } } 청구 금액의 리스트를 취해서 사용한 신용카드에 따라 그룹으로 나누고, 각 그룹의 청구 금액을 하나로 합쳐서 카드마다 하나씩 청구로 만들어낸다. 순수 함수란? # 어떤 함수가 주어진 입력으로부터 결과를 계산하는 것 외에 다른 어떤 관찰 가능한 효과가 없다 -\u0026gt; \u0026ldquo;부수효과가 없다\u0026rdquo; \u0026ldquo;부수 효과가 없는 함수\u0026rdquo; -\u0026gt; \u0026ldquo;순수 함수\u0026rdquo; ex) String의 length 함수 : 주어진 문자열에 대해 항상 같은 길이가 반환되며, 다른 일은 발생하지 않는다. 참조 투명성(RT, Referential Transparency)이라는 개념을 사용해 형식화할 수 있다. 예시로 이해하자. 2 + 3은 순수함수 plus(2, 3)에 적용하는 식이다. 이 식에는 아무 부수효과가 없다. 결과는 언제나 5다. 실제로 프로그램에서 2 + 3을 볼때마다 이 식을 5로 치환할 수 있다. 이렇게 해도 프로그램의 의미가 전혀 바뀌지 않는다. 이는 어떤 식이 참조 투명하다는 말이 지닌 뜻의 전부다. 어떤 프로그램에서 프로그램의 의미를 변경하지 않으면서 식을 그 결괏값으로 치환할 수 있다면, 이 식은 참조 투명하다. 어떤 함수를 참조 투명한 인자를 사용해 호출한 결과가 참조 투명하다면 이 함수도 참조 투명하다. 참조 투명성 예제 # class CreditCard { fun charge(price: Float): Unit = TODO() } data class Coffee(val price: Float = 2.50F) //tag::init[] fun buyCoffee(cc: CreditCard): Coffee { val cup = Coffee() cc.charge(cup.price) return cup } buyCoffee()는 cc.charge(cup.price)의 반환 타입과 관계없이 이 함수 호출의 반환값을 무시한다. 따라서 buyCoffee()를 평과한 결과는 그냥 cup이고, 이 값은 Coffee()와 동일하다. 순수 함수가 되기 위해서는 p에 관계없이 p(buyCoffee(aliceCreditCard)), p(Coffee())가 똑같이 작동해야한다. 성립되지 않는다. p(buyCoffee(aliceCreditCard)) : 카드사를 통해 커피 값을 청구한다. p(Coffee()) : 아무일도 하지 않는다. 참조 투명성 예제(2) # \u0026gt;\u0026gt;\u0026gt; val x = \u0026#34;Hello, World\u0026#34; \u0026gt;\u0026gt;\u0026gt; val r1 = x.reversed() \u0026gt;\u0026gt;\u0026gt; val r2 = x.reversed() x가 등장하는 부분을 x가 가리키는 식으로 바꿔치기 하면 다음과 같다. \u0026gt;\u0026gt;\u0026gt; val r1 = \u0026#34;Hello, World\u0026#34;.reversed() \u0026gt;\u0026gt;\u0026gt; val r2 = \u0026#34;Hello, World\u0026#34;.reversed() 위 r1, r2가 같은 값으로 평가된다. x가 참조 투명하기 때문에 r1, r2 값은 예전과 같다. -\u0026gt; r1, r2도 참조 투명하다. 참조 투명성을 위배하는 예제 # \u0026gt;\u0026gt;\u0026gt; val x = StringBuilder(\u0026#34;Hello\u0026#34;) \u0026gt;\u0026gt;\u0026gt; val y = x.append(\u0026#34;, World\u0026#34;) \u0026gt;\u0026gt;\u0026gt; val r1 = y.toString() \u0026gt;\u0026gt;\u0026gt; val r2 = y.toString() append() 함수 : StringBuilder에 작용하며 객체 내부를 변화시킨다. append()가 호출될 때마다 StringBuilder의 이전 상태가 파괴된다. StringBuilder에 대해 toString()을 여러번 호출해도 항상 똑같은 결과를 얻는다. \u0026gt;\u0026gt;\u0026gt; val x = StringBuilder(\u0026#34;Hello\u0026#34;) \u0026gt;\u0026gt;\u0026gt; val r1 = x.append(\u0026#34;, World\u0026#34;).toString() \u0026gt;\u0026gt;\u0026gt; val r2 = x.append(\u0026#34;, World\u0026#34;).toString() y를 모두 append() 호출로 치환했다. -\u0026gt; 순수 함수가 아니라고 결론을 내릴 수 있다. StringBuilder에 대해 toString()을 여러번 호출해도 결코 같은 결과가 생기지 않는다. r1, r2는 같은 식처럼 보이지만, 실제로는 같은 StringBuilder 객체의 다른 두 값을 가르킨다. "},{"id":115,"href":"/docs/reactive_streams/001_reactive_streams_component/","title":"001 Reactive Streams Component","section":"Reactive Streams","content":" 리액티브 스트림즈란? # 리액티브한 코드 작성을 위한 구성을 도와주는 리액티브 라이브러리가 있다. 이 리액티브 라이브러리를 어떻게 구현해야할지 정의해놓은 별도의 표준 사양을 리액티브 스트림즈(Reactive Streams)라고 한다.\n리액티브 스트림즈는 \u0026lsquo;데이터 스트림을 Non-Blocking이면서 비동기적인 방식으로 처리하기 위한 리액티브 라이브러리 표준 사양\u0026rsquo;이라고 표현할 수 있다. 이를 구현한 구현체로는 RxJava, Reactor, Akka Streams, Java 9 Flow API 등이 있고, 그 중에 Spring framework와 가장 궁합이 잘 맞는 구현체는 Reactor이다.\n리액티브 구성요소 # 리액티브 스트림즈를 통해 구현해야 되는 API 컴포넌트에는 Publisher, Subscriber, Subscription, Processor 가 있다. 이 4개의 컴포넌트를 반드시 기억해야한다. 아무래도 이 4개의 역할이 헷갈리면 전체적인 동작과정까지도 헷갈리게되기 때문에 확실히 짚고 넘어가자.\n컴포넌트 설명 Publisher 데이터를 생성하고 통지(발행, 게시, 방출)하는 역할을 한다. Subsriber 구독한 Publisher로부터 통지된 데이터를 전달받아서 처리한다. Subscription Publisher에 요청할 데이터의 개수를 지정하고, 데이터의 구독을 취소하는 역할을 한다. Processor Publisher와 Subscriber의 기능을 모두 가지고 있다. 즉, Subscriber로서 다른 Publisher를 구독할 수 있고, Publisher로서 다른 Subscriber가 구독할 수 있다. Publisher와 Subscriber의 동작과정을 나타내는 그림 # 1. 데이터를 구독한다. (subscribe)\n먼저 Subscriber는 전달받을 데이터를 구독한다.\n2. 데이터를 통지할 준비가 되었음을 알린다. (onSubscribe)\nPublisher는 데이터를 통지할 준비가 되었음을 Subscriber에 알린다.\n3. 전달 받을 통지 데이터 개수를 요청한다. (Subscription.request)\nPublisher가 데이터를 통지할 준비가 되었다는 알림을 받은 Subscriber는 전달받기를 원하는 데이터의 개수를 Publisher에게 요청한다.\n▶ 데이터의 요청 개수를 지정하는 이유가 뭘까?\nSubscriber가 Subscription.reuqest를 통해 데이터의 요청 개수를 지정한다. 이는 실제로 Publisher와 Subscriber는 각각 다른 스레드에서 비동기적으로 상호작용하는 경우가 대부분이기 때문이다.\n이럴 경우 Publisher가 통지하는 속도가 Publisher로부터 통지된 데이터를 처리하는 Subscriber가 처리하는 속도보다 더 빠르면 처리를 기다리는 데이터가 쌓이게되어 시스템 부하가 커질 수 있다. 이러한 결과를 방지하기 위한 행위다.\nPublisher의 데이터 통지 속도 \u0026gt; 통지된 데이터를 처리하는 Subscriber 속도 간단하게 생각해보면 데이터가 전송되는 속도가 데이터가 처리되는 속도보다 빠르면 계속해서 처리해야하는 데이터가 밀리기 때문인 것이다.\n4. 데이터를 생성한다.\n5. 요청 받은 개수만큼 데이터를 통지한다. (onNext)\nPublisher는 Subscriber로부터 요청받은 만큼의 데이터를 통지한다.\n6. 데이터 처리를 완료할때까지 위 3~5번의 과정을 반복한다.\n이렇게 Publisehr와 Subscriber 간에 데이터 통지, 데이터 수신, 데이터 요청의 과정을 반복한다.\n7. 완료 또는 에러가(onError) 발생할때까지 데이터 생성, 통지, 요청을 계속한다.\n8. 데이터 통지가 완료되었음을 알린다. (onComplete)\n반복하다가 Publisher가 모든 데이터를 통지하게 되면 마지막으로 데이터 전송이 완료되었음을 Subscriber에게 알린다. 만약에 Publisher가 데이터를 처리하는 과정에서 에러가 발생하면 에러가 발생했음을 Subscriber에게 알린다.\n위 글의 내용으로 이해가 되지 않아도 계속해서 포스팅을 읽자. 추후에 예제코드를 보고나서 다시 위 글을 읽으면 이해가 될것이다.\n코드로 보는 리액티브 스트림즈 컴포넌트 # 리액티브 스트림즈 컴포넌트의 동작과정을 이미지와 순서 설명으로 알아보았다. 처리 과정은 이해가 되지만 역시나 Publisher, Subscriber, Subscription, Processor의 코드 흐름이 머릿속으로 그려지지는 않는다. 이제 실제 코드를 보면서 조금씩 머릿속으로 그림을 그려보자.\nPublisher.java # public interface Publisher\u0026lt;T\u0026gt; { public void subscribe(Subscriber \u0026lt;? super T\u0026gt; subscriber); } subscribe() 메서드 1개만 존재한다. subscribe() 메서드는 파라미터로 전달받은 Subscriber를 등록하는 역할을 한다.\n\u0026lsquo;Publisher는 데이터를 생성하고 통지하는 역할을 하고, Subscriber는 Publisher가 통지하는 데이터를 전달받기 위해 구독을 한다.\u0026rsquo; 라는 내용으로 봤을때 우리는 구독을 처리하는 subscribe() 메서드가 당연히 Subscriber에 있을거라고 오해할 수 있다. ▶ 왜 Subscriber가 아닌 Publisher에 subscribe() 메서드가 정의되어 있을까?\n리액티브 스트림즈에서의 Publisher/Subscriber는 개념상으로는 Subscriber가 구독을 하는게 맞다. 하지만 실제 코드상으로는 Publisher가 subscribe() 메서드의 파라미터인 Subscriber를 등록하는 형태로 구독이 이뤄진다. 우선 아래의 호출로직으로 Publisher 객체의 subscribe() 메서드를 호출할때 Subscriber 객체를 파라미터로 넘긴다고 이해하자.\n// 구독 pub.subscribe(sub); Subscriber.java # public interface Subscriber\u0026lt;T\u0026gt; { //구독 시작 처리 public void onSubscribe(Subscription subscription); // 아래 Subscription을 인자로 전달 //데이터 통지시 처리 public void onNext(T item); //에러 통지시 처리 public void onError(Throwable error); //완료 통지시 처리 public void onComplete(); } 메서드 설명 onSubscribe 구독 시작 시점에 어떤 처리를 하는 역할을 한다. 여기서의 처리는 Publisher에게 요청할 데이터의 개수를 지정하거나 구독을 해지하는 것을 의미한다. 이것은 onSubscribe 메서드의 파라미터로 전달되는 Subscription 객체를 통해서 이뤄진다. onNext Publisher가 통지한 데이터를 처리하는 역할을 한다. onError Publisher가 데이터를 통지를 위한 처리 과정에서 에러가 발생했을때 해당 에러를 처리하는 역할을 한다. onComplete Publisher가 데이터 통지를 완료했음을 알릴 때 호출되는 메서드다. 데이터 통지가 정상적으로 완료될 경우에 어떤 후처리를 해야한다면 omComplete 메서드에서 처리 코드를 작성하면 된다. Subscripton.java # public interface Subscription { // 통지받을 데이터 개수를 지정해 데이터 통지를 요청하거나 통지받지 않게 구독을 해지할때 사용하는 인터페이스 //통지받을 데이터 개수 요청 public void request(long num); //구독 해지 public void cancel(); // Subscriber에서 호출함 (Subscription을 받은 Subscriber에서 구독 해지를 위해 호출) } 메서드 설명 request Subscriber가 구독한 데이터의 개수를 요청한다. Publisher에게 데이터의 개수를 요청할 수 있다. cancel 구독을 해지한다. 실제 구독하는 구현코드! 위의 설명을 돕기위한 예제 코드다.\n// Publisher Publisher\u0026lt;Integer\u0026gt; pub = new Publisher\u0026lt;Integer\u0026gt;() { @Override public void subscribe(Subscriber\u0026lt;? super Integer\u0026gt; sub) { sub.onSubscribe(new Subscription() { @Override public void request(long n) { ... } @Override public void cancel() { ... } }); } }; // Subscriber Subscriber\u0026lt;Integer\u0026gt; sub = new Subscriber\u0026lt;Integer\u0026gt;() { @Override public void onSubscribe(Subscription s) { log.debug(\u0026#34;onSubscribe\u0026#34;); s.request(Long.MAX_VALUE); } @Override public void onNext(Integer i) { log.debug(\u0026#34;onNext:{}\u0026#34;, i); } @Override public void onError(Throwable t) { log.debug(\u0026#34;onError:{}\u0026#34;, t); } @Override public void onComplete() { log.debug(\u0026#34;onComplete\u0026#34;); } }; // 구독 pub.subscribe(sub); 결국 Publisher의 subscribe() 메서드를 호출하여 구독하는데, 이때 파라미터 Subscriber 객체를 넘기면 된다.\n위 코드의 흐름\nPublisher와 Subscriber의 동작과정을 리액티브 스트림즈의 컴포넌트 코드 관점에서 다시 이해해보자.\n순서 과정 1 Publisher가 Subscriber 인터페이스 구현 객체를 subscribe 메서드의 파라미터로 전달한다. 2 Publisher 냅에서는 전달받은 Subscriber 인터페이스 구현 객체의 onSubscribe 메서드를 호출하면서 Subscriber의 구독을 의미하는 Subscription 인터페이스 구현 객체를 Subscriber에게 전달한다. 3 호출된 Subscriber 인터페이스 구현 객체의 onSubscribe() 메서드에서 전달받은 Subscription 객체를 통해 전달받을 데이터의 개수를 Publisher 에게 요청한다. 4 Publisher는 Subscriber로부터 전달받은 요청 개수만큼의 데이터를 onNext() 메서드를 호출해서 Subsriber에게 전달한다. 5 Publisher는 통지할 데이터가 더이상 없을 경우 onComplete 메서드를 호출해서 Subscriber에게 데이터 처리 종료를 알린다. Processor.java # public abstract interface Processor\u0026lt;T, R\u0026gt; extends Subscriber\u0026lt;T\u0026gt;, Publichser\u0026lt;R\u0026gt; {} Processor 인터페이스는 별도로 구현해야할 메서드가 없다. 대신 Subscriber, Publisher 인터페이스를 상속하고있다. 리액티브 스트림즈 컴포넌트에서 설명한대로 Processor가 Publisher과 Subscriber 기능을 모두 가지고있기 때문이다.\n구현 예제코드 # @Slf4j public class E05_PubSub_2 { public static void main(String[] args) { Publisher\u0026lt;Integer\u0026gt; pub = iterPub(Stream.iterate(1, a -\u0026gt; a + 1).limit(10).collect(Collectors.toList())); // 구독자 Subscriber\u0026lt;Integer\u0026gt; sub = logSub(); // 구독 시작 pub.subscribe(sub); } private static Subscriber\u0026lt;Integer\u0026gt; logSub() { Subscriber\u0026lt;Integer\u0026gt; sub = new Subscriber\u0026lt;Integer\u0026gt;() { @Override public void onSubscribe(Subscription s) { // Subscription 의 request 를 요청해야한다. log.debug(\u0026#34;onSubscribe\u0026#34;); s.request(Long.MAX_VALUE); } @Override public void onNext(Integer i) { log.debug(\u0026#34;onNext:{}\u0026#34;, i); } @Override public void onError(Throwable t) { log.debug(\u0026#34;onError:{}\u0026#34;, t); } @Override public void onComplete() { log.debug(\u0026#34;onComplete\u0026#34;); } }; return sub; } private static Publisher\u0026lt;Integer\u0026gt; iterPub(List\u0026lt;Integer\u0026gt; iter) { Publisher\u0026lt;Integer\u0026gt; pub = new Publisher\u0026lt;Integer\u0026gt;() { // Publisher 의 구현해야하는 메서드 @Override public void subscribe(Subscriber\u0026lt;? super Integer\u0026gt; sub) { // 호출하면 그때부터 데이터를 통지 // Subscription : Publisher, Subscriber 둘 사이의 구독이 한번 일어난다는 의미 sub.onSubscribe(new Subscription() { @Override public void request(long n) { try { // iterable 의 원소를 통지한다. iter.forEach(s -\u0026gt; sub.onNext(s)); // 여기서 멈추면 안되고, publisher 가 데이터 통지가 완료했으면 완료됨을 호출해야한다. sub.onComplete(); } catch (Throwable t) { // 에러 처리 sub.onError(t); } } /** * Subscriber 에서 Subscription 객체의 cancel()을 호출할 수 있다. * 더이상 데이터를 통지받지 않겠다고 알림 */ @Override public void cancel() { } }); } }; return pub; } } Publisher 객체 생성 Publisher\u0026lt;Integer\u0026gt; pub = iterPub(Stream.iterate(1, a -\u0026gt; a + 1).limit(10).collect(Collectors.toList())); Subscriber 객체 생성 Subscriber\u0026lt;Integer\u0026gt; sub = logSub(); 구독 실행! pub.subscribe(sub); 여기서부터 실행 흐름을 들여다보자. # Subscriber 객체의 onSubscribe() 호출 메서드 설명 onSubscribe 구독 시작 시점에 어떤 처리를 하는 역할을 한다. 여기서의 처리는 Publisher에게 요청할 데이터의 개수를 지정하거나 구독을 해지하는 것을 의미한다. 이것은 onSubscribe 메서드의 파라미터로 전달되는 Subscription 객체를 통해서 이뤄진다. Subscriber 객체의 onNext() 호출 메서드 설명 onNext Publisher가 통지한 데이터를 처리하는 역할을 한다. !\n위 1)~2)번 반복\n1, 2, 3 ~ 9까지 반복 완료 후 마지막 데이터인 10의 onNext()가 호출된 시점이 왔다.\n모든 데이터 통지가 완료되었으므로 onComplete() 메서드 호출 메서드 설명 onComplete Publisher가 데이터 통지를 완료했음을 알릴 때 호출되는 메서드다. 데이터 통지가 정상적으로 완료될 경우에 어떤 후처리를 해야한다면 omComplete 메서드에서 처리 코드를 작성하면 된다. ▶ Subscriber 객체의 onComplete() 호출된 모습\n실행결과\n[main] DEBUG com.reactive.step02.E05_PubSub_2 - onSubscribe [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:1 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:2 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:3 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:4 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:5 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:6 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:7 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:8 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:9 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onNext:10 [main] DEBUG com.reactive.step02.E05_PubSub_2 - onComplete "},{"id":116,"href":"/docs/algorithm/000_sample/","title":"000 Sample","section":"Algorithm","content":"sample\n"},{"id":117,"href":"/docs/data_structure/000_sample/","title":"000 Sample","section":"Data Structure","content":"sample\n"},{"id":118,"href":"/docs/ddd/000_sample/","title":"000 Sample","section":"Ddd","content":"sample\n"},{"id":119,"href":"/docs/docker/000_sample/","title":"000 Sample","section":"Docker","content":"sample\n"},{"id":120,"href":"/docs/jenkins/000_sample/","title":"000 Sample","section":"Jenkins","content":"sample\n"},{"id":121,"href":"/docs/jpa/000_sample/","title":"000 Sample","section":"Jpa","content":"sample\n"},{"id":122,"href":"/docs/linux/000_sample/","title":"000 Sample","section":"Linux","content":"sample\n"},{"id":123,"href":"/docs/mvc/000_sample/","title":"000 Sample","section":"Mvc","content":"sample\n"},{"id":124,"href":"/docs/mysql/000_sample/","title":"000 Sample","section":"Mysql","content":"sample\n"},{"id":125,"href":"/docs/network/000_sample/","title":"000 Sample","section":"Network","content":"sample\n"},{"id":126,"href":"/docs/oracle/000_sample/","title":"000 Sample","section":"Oracle","content":"sample\n"},{"id":127,"href":"/docs/security/000_sample/","title":"000 Sample","section":"Security","content":"sample\n"},{"id":128,"href":"/docs/servlet/000_sample/","title":"000 Sample","section":"Servlet","content":"sample\n"},{"id":129,"href":"/docs/spring/000_sample/","title":"000 Sample","section":"Spring","content":"sample\n"},{"id":130,"href":"/docs/webflux/000_sample/","title":"000 Sample","section":"Webflux","content":"sample\n"}]